{
  "research_topic": "Graph Attention Networkの学習の高速化",
  "queries": [
    "graph attention network acceleration",
    "GAT training optimization",
    "efficient GAT training",
    "sparse attention GAT",
    "distributed GAT training"
  ],
  "research_study_list": [
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    },
    {
      "title": "Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level",
      "abstract": "Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...",
      "full_text": "Faster Neighborhood Attention: Reducing the O(n2) Cost of Self Attention at the Threadblock Level Ali Hassani1, Wen-mei Hwu2,3, Humphrey Shi1,3 1SHI Labs @ Georgia Tech, 2NVIDIA, 3UIUC Abstract Neighborhood attention reduces the cost of self attention by restricting each to- ken’s attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention pat- terns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we aim to massively improve upon existing infrastructure by providing two new methods for implementing neighbor- hood attention. We first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision runtime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood attention respectively. We find that aside from being heavily bound by memory bandwidth, certain inherent inefficiencies exist in all unfused implementations of neighborhood attention, which in most cases undo their theoretical efficiency gain. Motivated by the progress made into fused dot-product attention kernels, we developed fused neighborhood attention; an adaptation of fused dot-product attention kernels that allow fine-grained control over attention across different spatial axes. Known for reducing the quadratic time complexity of self attention to a linear complexity, neighborhood attention can now enjoy a reduced and constant memory footprint, and record-breaking half precision runtime. We observe that our fused implementation successfully circumvents some of the unavoidable inefficiencies in unfused implementations. While our unfused GEMM-based kernels only improve half precision performance compared to naive kernels by an average of 548% and 193% in 1-D and 2-D problems respectively, our fused kernels improve naive kernels by an average of 1759% and 958% in 1-D and 2-D problems respectively. These improvements translate into up to 104% improvement in inference and 39% improvement in training existing models based on neighborhood attention, and additionally extend its applicability to image and video perception, as well as other modalities. Our work is open-sourced at https://github.com/SHI-Labs/NATTEN/. 1 Introduction Inarguably among the most highly utilized and influential primitives in modern deep learning, attention has long been cited for its complexity and memory footprint, especially when the query and context sets are identical (self attention). For years since its adoption in deep learning [23], the most common implementation of attention was through two batched GEMM (General Matrix-Matrix Multiplication) operations, sometimes referred to as “BMM-style” attention. This implementation stores attention weights to global memory, which can become a bottleneck in both speed and memory 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2403.04690v3  [cs.CV]  31 Oct 2024NA-1D NA-2D NA-3D 200% 400% 600% 800% 1000% 1200% 1400% 1600% 1800% Baseline 548 % 1,759 % 193 % 958 % 1,135 % Forward pass only (FP16) Speedup (%) NA-1D NA-2D NA-3D 502 % 844 % 92 % 385 % 447 % Forward pass + backward pass (FP16) Naive GEMM NA Fused NA Figure 1: Overview of average improvement in speed on A100 from our proposed imple- mentation. Baseline is the set of naive CUDA kernels introduced in Neighborhood Attention Transformer [9]. GEMM-based NA improves 1-D problems by an average of 548% (forward pass) and 502% (forward + backward), and 2-D problems by an average of 193% (forward pass) and 92% (forward + backward). GEMM-based NA does not implement 3-D problems yet. Fused NA boosts performance further and improves 1-D problems by an average of 1759% (forward pass) and 844% (forward + backward), and 2-D problems by an average of 958% (forward pass) and 385% (forward + backward), and 3-D problems by an average of 1135% (forward pass) and 447% (forward + backward). footprint. As the number of tokens grow, the number of attention weights grow as well, and the problem gets bounded by global memory bandwidth and capacity. Over the past few years, some works proposed attention implementations in which attention weights are kept in on-chip memory (shared memory or register file) instead, until the second matrix mul- tiplication is performed and the resulting attention outputs are written directly to global memory [18, 6]. These implementations, known as fused or memory-efficient attention, reduce the number of global memory accesses in addition to global memory usage, and successfuly turn dot product attention into a compute-bound problem at scale. Thanks to the first open-source implementation, Flash Attention [6], these fused attention kernels have started replacing the standard BMM-style implementations in many deep learning frameworks and inference engines such as PyTorch [16]. Orthogonal to these efforts, many have sought to address the quadratic complexity of self attention, which can become a significant bottleneck in vision models more quickly. Neighborhood attention [9] is one such method in which each query token is restricted to only interact with its nearest neighboring context tokens. In most cases, this pattern creates a sliding window pattern, like that of the discrete convolution operator heavily employed in vision models. This restriction can similarly be parame- terized by a window size and dilation factor, and reduces the quadratic complexity of self attention down to a linear complexity. This approach is, however, very difficult to implement at the tensor library or deep learning framework level. Tensor views can represent sliding window attention [19], but not the neighborhood attention pattern. In addition, standard GEMM implementations typically do not support such tensor views in higher-rank/multi-dimensional spaces (2-D and 3-D) without explicit copying into contiguous tensors, which in practice undoes the theoretical efficiency gain from the reduced attention complexity. As a result, neighborhood attention was proposed along with an extension carrying naive CUDA kernels [ 9] implementing the operation. While those kernels provided competitive FP32 performance in eager mode inference, and in some cases even FP16/BF16 performance, they fall short of general adoption in larger scale experiments. In addition, fused attention implementations, such as Flash Attention, effectively eliminate theO(n2) memory footprint in self attention, while also reducing runtime significantly [6], making subquadratic attention patterns that are only possible to implement “BMM-style” less practical. In this work, we present two new classes of neighborhood attention kernels: GEMM-based BMM- style kernels (GEMM NA), and fused kernels (Fused NA), which are aimed at providing significantly improved infrastructure for neighborhood attention. We first show that neighborhood attention, and by 2Linear Proj = NA (1 ×1) NA (3 ×3) NA (5 ×5) NA (7 ×7) NA (9 ×9) NA (11 ×11) NA (13 ×13) = Self Attn DiNA (3 ×3, 2 ×2) DiNA (3 ×3, 3 ×3) DiNA (3 ×3, 4 ×4) DiNA (5 ×5, 2 ×2) Causal NA (3 ×3) Causal NA (5 ×5) Causal NA (7 ×7) Causal NA (9 ×9) Figure 2: Illustration of the spectrum of possible attention patterns provided by neighborhood attention. Neighborhood attention only attempts to center the query token (red) within the context window (blue), unlike sliding window attention [19] which forces it. Neighborhood attention with window size 1 is equivalent to linear projection (“no attention”). Neighborhood attention approaches self attention as window size grows, and matches it when equal to input size. Dilation introduces sparse global context, and causal masking prevents interaction between query tokens that have a smaller coordinate than neighboring context tokens along the corresponding mode. Window size, dilation, and whether or not causally masked, can be defined per mode/axis. extension sliding window attention, both of which are GEMV (General Matrix-Vector Multiplication) problems, can be expressed as GEMM problems with space-aware tiling and gather/scatter fusion. This would allow implementing such attention patterns with performance-optimized GEMM primi- tives, which can also utilize specialized hardware components such as NVIDIA’s Tensor Cores. We then extend the same logic to fused attention kernels by removing all assumptions that the token mode (“space”) is rank-1 (single-axis). We write specializations that support higher-rank/multi-dimensional spaces, such as 2-D and 3-D. This, in theory, allows any fused attention kernel to be modified to accommodate token spaces of any rank. In addition, part of the logic is evaluated at compile time, resulting in less overhead. Finally, the structural simplicity of the resulting fused neighborhood attention kernels allows for easily adding features such as varying window sizes / dilation values across ranks/axes, causal masking, and more. 2 Related works Attention being adopted as a deep learning primitive is largely owed to the Transformer architec- ture [23], which despite its original application in machine translation rose to the position of being the predominant deep learning architecture. Its design and use of the attention operator have been extended to many other applications and modalities [15, 7, 1, 17]. Attention is defined as an operation between a two sets of vectors: a query set and a context set. The two undergo linear projections, with the latter projected into a set of key and value pairs. Scaled dot product of query and key vectors, A, is mapped into a probability distribution through the softmax operator, which produces the final attention weights, P. The output is a set of vectors, each derived from the weighted sum of all value vectors according to the query vector’s attention weights. It can be expressed as follows: Attention(Q, K, V) = P z }| { softmax   QKT √ d| {z } A  V, (1) 3where Q, K, and V are matrices of query, key, and value vectors as rows respectively, √ d is the scale term, and d is the number of dimensions for which the dot product is computed (number of columns in Q and K). Dot product self attention, or simply, self attention, is a special case in which the query and context sets are identical. This means for a set of n input vectors, the attention weights matrix, P, is ∈ Rn×n, incurring an O(n2) time and space complexity. In addition, the softmax term requires column-wise reduction over the attention weight matrix, making kernel fusion more challenging. Nevertheless, fused attention kernels successfully eliminate the O(n2) global memory footprint, which makes self attention finally bound by compute and not memory bandwidth. These two achievements paved the way for the scaling and application of attention across modalities. To our knowledge, the first open-source implementation of a fused multi-headed attention (FMHA) kernel was contributed to the NVIDIA Apex 1 project by Young-Jun Ko, which was primarily used for accelerating inference of Transformer-based language models. As a result of that, it was heavily limited in terms of supported models and problem sizes, as it was performing a full softmax reduction step within the kernel. On the other hand, Milakov and Gimelshein [14] presented a technique for computing partial softmax statistics, over which we can perform a final reduction step and derive exact softmax results. This method makes the fusion of attention kernels more practical, because they would no longer be required to compute a full row of attention weights before proceeding to perform the second matrix multiplication. Dao et al. [6] presented and open-sourced Flash Attention, which utilizes online softmax in order to create a performant and generic fused attention implementation. Outperforming BMM-style implementations available in both training and inference, Flash Attention was quickly adopted by many frameworks such as PyTorch [ 16], and further improved for the NVIDIA Ampere [5] and Hopper architectures [20]. Parallel to these efforts, many proposed restricted self attention patterns, in which context is restricted to a subset in order to generate fewer attention weights, which in turn reduces the O(n2) time and space complexity. Stand-alone self attention (SASA) [19] is a simple 2-dimensional sliding window attention pattern, which was shown to effectively replace convolution operations in ResNet [ 10] variants. Noting challenges in implementing such patterns without incurring additional overhead from tensor copies and expansion, the authors later moved away from explicit sliding window attention patterns to alternatives that relaxed the sliding window movement in HaloNet [22]. In addition to these works, sliding window attention patterns in 1-dimensional spaces has been explored in language, in works such as Sparse Transformers [4], Longformer [2], BigBird [26], and more recently, Mistral [11]. Neighborhood attention [9, 8] is the practice of restricting the context of each token to its nearest neighbors, which in many cases behaves like a sliding window pattern, with the exception of corner cases in which the query cannot be centered in a sliding window. Per definitions from SASA [19] and Longformer [2], the sliding context window can go out of bounds, in which case the attention weights corresponding to out-of-bounds tokens are masked. This means tokens close to spatial bounds interact with fewer context tokens. This difference allows neighborhood attention to approach self attention as window size grows. In addition, neighborhood attention defines a dilation factor [ 8], where the number of such corner cases only increase. Fig. 2 depicts possible attention patterns for a single token under different neighborhood attention parameters. Facing similar implementation challenges as previous works [ 19], neighborhood attention was implemented with naive CUDA kernels packaged as a PyTorch extension, named NATTEN . While those kernels have accelerated research in this direction, they were simply not intended to fully utilize the underlying hardware. The only exception is the tiled kernels, which are somewhat better optimized, but only apply to a fraction of common use cases, and are not extensible. In addition, with the rise of fused attention kernels such as Flash Attention [6], such implementations which are not performance-optimized and heavily memory-bandwidth-bound, can hardly compete in terms of performance and memory footprint. To address these challenges, we present two new implementations and integrate them intoNATTEN , aiming to accelerate all neighborhood attention applications, reduce their existing memory overhead, and extend existing functionality. We first simplify the operations that implement neighborhood attention’s forward and backward pass into 3 primary operators, and show each can be implemented as batched GEMM kernels with a fused gather/scatter operation. We then point out key limita- tions in unfused neighborhood attention implementations that would prevent them from achieving competitive performance compared to standard BMM-style attention implementations (in more memory-bandwidth-bound cases.) Motivated by this, and the progress made in fused attention kernels, we propose fused neighborhood attention, which directly extends our batched GEMM 1https://github.com/NVIDIA/apex 4T h × T w × d T ′ h × T ′ w × d k h × k w × d Q K GEMM output T h T w × T ′ h T ′ w Flattened Q tileT h T w × d Flattened K (sub-)tile T ′ h T ′ w × d T h × T w × k h k w A Select & Scatter global memory shared memory / register file global memory Figure 3: Illustration of our GEMM-based implementation of the 2-D PN operation. Input tensors Q and K are tiled according to their 2-D spatial layout. Q is tiled with a static tile shape, Th × Tw. K is tiled with a haloed shape of the Q tile, T′ h × T′ w, which is a function of the attention window size (kh × kw) and the Q tile coordinates. Once tiles are moved into local memory, they are viewed in matrix layout, and a ThTw × T′ hT′ w × d shaped GEMM is computed (d is embedding dim). Once done, the tile of dot products with shape ThTw × T′ hT′ w is scattered into valid attention weights of shape Th × Tw × khkw. methodology. Since our main objectives are efficiency, Tensor Core utilization, and performance optimization, and our methodology requires significant flexibility in the programming model, we implement both approaches in CUDA C++ using NVIDIA’s CUTLASS [21] framework. We show that the batched GEMM kernels can successfully outperform most existing NATTEN kernels in performance, and that our fused kernels can outperform our batched GEMM kernels while reducing the memory footprint. 3 Methodology Herein we describe three primary operations (excluding softmax) that are required to implement a full neighborhood attention forward and backward pass. We then show that each operation can be expressed as a batched GEMM problem, as long as tiling is done according to the underlying spatial rank, and attention weights are scatter/gathered. However, we find that scatter/gather is a major bottleneck for all unfused implementations of neighborhood attention, limiting their low-precision performance specifically on more recent architectures (Ampere and later.) We then introduce our fused neighborhood attention (FNA) formulation, which builds on our batched GEMM formulation and tiles according to the underlying spatial rank. This approach no longer requires scatter/gathering of attention weights to/from global memory by definition, and thereby circumvents the aforementioned bottleneck and successfully boosts lower-precision performance on modern architectures. 3.1 Operators A standard BMM-style attention forward pass (excluding softmax) is comprised of two operations: QKT , which produces pre-softmax attention weights ( A), and PV , which applies post-softmax attention weights (P) to values (V ). These operations are different due to layout differences in the matrix multiplications (note that K is transposed, V is not). 2 In the case of neighborhood attention, and sliding window attention in general, these will become General Matrix-Vector Multiplication (GEMV) problems. In QKT , each query token (vector) is multiplied by its neighboring or surrounding key tokens (matrix), and in PV , the set of attention 2QKT is a TN-layout and PV is a TT-layout GEMM in BLAS. 5weights corresponding to each query token (vector) is multiplied by corresponding value tokens (matrix). Given that some of these operations can be reused in the backward pass, we dub the QKT operation “Pointwise-Neighborhood” (PN) and the PV operation “Neighborhood-Neighborhood” (NN). PN can compute the gradient for post-softmax attention weights (∇P) when operating on the output gradient instead of Q, and V instead of K. Similarly, NN can compute the gradient for Q (∇Q) when operating on the pre-softmax attention gradient (∇A) instead of A and K instead of V . We define a third operator, which can compute gradients for bothK and V : Inverse-Neighborhood (IN). This operation is very similar to NN, but differs in gather pattern, as well as the number of attention weights. IN may require loading more attention weights for every token, because unlike in self attention, the relationship between query and context tokens in neighborhood attention is not commutative. In other words, query token at coordinate i attending to context token at coordinate j does not imply that query token at coordinate j attends to context token at coordinate i. BMM-style implementations of standard self attention have a clear edge over neighborhood and sliding window attention implementations, because they are GEMM problems and by extension not as bound by memory bandwidth as the latter, all of which are GEMV problems. In addition, GEMV problems cannot effectively utilize matrix multiply and accumulate (MMA) accelerators, such as Tensor Cores. We aim to minimize this issue by formulating all three operators as batched GEMM problems with scatter/gather fusion, in order to better utilize modern hardware accelerators. 3.2 Batched GEMM NA We transform the aforementioned GEMV problems into batched GEMMs with scatter/gather fusion. At an abstract level, implementations of GEMM-based neighborhood attention predicate the execution of tiled MMAs on whether any of the rows in the query tile interact with at least one of the rows in the context tile, given the context window size, dilation, and other masking-related parameters. We propose modifying a CUTLASS GEMM as follows in order to implement PN, NN, and IN: 1. GEMM tiling is done according to the original multi-dimensional layout of the token mode in QKV . For example, if the attention problem is 1-D, query and context tensors are tiled along the sequence into tiles of size 64, for a 2-D problem, the token mode, which is comprised of height and width, are tiled by a 2-D tiler of the same size, like 8 × 8. 2. Predication logic, and global pointer iterators and accessors are modified to iterate according to the original layout in global memory instead of assuming a standard rank-2 matrix layout. 3. Attention weights are required to be scattered to and gathered from global memory, which in 16-bit or lower precision cannot be copied asynchronously (with LDGSTS), which breaks pipelining in those kernels on modern architectures. This is because the minimum transaction size for LDGSTS is 32 bits. We implemented these concepts by extending implicit GEMM (convolution) in CUTLASS (2.X API) into kernels that compute the three neighborhood attention operators in 1-D and 2-D. Fig. 3 shows an illustration of the 2-D GEMM-based PN kernel. The first change is relatively inexpensive, but the second change incurs additional predication and indexing logic that can result in additional overhead and register pressure. The final change is a major bottleneck, and leads to lower-precision kernels (FP16/BF16) providing little to no improvement compared to their full precision (FP32/TF32) counterparts. NN and IN suffer from this issue more significantly, because gathering attention weights (LDG) breaks pipelined kernels on Ampere, since they load GEMM operands asynchronously (LDGSTS), which has a minimum transaction size of 32 bits. This forces our FP16/BF16 GEMM- based kernels to fall back to global loads (LDG), which significantly impacts achievable runtime. To our knowledge, this issue is unavoidable in most cases, and will continue to be a bottleneck as long as attention weights are stored in global memory. 3.3 Fused NA We extend our methodology for implementing neighborhood attention operators using batched GEMM kernels to fused attention kernels like Flash Attention [6]. This is not only motivated by the potential to reduce runtime and memory footprint, and potentially making neighborhood attention actually bound by compute, but also to circumvent the bottleneck in the batched GEMM and naive kernels: scatter/gathering attention weights to/from global memory. Since attention weights are only computed 6Q K / V Attention weights NA masking + partial softmax Flattened Q tile Flattened K sub-tile Flattened V sub-tile Output global memory shared memory / register file global memory Figure 4: A simplified illustration of fused neighborhood attention. Q and KV tensors are tiled according to their spatial layout (1-D, 2-D, 3-D), with the latter haloed to include the entire neighborhood for all corresponding queries in the query tile. Resulting attention weights from the first GEMM are masked according to neighborhood attention parameters, before undergoing online softmax scaling, and going through the second GEMM with the corresponding value sub-tile. at the threadblock level and never fully stored in global memory in fused kernels, the bottleneck will simply cease to exist. We started off with xFormers FMHA [13], a fused multi-headed attention kernel based on the CUTLASS 2.X API, which can target architectures even older than Ampere (Maxwell, SM50; V olta, SM70; and Turing, SM75.) By carefully applying our methodology for space-aware tiling, neighborhood attention masking, and software predication for multi-dimensional tensor layouts, we successfully implemented neighborhood attention for 1-D, 2-D, and 3-D problems. Fig. 4 presents an overview of how our fused kernels function when dealing with multi-dimensional (multi-axis) data. 3.4 Dilation and causal masking Our methodology allows for dilation support trivially, through simple partitioning and slicing ahead of time. A dilated neighborhood attention problem can be mapped to a set of non-dilated neighborhood attention problems over non-overlapping tiles of the input. All sub-problems can be computed within the same kernel call, simply by issuing more CTAs in the grid. We additionally define and implement causal neighborhood attention into our fused kernel, which can be crucial to certain applications where only one spatial dimension requires causal masking (i.e. video embeddings may benefit from causally masked attention across the time axis and standard attention across height and width axes, which would be an exact 3-D spatio-temporal attention module.) 3.5 Notes on arithmetic intensity Arithmetic intensity is the ratio of floating point operations over bytes of memory transactions, as defined by the Roofline model [25]: Arithmetic Intensity = Nops Nbytes (2) Arithmetic intensity is typically used to determine whether an implementation/algorithm is bound by memory bandwidth or computational capacity, on a given problem size and hardware. Let’s consider a simplified representation of self attention, where we only look at pure matrix multiplication FLOPs and bytes. Self attention is comprised of two back-to-back BMMs, which would be 2bhn2d FLOPs for each of the BMMs, where b, h, n, and d denote batch size, number of attention heads, sequence 7length, and per-head dimension respectively. In total, that would be 4bhn2d FLOPs. In the unfused implementation, 4 tensors with size bhnd (Q, K, V and output) are accessed in global memory, along with one intermediary tensor with size bhn2 (attention weights or P), which is accessed twice. In total, that is (4 ×bhnd+ 2×bhn2) ×sdtype bytes, where sdtype is the byte size of the tensor element type. When implemented with fused attention, however, the number of bytes accessed for matrix multiplication from global memory is reduced to only reads and writes for Q, K, V , and attention outputs, or 4 × bhnd × sdtype. Unfused implementations of attention are typically memory-bandwidth-bound at scale, given that their arithmetic intensity approaches a constant value as sequence length grows. If we take the limit of their intensity according to the aforementioned approximation of FLOPs and transaction bytes, as n → ∞with everything else as constants, we see that: lim n→∞ 4bhn2d (4bhnd + 2bhn2)sdtype = lim n→∞ 2nd (2d + n)sdtype = 2d sdtype (3) Fused attention therefore solves a key problem here, by reducing the number of memory transactions from O(n2) to O(n), which means as sequence length grows, the limit of arithmetic intensity in fused attention does not converge, and it will therefore only become bound by computational capacity. This means that optimal fused attention kernels can almost fully utilize the underlying computational power of modern GPUs, and is the reason behind FP8 attention kernels for the Hopper architecture exceeding the 1 petaFLOP/s threshold [3, 20]. A natural question to ask is what happens to local attention patterns such as neighborhood attention, which promised to deliver more efficiency. In fused implementations of neighborhood attention (i.e. our proposed FNA), we can look at the growth of arithmetic intensity similar to self attention. If we consider the FLOPs for neighborhood attention to be 4bhnℓd, where ℓ is the size of the attention window, and that the number of global memory transaction bytes is the same as fused self attention (worst case), 4 × bhnd × sdtype, then we see that as n → ∞, we converge towards a constant again, therefore making neighborhood attention more memory-bandwidth-bound as n alone scales: lim n→∞ 4bhnℓd 4bhndsdtype = ℓ sdtype (4) However, the constant here is a function of ℓ, the size of our attention window, which means that as we scale the sequence length or feature map size, attention window size will determine whether or not the problem is bound by memory bandwidth or computational power. Since smaller window sizes are closer to linear projections, and larger window sizes are closer to self attention, the fact that neighborhood attention can be bound by compute or memory bandwidth depending on window size is not a surprise. Therefore, it is highly recommended to choose neighborhood attention window sizes according to the input size and even hardware to maximize efficiency gain. 3.6 Limitations Our formulation of GEMM-based and fused neighborhood attention kernels poses a critical question: how much overhead can one expect from switching from a standard self attention kernel to neighbor- hood attention?As pointed out in Sec. 3.2, our GEMM-based kernels suffer from a major bottleneck, especially in lower-precision, which stems from scatter/gathering of attention weights. We consider this to be an unavoidable issue in unfused implementations of neighborhood and sliding window attention. Unsurprisingly, our proposed changes to fused implementations are also not free. Changes that we find unavoidable, which in some cases can cause our fused kernels to incur higher runtime than the self attention baseline (xFormers FMHA) are the following (ordered by most significant to least significant): 1. Kernels specialized for 2-D and 3-D problems are no longer GEMMs, they are General Tensor-Tensor contractions (GETTs)! Similar to convolution, if the input layout is multi- dimensional, then the GEMM is converted to a special case of GETT. On older GPU architectures, this requires more complicated software predication, which will incur more instructions and heavier register usage, whereas on modern architectures like Hopper, the Tensor Memory Accelerator (TMA) can easily provide hardware predication. Our software predication logic is similar to standard practice for such cases in CUTLASS 2.X GEMMs, and similarly less performant than predication for contiguous matrix layouts. We find this to 8Table 1: FP16 forward pass benchmark overview. We benchmark naive neighborhood attention ker- nels against our proposed GEMM and fused kernels in half precision, over a large set of problem sizes varying in batch size, spatial size, number of atten- tion heads, and dimensions per head, and over dif- ferent window sizes and dilation values. For every problem size, we also benchmarked self attention running with the xFormers FMHA (our baseline) and Flash Attention V2. NA Kernel % of problems matched or outperformed neighborhood attn self attn Naive GEMM Fused FMHA FAv2 1-dimensional neighborhood attention Naive - 1.7 % 0.0 % 21.8 % 8.8 % GEMM 98.7 % - 0.0 % 72.0 % 54.2 % Fused 100.0 % 100.0 % - 100.0 % 98.2 % 2-dimensional neighborhood attention Naive - 16.4 % 0.0 % 32.9 % 15.8 % GEMM 84.0 % - 0.0 % 59.3 % 29.8 % Fused 100.0 % 100.0 % - 98.6 % 92.4 % 3-dimensional neighborhood attention Naive - - 0.0 % 43.5 % 20.2 % Fused 100.0 % - - 97.3 % 87.0 % Table 2: FP32 forward pass benchmark overview. We benchmark naive neighborhood attention kernels against our proposed GEMM and fused kernels in full precision, over a large set of problem sizes varying in batch size, spa- tial size, number of attention heads, and dimen- sions per head, and over different window sizes and dilation values. For every problem size, we also benchmarked self attention running with the xFormers FMHA (our baseline). NA Kernel% of problems matched or outperformed neighborhood attn self attn Naive GEMM Fused FMHA 1-dimensional neighborhood attention Naive - 0.0 % 0.0 % 34.6 % GEMM 99.9 % - 37.7 % 98.4 % Fused 100.0 % 64.8 % - 99.9 % 2-dimensional neighborhood attention Naive - 11.7 % 5.4 % 52.0 % GEMM 89.5 % - 28.1 % 92.4 % Fused 96.0 % 74.0 % - 99.3 % 3-dimensional neighborhood attention Naive - - 0.0 % 61.1 % Fused 100.0 % - - 98.6 % be the most significant contributor to additional runtime in our fused kernels, when compared to the baseline fused self attention kernel, FMHA. However, FNA is perfectly capable of hiding this additional overhead in many cases, and only falls behind in cases close to self attention (window size is approximately the same as input size.) 2. The attention masking logic, which depends on corresponding query and context token coordinates, original layout, and window size, introduces additional indexing logic in order to map linear indices to coordinates (unlike in 1-D problems where the mapping is the identity function), and it gets more complicated with more dimensions. This, along with additional statements in the masking condition, contributes to runtime, and is expected to worsen with more dimensions. Together, these contribute to more serious register spilling than the original 1-D kernel. Despite these issues, we find that our fused kernels can still match or outperform our self attention baseline in approximately 100% of 1-D, 98.6% of 2-D, and 97.3% of 3-D problem sizes that we benchmarked. 4 Experiments We evaluate the performance of our proposed methods by measuring their runtime against existing kernels in NATTEN . Most use cases in NATTEN target naive CUDA kernels, with the exception of 2-D neighborhood attention with 32-dimensional attention heads. NATTEN implements tiled kernels for those cases for up to and including window size 13 × 13, and only for the QK operation. However, we treat all kernels in NATTEN as our baseline, and will refer to them as naive kernels. We use a fixed set of problem sizes that vary in batch size, spatial size, number of attention heads, and dimensions per attention head, and run them through every implementation on an NVIDIA A100 GPU and measure their runtime using CUDA events. We iterate through multiple neighborhood attention window sizes and dilation values for every problem size. A summary of these benchmarks is presented in Tab. 1 (FP16) and Tab. 2 (FP32). We find that our GEMM-based kernels can improve or match the naive runtime in approximately 99% of 1-D problems (of 6150), and 84% of 2-D problems (of 5676) in half precision, and approximately 100% of the 1-D problems and 96% of the 2-D problems in full precision. Note that over 40% of the 2-D problems target tiled kernels in NATTEN , which we find can sometimes outperform our GEMM-based kernels. Another point of disadvantage in the FP16/BF16 variants of our GEMM-based kernels is using LDGs in pipelined 9kernels, noted in Sec. 3.2. On the other hand, our fused kernels improve or match the naive runtime in approximately 100% of both 1-D (of 6150) and 3-D problems (of 2448) in both half precision and full precision, an 100% of 2-D problems in half precision, while only improving approximately 96% of 2-D problems in full precision. We also find that our fused kernels match or outperform our GEMM kernels in 100% of both 1-D and 2-D problems in half precision, while only doing so in approximately 65% of 1-D problems and 74% of 2-D problems in full precision, which is not very surprising given that full precision is typically more memory-bandwidth-bound. In both Tab. 1 and Tab. 2 we also inspect the percentage of problem sizes in which using our fused neighborhood attention kernel is outperformed by the FMHA kernel. This is only to inspect additional overhead caused by our implementation, which we expect to be more noticeable in 2-D and 3-D problems. Some of the overhead may be avoidable, but our takeaway is that it is unlikely to be fully avoidable, as pointed out in Sec. 3.6. We further present a breakdown of our benchmarks in Tab. 3, where we report the average, minimum, and maximum improvement observed from switching from naive to GEMM-based, naive to fused, and GEMM-based to fused kernels. GEMM-based kernels exhibit strong performance compared to both naive and fused kernels in full precision, where fused kernels only have a very minimal edge over unfused. GEMM-based kernels also outperform naive kernels in half precision, especially in cases where tiled kernels are not available. While the tiled kernels are sometimes the better choice, we note that they simply cannot generalize to all problem sizes as our GEMM-based kernels can, nor are they easily extensible. Table 3: Forward pass benchmark breakdown. Both GEMM-based and fused NA improve the baseline naive kernels on average. However, there exist cases in which naive kernels may be preferable to GEMM-based in both FP16 and FP32, but naive is rarely a good choice in half precision where both naive and GEMM are more memory bandwidth bound than fused. Dim GEMM over naive Fused over naive Fused over GEMM Average Min Max Average Min Max Average Min Max FP16 1-D ↑ 548 % ↓-53 % ↑3025 % ↑1759 % ↑ 60 % ↑11885 % ↑180 % ↑ 71 % ↑466 % 2-D ↑193 % ↓-57 % ↑862 % ↑ 958 % 0 % ↑7169 % ↑257 % ↑ 38 % ↑1199 % 3-D - - - ↑1135 % ↑ 118 % ↑5497 % - - - FP32 1-D ↑ 874 % ↓ -31 % ↑3565 % ↑ 978 % ↑ 13 % ↑4419 % ↑ 17 % ↓-54 % ↑136 % 2-D ↑ 386 % ↓ -43 % ↑1933 % ↑ 564 % ↓ -30 % ↑4043 % ↑ 43 % ↓-53 % ↑451 % 3-D - - - ↑ 712 % ↑ 25 % ↑3029 % - - - 5 Future work & Conclusion In this work, we formulated the neighborhood attention problem, and by extension multi-dimensional sliding window attention, which are inherently GEMV problems, as GEMM/GETT problems. Through this finding, we implemented extensible GEMM-based and fused CUDA kernels that implement neighborhood attention, which can significantly improve upon existing kernels in the NATTEN project. These kernels will not only speed up previously-proposed models based on neighborhood attention, but can also significantly enhance ongoing research efforts in this direction. In addition, our fused kernels are the most flexible in terms of parameterization, by supporting varying window sizes, dilation factors, and causal masking across different axes, which enable unique applications such as 3-D spatio-temporal attention with causal masking across time. They also enjoy a reduced memory footprint, and can avoid being bound by memory bandwidth at scale. Future directions in this area include but are not limited to: support for Context Parallelism (CP), implementations using more efficient predication (i.e. with the Hopper TMA), extension to more modern architectures (warp-specialized kernels in Hopper and Blackwell), extension to other AI accelerators, and better auto-tuning (or alternatives involving graph compilation). We’ve shown that multi-dimensional local attention can indeed serve as solutions for scaling future large-scale long-context architectures, when provided with suitable software infrastructure. We hope that this inspires more research into multi-dimensional attention, as deep learning systems continue to grow larger in both model and input size. 10Acknowledgements. We would like to thank NVIDIA and members of the CUTLASS project, in particular Haicheng Wu, for his valuable feedback and comments which led to the creation of GEMM-based NA. We also thank Meta xFormers team for developing FMHA, which is what our fused neighborhood attention kernels are based on. A. H. thanks Michael Isaev, Aditya Kane, and Kai Wang for their feedback on the paper. A. H. also thanks Bing Xu, Hao Lu, Michael Iovine, and Terry Chen for the invaluable learning experience while interning at HippoML, which helped accelerate the timeline of this project. This research was supported in part by National Science Foundation under Award #2427478 - CAREER Program, and by National Science Foundation and the Institute of Education Sciences, U.S. Department of Education under Award #2229873 - National AI Institute for Exceptional Education. This project was also partially supported by cyberinfrastructure resources and services provided by the Partnership for an Advanced Computing Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia, USA. References [1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [3] HippoML Blog. Petaflops inference era: 1 pflops attention, and preliminary end-to-end results. https: //medium.com/p/21f682cf2ed1, 2024. [4] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [5] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2023. [6] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory- efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2020. [8] Ali Hassani and Humphrey Shi. Dilated neighborhood attention transformer. arXiv preprint arXiv:2209.15001, 2022. [9] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [11] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2012. [13] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hackable transformer modelling library. https://github.com/facebookresearch/xformers, 2022. [14] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [15] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning (ICML), 2018. [16] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [17] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. [18] Markus N Rabe and Charles Staats. Self-attention does not need O(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [19] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 11[20] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 2024. [21] Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan Yan, Jack Kosaian, Mark Hoemmen, Haicheng Wu, Andrew Kerr, et al. Cutlass, 2023. [22] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. [24] Steven Walton, Ali Hassani, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Stylenat: Giving each head a new perspective. arXiv preprint arXiv:2211.05770, 2022. [25] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful visual performance model for multicore architectures. Communications of the ACM, 52(4):65–76, 2009. [26] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 12A Auto-tuner GEMM kernels are, among other settings, parameterized by their tiling shape. Multi-dimensional variants (2-D and 3-D neighborhood attention) can also be parameterized by their fine-grained tile sizes, introduced by our formulation. As mentioned earlier, a GEMM with row tile size 64 can be reinterpreted as a number of 2-D and 3-D tiles (i.e. x × y for all positive integers x and y where xy = 64, and x × y × z for all positive integers x, y, and z where xyz = 64.) As a result, selecting tiling sizes based on factors such as problem size, hardware, and environment can further decrease achievable runtime. We therefore implement a very simple auto-tuning method as a proof of concept. Auto-tuning creates and maintains a cache for the lifetime of the application, which maps problems (defined by problem size, data type, and other such factors) to a tiling configuration. On a cache miss, the problem is benchmarked over a set of tiling configurations, and the best configuration gets cached. While the auto-tuner can noticeably improve performance even further, we note that it is presently limited in the following: 1. Distributed training. auto-tuner context is limited to a single process, meaning jobs involv- ing distributed training or inference will run the auto-tuner separately in each individual process. Aside from the possibility of different processes choosing different settings, which can slightly impact numerical determinism, this behavior is counter-intuitive. A more advanced auto-tuner would distribute possible settings over available processes and reduce auto-tuning time in the process, and guarantee the same settings across processes. 2. Vast search space. there exist in the order of thousands of valid settings for any given problem size, and searching over all of them is intractable. Our solution so far has been to generate far fewer possible settings, and even reduce the number of settings further by introducing a “thorough mode”, which is disabled by default, but when enabled, will allow users to search over more settings and potentially gain more in speed. This issue is a common problem in modern computational packages, and we hope to alleviate it by common practices such as reducing benchmark time, distributing the process, caching to disk, lazy benchmarking, and approximate performance models. B Additional experiments Herein we present some additional performance metrics from our GEMM-based and fused kernels compared against the baseline. In Tab. 4, we break down expected performance improvements at the operation level from a single for- ward and backward pass. Both our GEMM-based and fused kernels provide significant improvement on average over the baseline, while there still exist cases where naive could potentially perform better, especially compared to our GEMM-based kernels. As pointed out in Sec. 3.2, the scatter and gather operation in our GEMM kernels are a significant bottleneck, especially in lower-precision and in NN and IN operations. In lower precision, NN and IN, which account for 75% of the backward pass operations (excluding softmax) will fail to hide their prefetch runtime from global reads, which are not asynchronous, and this will essentially impact the backward pass more than it does the forward pass. This issue, however, is limited to our unfused variant, and our fused kernels maintain their superior performance levels, offering up to an order of magnitude improvement in all variants (1-D, 2-D, and 3-D). Table 4: Forward + backward pass benchmark breakdown. Improvements over naive, while not as significant as in the forward pass, are still significant. We report benchmark the full forward and backward pass in half precision only, because most training is done in lower precision. Dim GEMM over naive Fused over naive Fused over GEMM Average Min Max Average Min Max Average Min Max 1-D ↑ 502 % ↓-48 % ↑3017 % ↑ 844 % ↓ -20 % ↑7605 % ↑57 % ↓-50 % ↑229 % 2-D ↑ 92 % ↓-70 % ↑474 % ↑ 385 % ↓ -61 % ↑3723 % ↑150 % ↓-49 % ↑855 % 3-D - - - ↑ 447 % ↓ -45 % ↑2824 % - - - In addition to our operation-level benchmarks, we also evaluate the effect of our proposed methodol- ogy on existing models that use neighborhood attention as a primitive, NAT [9] and DiNAT [8]. We 13benchmark the throughput from all variants according to ImageNet-1K [12] specifications, and report FP16 and FP32 measurements in Tab. 5 and Tab. 6 respectively. We also benchmark a style-based generative adversarial (GAN) model based on neighborhood attention, StyleNAT [24] and report performance improvements in Tab. 7. We find that at least in problem sizes that the ImageNet classification models NAT and DiNAT typically require, which are typically smaller in spatial size and window size, and larger in batch size, our GEMM-based approach fails to improve the baseline in half precision, and only minimally improves it in full precision. Our fused kernels on the other hand never fail to improve upon the baseline, but they only provide significant improvement in half precision, and cases that use dilation frequently (DiNAT [8] variants). Improvements in the generative model, StyleGAN [24], are only observed in full precision (half precision is not recommended in this application), where again we find that both our GEMM-based and fused kernels can improve inference speed compared to existing naive kernels, with our fused kernels having a much more noticeable edge. Table 5: Model-level throughput changes when using our proposed GEMM-based and fused kernels in ImageNet classification. Hierarchical vision transformers NAT and DiNAT can see between 26% to 104% improvement in FP16 throughput on an A100 (batch size 128) with our proposed fused kernel. Suffering from the memory alignment issue, our half precision GEMM kernels usually result in a much smaller improvement over naive kernels, particularly the tiled variants. The same measurements with FP32 precision are presented in Tab. 6. Model # of FLOPs Throughput Top-1 Params Naive GEMM Fused Accuracy (M) (G) (imgs/sec) (%) NAT-M 20 2.7 2975 2660 ( ↓ -11 % ) 3742 ( ↑ 26 % ) 81.8 DiNAT-M 20 2.7 2672 2548 ( ↓ -5 % ) 3930 ( ↑ 47 % ) 81.8 DiNATs-T 28 4.5 2850 2504 ( ↓ -12 % ) 3847 ( ↑ 35 % ) 81.8 NAT-T 28 4.3 2167 1939 ( ↓ -11 % ) 2772 ( ↑ 28 % ) 83.2 DiNAT-T 28 4.3 1910 1845 ( ↓ -3 % ) 2909 ( ↑ 52 % ) 82.7 DiNATs-S 50 8.7 1800 1571 ( ↓ -13 % ) 2445 ( ↑ 36 % ) 83.5 NAT-S 51 7.8 1457 1309 ( ↓ -10 % ) 1879 ( ↑ 29 % ) 83.7 DiNAT-S 51 7.8 1360 1313 ( ↓ -3 % ) 2145 ( ↑ 58 % ) 83.8 DiNATs-B 88 15.4 1351 1178 ( ↓ -13 % ) 1837 ( ↑ 36 % ) 83.8 NAT-B 90 13.7 1110 997 ( ↓ -10 % ) 1448 ( ↑ 30 % ) 84.3 DiNAT-B 90 13.7 982 950 ( ↓ -3 % ) 1517 ( ↑ 54 % ) 84.4 DiNATs-L 197 34.5 846 744 ( ↓ -12 % ) 1119 ( ↑ 32 % ) 86.5 DiNAT-L 200 30.6 669 647 ( ↓ -3 % ) 1042 ( ↑ 56 % ) 86.6 DiNATs-L(384 × 384) 197 101.5 295 239 ( ↓ -19 % ) 391 ( ↑ 33 % ) 87.4 DiNAT-L(384 × 384) 200 92.4 153 134 ( ↓ -12 % ) 312 ( ↑ 104 % ) 87.5 Finally, we also attempted to estimate improvements in training time compared to our baseline. As suggested by our earlier findings regarding the limit of our GEMM-based implementation in the backward pass, we do not see any improvement in training time compared to the naive baseline. However, we find that our fused kernels deliver on the promise of improved half precision training time. We present our estimates in Tab. 8, which are based on measurements from training NAT [9] and DiNAT [8] variants according to their original specifications. We ran each model for 1 warmup epoch, and 1 benchmark epoch, the average throughput of which is used to estimate training time for 300 epochs. 14Table 6: Model-level throughput changes when using our proposed GEMM-based and fused kernels in ImageNet classification (full precision). While fused attention kernels are not expected to have as large of an edge over BMM-style attention kernels in FP32, our fused kernels still happen to outperform naive kernels in full precision. It is also visible that our GEMM kernels can outperform naive kernels when we eliminate the memory alignment issue. That said, our FP32 GEMM kernels still impose a maximum alignment of 1 element on the attention weights tensor, which limits its ability to compete with other BMM-style attention kernels. Model # of FLOPs Throughput Top-1 Params Naive GEMM Fused Accuracy (M) (G) (imgs/sec) (%) NAT-M 20 2.7 2416 2481 ( ↑ 3 % ) 2658 ( ↑ 10 % ) 81.8 DiNAT-M 20 2.7 2217 2364 ( ↑ 7 % ) 2905 ( ↑ 31 % ) 81.8 DiNATs-T 28 4.5 2270 2255 ( ↓ -1 % ) 2771 ( ↑ 22 % ) 81.8 NAT-T 28 4.3 1739 1802 ( ↑ 4 % ) 1942 ( ↑ 12 % ) 83.2 DiNAT-T 28 4.3 1591 1706 ( ↑ 7 % ) 2123 ( ↑ 33 % ) 82.7 DiNATs-S 50 8.7 1403 1393 ( ↓ -1 % ) 1717 ( ↑ 22 % ) 83.5 NAT-S 51 7.8 1160 1199 ( ↑ 3 % ) 1293 ( ↑ 11 % ) 83.7 DiNAT-S 51 7.8 1102 1183 ( ↑ 7 % ) 1490 ( ↑ 35 % ) 83.8 DiNATs-B 88 15.4 1020 1009 ( ↓ -1 % ) 1240 ( ↑ 22 % ) 83.8 NAT-B 90 13.7 867 897 ( ↑ 3 % ) 966 ( ↑ 11 % ) 84.3 DiNAT-B 90 13.7 795 851 ( ↑ 7 % ) 1059 ( ↑ 33 % ) 84.4 DiNATs-L 197 34.5 609 601 ( ↓ -1 % ) 721 ( ↑ 18 % ) 86.5 DiNAT-L 200 30.6 506 540 ( ↑ 7 % ) 669 ( ↑ 32 % ) 86.6 DiNATs-L(384 × 384) 197 101.5 211 193 ( ↓ -9 % ) 245 ( ↑ 16 % ) 87.4 DiNAT-L(384 × 384) 200 92.4 116 115 ( ↓ -1 % ) 179 ( ↑ 54 % ) 87.5 Table 7: Model-level throughput changes when using our proposed GEMM-based and fused kernels in style-based image generation. We benchmark StyleNAT [24], a style-based generative adversarial model based on neighborhood attention under different kernels. We experimented with different batch sizes in order to achieve peak performance, and settled for 64 for the 256 × 256 variant, and 8 for the 1024 × 1024. StyleNAT does not recommend lower-precision, therefore these measurements are only done in FP32. Dataset # of Throughput FID Params Naive GEMM Fused (imgs/sec) FFHQ (256 × 256) 48.9 M 36.7 40.6 ( ↑ 11 % ) 45.5 ( ↑ 24 % ) 2.05 FFHQ (1024 × 1024) 49.4 M 8.2 8.5 ( ↑ 3 % ) 11.5 ( ↑ 40 % ) 4.17 15Table 8: Training time improvement when using fused neighborhood attention kernels. We ran each of the classification models based on neighborhood attention for one warmup epoch and one benchmark epoch, all with half precision (the typical training scenario), and report the estimated training time. Note that these numbers exclude positional biases, as our fused backward kernel does not support it. Model # of FLOPs Training time estimate Params Naive GEMM Fused (M) (G) (hours) NAT-M 20 2.7 19.4 20.4 ( ↓ -5 % ) 16.6 ( ↑ 17 % ) DiNAT-M 20 2.7 20.4 21.2 ( ↓ -4 % ) 17.4 ( ↑ 17 % ) DiNATs-T 28 4.5 21.1 22.0 ( ↓ -4 % ) 17.4 ( ↑ 21 % ) NAT-T 28 4.3 26.5 28.2 ( ↓ -6 % ) 24.0 ( ↑ 10 % ) DiNAT-T 28 4.3 27.4 28.5 ( ↓ -4 % ) 21.9 ( ↑ 25 % ) DiNATs-S 50 8.7 33.3 33.2 ( 0 % ) 25.1 ( ↑ 33 % ) NAT-S 51 7.8 39.2 41.8 ( ↓ -6 % ) 33.7 ( ↑ 16 % ) DiNAT-S 51 7.8 38.0 40.1 ( ↓ -5 % ) 30.8 ( ↑ 23 % ) DiNATs-B 88 15.4 45.4 46.1 ( ↓ -2 % ) 32.6 ( ↑ 39 % ) NAT-B 90 13.7 51.1 54.6 ( ↓ -6 % ) 47.7 ( ↑ 7 % ) DiNAT-B 90 13.7 54.4 56.0 ( ↓ -3 % ) 41.0 ( ↑ 33 % ) 16",
      "meta_data": {
        "arxiv_id": "2403.04690v3",
        "authors": [
          "Ali Hassani",
          "Wen-Mei Hwu",
          "Humphrey Shi"
        ],
        "published_date": "2024-03-07T17:35:58Z",
        "pdf_url": "https://arxiv.org/pdf/2403.04690v3.pdf"
      }
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "$\\rm A^2Q$: Aggregation-Aware Quantization for Graph Neural Networks",
      "abstract": "As graph data size increases, the vast latency and memory consumption during\ninference pose a significant challenge to the real-world deployment of Graph\nNeural Networks (GNNs). While quantization is a powerful approach to reducing\nGNNs complexity, most previous works on GNNs quantization fail to exploit the\nunique characteristics of GNNs, suffering from severe accuracy degradation.\nThrough an in-depth analysis of the topology of GNNs, we observe that the\ntopology of the graph leads to significant differences between nodes, and most\nof the nodes in a graph appear to have a small aggregation value. Motivated by\nthis, in this paper, we propose the Aggregation-Aware mixed-precision\nQuantization ($\\rm A^2Q$) for GNNs, where an appropriate bitwidth is\nautomatically learned and assigned to each node in the graph. To mitigate the\nvanishing gradient problem caused by sparse connections between nodes, we\npropose a Local Gradient method to serve the quantization error of the node\nfeatures as the supervision during training. We also develop a Nearest Neighbor\nStrategy to deal with the generalization on unseen graphs. Extensive\nexperiments on eight public node-level and graph-level datasets demonstrate the\ngenerality and robustness of our proposed method. Compared to the FP32 models,\nour method can achieve up to a 18.6x (i.e., 1.70bit) compression ratio with\nnegligible accuracy degradation. Morever, compared to the state-of-the-art\nquantization method, our method can achieve up to 11.4\\% and 9.5\\% accuracy\nimprovements on the node-level and graph-level tasks, respectively, and up to\n2x speedup on a dedicated hardware accelerator.",
      "full_text": "arXiv:2302.00193v1  [cs.LG]  1 Feb 2023 Published as a conference paper at ICLR 2023 A2Q: A G G R E G AT I O N-AW A R E QUA N T I Z AT IO N F O R GR A P H NE U R A L NE T WO R K S Zeyu Zhu1, 2 Fanrong Li2 Zitao Mo2 Qinghao Hu2 Gang Li3 Zejian Liu2 Xiaoyao Liang3 Jian Cheng2∗ 1School of Future T echnology, University of Chinese Academy of Sciences 2Institute of Automation, Chinese Academy of Sciences 3Shanghai Jiao T ong University {zhuzeyu2021, lifanrong2017, mozitao2017}@ia.ac.cn, {huqinghao2014, liuzejian2018}@ia.ac.cn, {gliaca}@sjtu.edu.cn {liang-xy}@cs.sjtu.edu.cn {jcheng}@nlpr.ia.ac.cn ABSTRACT As graph data size increases, the vast latency and memory con sumption during in- ference pose a signiﬁcant challenge to the real-world deplo yment of Graph Neural Networks (GNNs). While quantization is a powerful approach to reducing GNNs complexity, most previous works on GNNs quantization fail t o exploit the unique characteristics of GNNs, suffering from severe accuracy de gradation. Through an in-depth analysis of the topology of GNNs, we observe that the topology of the graph leads to signiﬁcant differences between nodes, an d most of the nodes in a graph appear to have a small aggregation value. Motivate d by this, in this paper, we propose the Aggregation-A ware mixed-precision Q uantization ( A2Q) for GNNs, where an appropriate bitwidth is automatically le arned and assigned to each node in the graph. T o mitigate the vanishing gradient problem caused by sparse connections between nodes, we propose a Local Gradie nt method to serve the quantization error of the node features as the supervisi on during training. W e also develop a Nearest Neighbor Strategy to deal with the gen eralization on unseen graphs. Extensive experiments on eight public node-level a nd graph-level datasets demonstrate the generality and robustness of our proposed m ethod. Compared to the FP32 models, our method can achieve up to a 18.6x (i.e., 1. 70bit) compression ratio with negligible accuracy degradation. Morever, comp ared to the state-of-the- art quantization method, our method can achieve up to 11.4% a nd 9.5% accuracy improvements on the node-level and graph-level tasks, resp ectively, and up to 2x speedup on a dedicated hardware accelerator. 1 I NTRODUC TI ON Recently, Graph Neural Networks (GNNs) have attracted much attention due to their superior learn- ing and representing ability for non-Euclidean geometric d ata. A number of GNNs have been widely used in real-world applications, such as recommendation sy stem (Jin et al., 2020), and social net- work analysis (Lerer et al., 2019), etc. Many of these tasks p ut forward high requirements for low- latency inference. However, the real-world graphs are ofte n extremely large and irregular, such as Reddit with 232,965 nodes, which needs 19G ﬂoating-point op erations (FLOPs) to be processed by a 2-layer Graph Convolutional Network (GCN) with only 81KB pa rameters (T ailor et al., 2020), while ResNet-50, a 50-layer DNN, only takes 8G FLOPs to process an i mage (Canziani et al., 2016). What is worse, it requires a huge amount of memory access for GNNs i nference, e.g., the nodes features size of Reddit is up to 534MB, leading to high latency. Theref ore, the aforementioned problems pose a challenge to realize efﬁcient inference of GNNs. Neural network quantization can reduce the model size and accelerate inference without modify- ing the model architecture, which has become a promising met hod to solve this problem in re- ∗ Corresponding author 1Published as a conference paper at ICLR 2023 /uni0000003e/uni00000014/uni0000000f/uni00000014/uni00000013/uni00000040/uni0000003e/uni00000014/uni00000014/uni0000000f/uni00000015/uni00000013/uni00000040/uni0000003e/uni00000015/uni00000014/uni0000000f/uni00000016/uni00000013/uni00000040/uni0000003e/uni00000016/uni00000014/uni0000000f/uni00000017/uni00000013/uni00000040/uni0000003e/uni00000017/uni00000014/uni0000000f/uni00000014/uni00000019/uni0000001b/uni00000040 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni0000000b/uni00000024/uni00000049/uni00000057/uni00000048/uni00000055/uni00000003/uni00000024/uni0000004a/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c /uni0000002a/uni00000026/uni00000031 /uni0000002a/uni0000002c/uni00000031 /uni0000002a/uni00000024/uni00000037 (a) /uni0000003e/uni00000014/uni0000000f/uni00000016/uni00000013/uni00000040/uni0000003e/uni00000016/uni00000014/uni0000000f/uni0000001a/uni00000013/uni00000040/uni0000003e/uni0000001a/uni00000014/uni0000000f/uni00000014/uni00000014/uni00000013/uni00000040/uni0000003e/uni00000014/uni00000014/uni00000014/uni0000000f/uni00000014/uni00000018/uni00000013/uni00000040/uni0000003e/uni00000014/uni00000018/uni00000013/uni0000000f/uni00000015/uni00000013/uni00000013/uni00000040 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni0000000b/uni00000024/uni00000049/uni00000057/uni00000048/uni00000055/uni00000003/uni00000024/uni0000004a/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c /uni0000002a/uni0000002c/uni00000031/uni00000042/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000015 /uni0000002a/uni0000002c/uni00000031/uni00000042/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000016 /uni0000002a/uni0000002c/uni00000031/uni00000042/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000017 /uni0000002a/uni0000002c/uni00000031/uni00000042/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000018 (b) Figure 1: The analysis of the average aggerated node feature s in different in-degrees node groups on various tasks. (a) The values at the ﬁnal layer for GNNs train ed on Cora. (b) The values at the 2-5 layer of GIN trained on REDDIT -BINAR Y . The average values ar e all generated from 10 runs. cent years. Unfortunately, there remain some issues in the e xisting works on GNNs quantization. Feng et al. (2020) only quantizes the node feature and keeps ﬂ oating point calculations during infer- ence. T ailor et al. (2020) proposes a degree-quant training strategy to quantize GNNs to the low-bit ﬁxed point but causes a large accuracy drop, e.g., 11.1% accu racy drops when quantizing to 4bits. Moreover, some works (W ang et al., 2021b; Bahri et al., 2021; W ang et al., 2021a; Jing et al., 2021) quantize GNNs into 1-bit and compute with XNOR and bit count o perations. However, these 1-bit quantization methods are either restricted to the node-lev el tasks or can not generalize well to other GNNs. Most of the above methods do not make full use of the property o f GNNs and graph data, re- sulting in severe accuracy degradation or poor generalizat ion. As presented in MPNN framework (Gilmer et al., 2017), GNNs processing is divided into two ph ase: First, in the aggregation phase, a node collects information from neighboring nodes and uses the aggregation function to generate hidden features; second, in the update phase, the hidden fea tures are transformed into new features by an update function. W e analyze the nodes features after ag gregation in Figure 1 and ﬁnd that the higher the in-degree is, the larger the node features ten d to be after aggregation. And the fea- tures vary signiﬁcantly between nodes with different in-de grees, which represent the topology of a graph. Moreover, according to Xie et al. (2014); Aiello et al . (2001), the degrees of nodes in most real-world graph data often follow the power-law distribut ion, i.e., nodes with a low degree account for the majority of graph data. Therefore, specially quanti zing the nodes features according to the topology of the graphs will be beneﬁcial to reduce the quanti zation error while achieving a higher compression ratio. In this paper, we propose theAggregation-A ware Quantization(A2Q) method, which quantizes different nodes features with different learnable quantiz ation parameters, including bitwidth and step size. These parameters can be adaptively learned durin g training and are constrained by a penalty on memory size to improve the compression ratio. How ever, when quantizing the model in semi-supervised tasks, the gradients for most quantizat ion parameters are zero due to the sparse connections between nodes, which makes the training non-tr ivial. W e propose the Local Gradient method to solve this problem by introducing quantization er ror as supervised information. Finally, to generalize our method to unseen graphs in which the number of the nodes varies, we develop the Nearest Neighbor Strategy which assigns the learned quantization parameters to the un seen graph nodes. T o the best of our knowledge, we are the ﬁrst to introdu ce the mixed-precision quantization to the GNNs. Compared with the previous works, our proposed m ethods can signiﬁcantly compress GNNs with negligible accuracy drop. In summary, the key contributions of this paper are as follows: 1) W e propose the Aggregation-A ware mixed-precision Quant ization ( A2Q) method to enable an adaptive learning of quantization parameters. Our learn ing method is powerful by fully 2Published as a conference paper at ICLR 2023 utilizing the characteristic of GNNs, and the learned bitwi dth is strongly related to the topology of the graph. 2) A Local Gradient method is proposed to train the quantization parameters in s emi- supervised learning tasks. Furthermore, to generalize our method to the unseen graphs in which the number of input nodes is variable, we develop the Nearest Neighbor Strategy to select quantization parameters for the nodes of the unsee n graphs. 3) Experiments demonstrate that we can achieve a compressio n ratio up to 18.6x with negli- gible accuracy degradation compared to the full-precision (FP32) models. Moreover, the model trained with our A2Q method outperforms the state-of-the-art (SOT A) method up to 11.4% with a speedup up to 2.00x in semi-supervised tasks, and obtains up to 9.5% gains with a 1.16x speedup in graph-level tasks. W e provide o ur code at this URL: https://github.com/weihai-98/A2Q. 2 R ELATED WORK Graph Neural Networks: The concept of the graph neural network was ﬁrst proposed in Scarselli et al. (2008), which attempted to generalize neur al networks to model non-Euclidean data. In the following years, various GNN models were proposed. Fo r example, Graph Convolution Net- work (GCN) (Kipf & W elling, 2016) uses a layer-wise propagat ion rule that is based on a ﬁrst-order approximation of spectral convolutions on graphs, Graph Is omorphism Network (GIN) (Xu et al., 2018) designed a provably maximally powerful GNN under the M PNN framework, and Graph At- tention Network (GA T) (V eliˇ ckovi´ c et al., 2017) introduc es the attention mechanism to graph pro- cessing. Although GNNs have encouraging performance in a wi de range of domains (Jin et al., 2020; Y ang, 2019), the huge amount of ﬂoat-point operations and memory access in process pose a challenge to efﬁcient inference, which hinder the applicat ions of GNNs. Quantized GNNs: As a promising method to reduce the model size and accelerate the inference process, quantization is also applied to GNNs. Some works qu antize features and weights in GNNs to low bitwidths (Feng et al., 2020; T ailor et al., 2020) or ev en 1-bit (W ang et al., 2021b; Bahri et al., 2021; W ang et al., 2021a; Jing et al., 2021), i.e., use ﬁxed-p oint numbers instead of ﬂoating-point numbers for computation. But when the compression ratio is h igh (e.g., <4bit), the performance degradation of these works is signiﬁcant, and the generaliz ation of 1-bit method is limited. There are also some works on vector quantization (VQ), which use th e vectors in a codebook obtained during the training process instead of the original feature s (Ding et al., 2021; Huang et al., 2022). However, searching for vectors in the codebook is computati onally complex. Mixed-Precision Quantization: Based on the idea that different layers have different sensi tiv- ities to quantization, mixed-precision quantization is pr oposed in CNNs to quantize different layers to different bitwidths for better model compression. Early works (W ang et al., 2019; Lou et al., 2019) proposed reinforcement learning (RL) based methods t o search bitwidth for different lay- ers, but they often require large computational resources, which limits the exploration of the search space. Another important class of mixed-precision method i s the criteria-based method, they use the speciﬁc criteria to represent the quantization sensitivit y, e.g., (Dong et al., 2019; 2020; Chen et al., 2021)quantize different layers with different bitwidths b ased on the trace of the Hessian. Recently, there are some other methods to learn the bitwidth during tra ining (Uhlich et al., 2019; Esser et al., 2019; Jain et al., 2020). However, due to the huge difference between GNNs and CNNs, it is dif- ﬁcult to use these methods on GNNs directly, and our A2Q is the ﬁrst method to introduce the mixed-precision quantization to GNNs, further improving t he inference efﬁciency of GNNs. 3 M ETHOD In this section, we describe our proposed Aggregation-A war e Quantization in detail. Firstly, we present the formulation of the mixed-precision quantizati on for GNNs, which fully utilizes the prop- erty of GNNs and graph data. Secondly, we introduce the Local Gradient method to address the gradient vanishing problem during training. Finally, we de tail the Nearest Neighbor Strategy, which is used for generalizing our approach to the unseen graphs. 3Published as a conference paper at ICLR 2023  !\"\"  !\"#  !\"$   !#\"  !##  !#$   !$\"  !$#  !$$   !%\"  !%#  !%$   !&\"  !&#  !&$  !'\"  !'#  !'$ (\" (# ($ (% (& (' ) *\"\" ) *\"# ) *#\" ) *## ) *$\" ) *$# +\" ´ = ,\"\" ,\"# ,#\" ,## ,$\" ,$# ,%\" ,%# ,&\" ,&# ,'\" ,'# (\"-\" (\"-# (#-\" (#-# ($-\" ($-# (%-\" (%-# (&-\" (&-# ('-\" ('-# Nodes Features Weights   ! N F2 F Figure 2: Perform matrix multiplication by the integer represented.¯x and ¯w are both integers. Figure 3: The gradients to xq in GCN trained on Cora by sampling 400 nodes. 3.1 A G G RE G AT IO N -AWA RE QUA N T IZ AT IO N W e assume a graph data with N nodes and the node features are F -dimensional, i.e., the feature map is X ∈ RN×F and xi is the features of node i. W e use the learnable parameters step size α i ∈ R+ and bitwidth bi ∈ R+ to quantize the features of the i-th node as: ¯xi = sign(xi)      ⌊|xi| α i + 0. 5⌋, |x| < α i(2[bi]−1 − 1) 2[bi]−1 − 1, |xi| ≥ α i(2[bi]−1 − 1) , (1) where ⌊·⌋ is the ﬂoor function, and [·] is the round function to ensure the bitwidth used to quantize is an integer. The learnable parameters are sX = ( α 1, α 2, ..., α N ), and bX = ( b1, b 2, ..., b N ). Then we can obtain the ﬁxed-point feature map ¯X, and the original feature can be represented as Xq = SX · ¯X, where SX = diag(α 1, α 2, ..., α N ). Note that we use [b] + 1 as the quantization bitwidth for the features after ReLU because the values are a ll non-negative. In the update phase, the node features are often transformed with a linear mapping or an MLP in which matrix multiplication XW is the main computation, and the transformed node features a re the input to the next layer in GNNs. In order to accelerate the update phase, we also quantize W . Due to the fact that W in a certain layer is shared by all nodes, we quantize W to the same bitwidth of 4bits for all GNNs in this paper. However, each column of W has its learnable quantization step size, i.e., sW = ( β1, β 2, .., β F2 ), where F2 is the output-dimension of the node features in current layer and βi is the quantization step size for the i-th column of W , and we also use Eq. 1 to quantize W . W e can obtain the integer representation ¯W and the quantized representation Wq = ¯W · SW , where SW = diag(β1, β 2, ..., β F2 ). The ﬂoat-point matrix multiplication in the update phase c an be reformulated as follow: X · W ≈ Xq · Wq = ( SX · ¯X) · ( ¯W · SW ) = ( ¯X · ¯W ) ⊙ (sX ⊗ sW ) , (2) where ⊙ denotes an element-wise multiplication, and ⊗ denotes the outer product. After training, we can obtain sX and sW so that the outer product can be pre-processed before infere nce. An example is illustrated in Figure 2. For the aggregation phas e, i.e., AX, A is the adjacency matrix and A ∈ { 0, 1}N×N , we quantize the X as the quantization way of W because the nodes features involved in the aggregation process come from the update pha se, in which the features lose the topology information of graphs. Then the aggregation phase can be performed by integer operations to reduce the computational overhead. The quantization parameters(s, b ) are trained by the backpropagation algorithm. Since the ﬂoo r and round functions used in the quantization process are not differentiable, we use the straight- through estimator (Bengio et al., 2013) to approximate the g radient through these functions, and the gradients of the quantization parameters can be calculated by: ∂L ∂s = d∑ i=1 ∂L ∂xi q · ∂xi q ∂s , (3) ∂L ∂b = d∑ i=1 ∂L ∂xi q · ∂xi q ∂b , (4) where d is the dimension of the vector x, (s, b ) are the quantization parameters for x, and xi q is the value of i-th dimension in xq. Detailed information about quantization process and the backpropagation are shown in Appendix A.1 and A.3 Proof 2 and 3. 4Published as a conference paper at ICLR 2023 In order to improve the compression ratio of the node feature s, we introduce a penalty term on the memory size: Lmemory = ( 1 η · L∑ l=1 N∑ i=1 diml ·bl i− Mtarget )2 , (5) where L is the number of layers in the GNNs, N is the total number of nodes, diml is the length of the node features in l-th layer, bl iis the quantization bitwidth for node i in l-th layer, Mtarget is the target memory size on the total node features memory size, an d η = 8 ∗ 1024, which is a constant to convert the unit of memory size to KB. Then the model and quantization parameters can be trained by the loss function: Ltotal = Ltask + λ · Lmemory , (6) where Ltask is the task-related loss function and λ is a penalty factor on Lmemory . 3.2 L O CA L GRA D IE N T Although the above end-to-end learning method is concise an d straightforward, the gradients for the quantization parameters of nodes features, i.e., ∂L task ∂s and ∂L task ∂b , are almost zero during the training process of semi-supervised tasks, which poses a si gniﬁcant challenge to train the quantiza- tion parameters for nodes features. W e analyze the property of GNNs and graph data, and ﬁnd that two reasons lead to this phenomenon: 1. The extreme sparsity of the connections between nodes in graph data. 2. Only a tiny fraction of nodes with labels are used for training in semi-supervised tasks (e.g., 0. 30% in PubMed dataset). Therefore, ∂L task ∂x q for most node features are zero (detailed proof in Appendix A.3.2), which results in that the gradient s for quantization parameters of these nodes vanish according to Eq. 3 and Eq. 4. T o clarify, we visua lize the ∂L task ∂x q in the second layer of GCN trained on Cora. As shown in Figure 3, most gradients fo r the nodes features are zero. The gradients of the Ltask w .r.t. quantized nodes features can be viewed as the supervi sed infor- mation from the labeled nodes which enable the training of th e quantization parameters for nodes features. However, this supervised information is missing due to zero gradients. Considering the quantization error is related to the Ltask, we introduce the quantization error E = 1 d |xq − x|1 as the supervised information for the quantization parameter s of nodes features, where x is the features before quantization, xq is the features after quantization and | · | 1 denotes the L1 norm. W e refer to this method as Local Gradient because the gradients are computed by the local quantizatio n er- rors instead of back-propagated task-related gradients. T hen the quantization parameters for node features can be trained by gradients from E: ∂E ∂s = 1 d d∑ i=1 sign(xi q− xi) · ∂xi q ∂s , (7) ∂E ∂b = 1 d d∑ i=1 sign(xi q− xi) · ∂xi q ∂b . (8) Note that the quantization parameters of W are still trained by utilizing the gradients in Eq. 3. 3.3 N E A RE S T NE IG H BO R ST RAT E G Y In graph-level tasks, the quantized GNNs are required to gen eralize to unseen graphs. In such a scenario, the number of input nodes may vary during training or inference. However, the learnable method can only train a ﬁxed number of (s, b ) pairs which are the same as the number of input nodes, so it is challenging to learn the s and b for every node in graph-level tasks. T o solve this problem, we propose the Nearest Neighbor Strategy , which allows learning of a ﬁxed number of quantization parameters and select quantization paramete rs for the unseen graphs. The proposed strategy is shown in Algorithm 1. T o ensure the n umerical range of xq is as close as to x at FP32, a simple way is to keep the maximum quantization valu e equal to the maximum absolute value of x. Based on this idea, we ﬁrst initialize m groups of quantization parameters, then we calculate the maximum quantization value for every group , i.e., qmax = s(2[b]−1 − 1). When quantizing the features of node i, the feature with the largest absolute value fi in the node features xi is ﬁrst selected, and then we ﬁnd the nearest qmax and quantize the node features with the (s, b ) corresponding to this qmax. When performing backpropagation, we ﬁrst calculate the gr adients of the loss function w .r.t. quantization parameters accordin g to Eq. 3 and Eq. 4. For a speciﬁc set of quantization parameters (sj , b j), we collect the gradients from the nodes that have used them 5Published as a conference paper at ICLR 2023 Algorithm 1 Nearest Neighbor Strategy 1: ForwardPass (X = ( x1, x2, ..., xN )T ): 2: Initialize(s, b), s ∈ Rm×1 + , b ∈ Rm×1 + before training 3: Calculate qmax = s ⊙ (2b−1 − 1) 4: Calculate the maximum absolute value in the features of each node: fi = max j abs(x(j) i ) 5: Search the index of quantization parameters for each node: indexi = arg min k |fi − qk max| 6: Quantize the i-th node features using (sindexi , b indexi ) 7: return Xq 8: end T able 1: The results comparison on node-level tasks. The ave rage bits are counted for each task when the best results are achieved. Dataset Model Accuracy A verage bits Compression Ratio Spee dup Cora GCN(FP32) 81.5±0.7% 32 1x — GCN(DQ ) 78.3±1.7% 4 8x 1x GCN(ours)80.9±0.6% 1.70 18.6x 2.00x GA T(FP32) 83.1±0.4% 32 1x — GA T(DQ ) 71.2±2.9% 4 8x 1x GA T(ours)82.6±0.6% 2.03 15.4x 1.49x CiteSeer GCN(FP32) 71.1±0.7% 32 1x — GCN(DQ ) 66.9±2.4% 4 8x 1x GCN(ours)70.6±1.1% 1.87 17.0x 1.91x GIN(FP32) 66.1±0.9% 32 1x — GIN(DQ ) 60.8±2.1% 4 8x 1x GIN(ours)65.1±1.7% 2.54 12.6x 1.37x PubMed GA T(FP32) 79.0±0.3% 32 1x — GA T(DQ) 70.6±12.5% 4 8x 1x GA T(ours)78.8±0.4% 2.12 15.1x 1.38x ogbn-arxiv GCN(FP32) 71.7±0.3% 32 1x — GCN(DQ) 65.4±3.9% 4 8x 1x GCN(ours)71.1±0.3% 2.65 12.1x 1.28x and add these gradients together. After the model has been tr ained, we obtain the quantization parameters (s, b). Since qmax can be calculated and sorted in advance, searching the neare st qmax can be implemented by binary searching. Usually, we set m = 1000 for all graph-level tasks in our paper and the overhead introduced to inference time is negli gible. 4 E XPERIME NT S 4.1 E X P E RIM E N TA L SE T T IN G S In this section, we evaluate our method on three typical GNN m odels, i.e., GCN, GIN, and GA T . And we compare our method with the FP32 GNN model and DQ-INT4 ( T ailor et al., 2020) on eight datasets, including four node-level semi-learning t asks (Cora, CiteSeer, PubMed, ogbn-arxiv) (Hu et al., 2020; Y ang et al., 2016) and four graph-level task s (REDDIT -BINAR Y , MNIST , CI- F AR10, ZINC) (Y anardag & V ishwanathan, 2015; Dwivedi et al. , 2020), to demonstrate the gen- erality and robustness of our method. Among these datasets, ZINC is a dataset for regression tasks, which uses regression loss as the metric of the model perform ance, while others are all for classiﬁ- cation tasks. For a fair comparison, we set the quantization bitwidth ofW for all GNNs to 4bits as DQ-INT4. W e count the average bitwidths for nodes features in all layers of the overall model and list them in our 6Published as a conference paper at ICLR 2023 T able 2: The results comparison on graph-level tasks. Dataset Model Accuracy (Loss ↓ ) A verage bits Compression ratio Speedup MNIST GCN(FP32) 90.1±0.2% 32 1x — GCN(DQ) 84.4±1.3% 4 8x 1x GCN(ours)89.9±0.8% 3.50 9.12x 1.17x GIN(FP32) 96.4±0.4% 32 1x — GIN(DQ) 95.5±0.4% 4 8x 1x GIN(ours)95.7±0.2% 3.75 8.52x 1.07x CIF AR10 GCN(FP32) 55.9±0.4% 32 1x — GCN(DQ) 51.1±0.7% 4 8x 1x GCN(ours)52.5±0.8% 3.32 9.62x 1.25x GA T(FP32) 65.4±0.4% 32 1x — GA T(DQ) 56.5±0.6% 4 8x 1x GA T(ours)64.7±2.8% 3.73 8.57x 1.12x ZINC GCN(FP32) 0.450±0.008 32 1x — GCN(DQ) 0.536±0.011 4 8x 1x GCN(ours)0.492±0.056 3.68 8.68x 1.08x REDDIT - BINARY GIN(FP32) 92.2±2.3% 32 1x — GIN(DQ) 81.3±4.4% 4 8x 1x GIN(ours)90.8±1.8% 3.50 9.14x 1.16x results, denoted by “ A verage bits”. Since today’s CPUs and G PUs can not support mixed-precision operations well, we implement a precision-scalable hardwa re accelerator to perform the overall in- ference process for GNN. The accelerator employs massive bi t-serial multipliers Judd et al. (2016), therefore, the latency of the integer multiplications is de termined by the bitwidth of the node fea- tures. T o evaluate the performance gains of our method over D Q-INT4, we develop a cycle-accurate simulator for our accelerator. More details about accelera tor architecture are shown in Appendix A.7.5. Moreover, we show the compression ratio of quantized GNNs compared to the FP32 models in terms of overall memory size. For simplicity, we use GNN(D Q) to represent the GNNs quantized by DQ-INT4 and GNN-dataset to represent the task in which we r un the experiment, e.g., GCN- Cora represents the GCN model trained on Cora. Detailed info rmation about datasets and settings is in Appendix A.5 and Appendix A.6. 4.2 N O D E -L E V E L TA S K S T able 1 shows the experimental results on three GNN architec tures trained on four node-level datasets. Compared with DQ-INT4, our method can achieve sig niﬁcantly better accuracy on each task, even with a higher compression ratio, improving the in ference performance with 1.28x to 2.00x speedups. On almost all node-level tasks, our proposed A2Q has negligible accuracy drop compared to the FP32 baselines while achieving 12.1x-18.6x compress ion ratio. Since both GIN and GA T in- volve more complex computations, such as the calculation of attention coefﬁcients in GA T , it is more challenging to quantize those models, and DQ performs p oorly on these two models. How- ever, our method can overcome this problem and maintain comp arable accuracy compared with the FP32 models. Our method can outperform the DQ-INT4 by 11.4% o n the GA T -Cora task with a smaller bitwidth (2.03 v.s. 4). Even on ogbn-arxiv, which ha s a large number of nodes, A2Q can achieve a 12.1x compression ratio compared with FP32 baseli ne with comparable accuracy, which demonstrates the robustness of our method. Moreover, to dem onstrate the generality of our method, we also evaluate our method on heterogeneous graphs and the i nductive learning tasks and compare with more related works in Appendix A.7.1. 4.3 G RA P H -L E V E L TA S K S T able 2 presents the comparison results on the graph-level t asks. Our method can obtain better results on all tasks than DQ-INT4 with higher compression and a consi derable speedup. Especially on the GIN-REDDIT -BINAR Y task, our method outperforms DQ-INT4 by 9.5% while achieving a 1.16x 7Published as a conference paper at ICLR 2023 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000014/uni0000004e /uni00000015/uni0000004e /uni00000016/uni0000004e /uni00000019/uni0000001b/uni00000016 /uni00000015/uni00000016/uni0000001c/uni00000018 /uni00000015/uni00000017/uni00000019 /uni00000016/uni00000013 /uni00000013 (a) GCN-CiteSeer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000013 /uni00000014/uni0000004e /uni00000015/uni0000004e /uni00000016/uni0000004e /uni00000014/uni00000018 /uni00000016/uni00000018/uni00000018 /uni00000015/uni0000001a /uni00000014/uni0000001c/uni00000016/uni0000001b /uni0000001c/uni00000019/uni0000001b /uni00000015/uni00000016/uni00000014 (b) GIN-CiteSeer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000018 /uni00000015/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013 /uni00000014/uni0000004e /uni00000015/uni0000004e /uni00000016/uni0000004e /uni00000017/uni00000013/uni0000001c /uni00000015/uni0000001a/uni00000013/uni00000013 /uni00000015/uni00000014/uni0000001b /uni00000013 /uni00000013 /uni00000013 (c) GA T -CiteSeer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013/uni0000004e /uni00000015/uni00000013/uni0000004e /uni00000016/uni00000013/uni0000004e /uni00000017/uni00000013/uni0000004e /uni00000018/uni00000013/uni0000004e /uni00000013 /uni00000015/uni00000015/uni00000016/uni00000018/uni0000001b /uni00000015/uni0000001c/uni00000017/uni0000001a/uni00000013 /uni00000018/uni00000018/uni0000001c/uni00000017 /uni00000013 /uni00000013 (d) The ﬁrst layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000013 /uni00000014/uni00000013/uni0000004e /uni00000015/uni00000013/uni0000004e /uni00000016/uni00000013/uni0000004e /uni00000017/uni00000013/uni0000004e /uni00000018/uni00000013/uni0000004e /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013/uni00000016/uni00000013 /uni00000018/uni0000001a/uni00000015/uni00000018/uni00000016 /uni00000014/uni00000016/uni0000001c/uni00000013 (e) The second layer Figure 4: The relationship between quantized bitwidth and a verage in-degrees of nodes. (a), (b) and (c) represent the results of three GNN models trained on C iteSeer. (d) and (e) are results about the ﬁrst and the second layer of an MLP , which is the update fun ction of GIN trained on REDDIT - BINAR Y . The green bars represent the average in-degrees for the certain bitwidth used by nodes and the orange polylines represent the number of the nodes that u se this certain bitwidth. speedup. Even for graph datasets with similar in-degrees, s uch as MNIST and CIF AR10, our method also learns the appropriate bitwidths for higher compressi on ratio and better accuracy. Although on GIN-MINST task, the improvement of our method is relatively small due to the similarity of the in- degrees between different nodes, our method can achieve com parable accuracy with smaller bitwidth (3.75 v.s. 4). 4.4 A NA LY S IS T o understand why our approach works, we analyze the relatio nship between the learned bitwidths and the topology of the graph. Figure 4(a) and 4(b) reveal tha t the bitwidth learned by A2Q is strongly related to the topology of graph data in the node-le vel tasks. As the bitwidth increases, the average in-degrees of nodes become larger. In other word s, A2Q method tends to learn higher bitwidth for nodes with higher in-degrees. However, in GA T , as shown in Figure 4(c), the learned bits are irregular. This is because the features aggregated in GA T are topology-free. However, our method can still learn appropriate quantization bitwid ths for different nodes, which improves accuracy while reducing memory usage. In addition, Figure 4 also shows the node distribution for different bitwidths and the result is consistent with power -law distribution. Since nodes in graph data mainly have low in-degrees, most of the nodes are quantized t o low bitwidth ( ≤ 4), compressing the GNNs as much as possible. And there are also some high in-d egree nodes quantized to high bitwidth, which can help to maintain the accuracy of the GNN m odels. As a result, the average bitwidth of the entire graph features is low , and the accurac y degradation is negligible. For the graph-level tasks in which the number of nodes varies , our method is also aggregation- aware. W e select a layer of GIN trained on REDDIT -BINAR Y and a nalyze the relationship between bitwidth and average in-degrees of nodes using the correspo nding bitwidth to quantize in Figure 4(d) and 4(e). It can be seen that the bitwidth learned for nod es features input to the second layer of MLP , which is the update function in GIN for graph-level task s, does not present a correlation with the topology of graph. W e analyze the reason and ﬁnd that the n ode features before the second layer is the result mapped by the ﬁrst layer of MLP and is activated b y the activation function, e.g., ReLU, which results in the node features losing the topology infor mation. W e present more experiment results in Appendix A.7. to demonstrate that our method is ge nerally applicable. 5 A BLATION STUDY The advantage of learning-based mixed-precision quantiza tion: In Figure 5, we compare our A2Q with the manual mixed-precision method, which manually ass igns high-bit to those nodes with high in-degrees and low-bit to those nodes with low in-degre es. In the ﬁgure, the postﬁx “learn” denotes that using A2Q method, “manual” denotes that we assign bits to nodes and the model only learns the stepsize, and “mixed-precision” denotes that th e model uses the same quantization method as DQ-INT4 but assigning different bitwidths to nodes. For t he “mixed-precision”, we assign 5bits to those nodes with 50% top in-degrees and assign 3bits to oth ers. The implications are two-fold. First, compared with the DQ-INT4, which uses the same quanti zation bitwidth, the mixed-precision 8Published as a conference paper at ICLR 2023 T able 3: Ablation Study. Model Conﬁg Accuracy A verage bits GIN-Cora no-lr 33.7±4.1% 4 no-lr-b 75.6±0.2% 4 no-lr-s 56.1±4.9% 3.85 lr-all 77.8±1.6% 2.37 GCN- CiteSeer FP32 71.1±0.7% 32 Global 56.8±6.7% 3 Local 70.6±1.1% 1.87 /uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000025/uni0000004c/uni00000057/uni00000056 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000008/uni0000000c /uni00000019/uni00000014/uni00000011/uni00000019/uni00000008/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000008 /uni0000001a/uni0000001c/uni00000011/uni0000001c/uni00000008/uni0000001a/uni0000001b/uni00000011/uni0000001b/uni00000008 /uni00000015/uni00000011/uni00000016/uni00000008 /uni00000015/uni00000014/uni00000011/uni00000018/uni00000008 /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni00000010/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051 /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni00000010/uni00000050/uni00000044/uni00000051/uni00000058/uni00000044/uni0000004f /uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni00000010/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051 /uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni00000010/uni00000050/uni00000044/uni00000051/uni00000058/uni00000044/uni0000004f /uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni00000010/uni00000027/uni00000034/uni00000010/uni0000002c/uni00000031/uni00000037/uni00000017 /uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni00000010/uni00000050/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000053/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni00000010/uni00000027/uni00000034/uni00000010/uni0000002c/uni00000031/uni00000037/uni00000017 /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni00000010/uni00000050/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000053/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 Figure 5: The comparison between learning bitwidth and assign manually. method obtains 1.1% gains on GCN-Cora tasks demonstrating t hat the mixed-precision method is more effective. Second, the results of the learning metho d outperform the manual method on all tasks. Especially for the models with a high compression ratio, on GIN-CiteSeer task, learning method can achieve 21.5% higher accuracy. This demonstrate s that our learning method can perform better than the assignment method according to prior knowle dge for mixed-precision quantization of GNNs. The power of learning the quantization parameters:Ablations of two quantization parameters (s, b) on the GIN-Cora task are reported in the ﬁrst row of T able 3. Th e “no-lr” denotes that do not use learning method, “no-lr-b” denotes that only learn the s tep size s , “no-lr-s” denotes that only learn the bitwidths b, and “lr-all” denotes that learn the bitwidth and step size s imultaneously. W e can see that learning the step size can signiﬁcantly increas e the accuracy and even the “no-lr-bit” model can outperform the DQ-INT4 at the same compression rat io. When learning the bitwidth and step size simultaneously, the model can achieve higher accu racy with a higher compression ratio. This is because our method learns lower bitwidths for most no des with low in-degrees and higher bitwidths for a tiny fraction of nodes with high in-degrees, which can improve the compression ratio while achieving higher accuracy. Local Gradient v .s. Global Gradient:T o demonstrate the effectiveness of our Local Gradient method, we compare the models trained with and without it on t he GCN-CiteSeer task in the last row of T able 3. The “Global” denotes that the model is trained with Eq. 3 and Eq. 4. The model trained with the local method outperforms the global method by 13.8% with a higher compression ratio. This is because the Local Gradient method can learn qu antization parameters for all nodes, while only quantization parameters for a part of nodes can be updated with the Global Gradient method due to the extreme sparse connection in the graph on th e node-level semi-supervised tasks. The overhead of Nearest Neighbor Strategy:W e evaluate the real inference time of the GIN model on the 2080ti GPU. On REDDIT -BINAR Y task, the model without t he selection process requires 121.45ms, while it takes 122.60ms for the model with our Near est Neighbor Strategy, which only introduces 0.95% overhead. But with the help of the Nearest N eighbor Strategy, our model can obtain 19.3% accuracy gains for quantized GIN on REDDIT -BIN AR Y . 6 C ONCLUSIO N This paper proposes A2Q, an aggregation-aware mixed-precision quantization meth od for GNNs, and introduces the Local Gradient and Nearest Neighbor Stra tegy to generalize A2Q to the node- level and graph-level tasks, respectively. Our method can l earn the quantization parameters for different nodes by fully utilizing the property of GNNs and g raph data. The model quantized by our A2Q can achieve up to a 18.6x compression ratio, and the accuracy degradation is negligible compared with the FP32 baseline. Compared with the prior SOT A, DQ-INT4, our method can signiﬁcantly improve 11.4% accuracy with up to a 2.00x speed up on different tasks. Our work provides a general, robust and feasible solution to speed up the inference of GNNs. 9Published as a conference paper at ICLR 2023 REFERENC ES Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lu cchi, Pascal Fua, and Sabine S ¨ usstrunk. Slic superpixels compared to state-of-the-ar t superpixel methods. IEEE transactions on pattern analysis and machine intelligence , 34(11):2274–2282, 2012. William Aiello, Fan Chung, and Linyuan Lu. A random graph mod el for power law graphs. Exper- imental mathematics , 10(1):53–66, 2001. Mehdi Bahri, Ga ´ etan Bahl, and Stefanos Zafeiriou. Binary g raph neural networks. In Proceedings of the IEEE/CVF Conference on Computer V ision and P attern Re cognition, pp. 9492–9501, 2021. Rajeev Balasubramonian, Andrew B Kahng, Naveen Muralimano har, Ali Shaﬁee, and V aishnav Srinivas. Cacti 7: New tools for interconnect exploration i n innovative off-chip memories. ACM T ransactions on Architecture and Code Optimization (TACO) , 14(2):1–25, 2017. Y oshua Bengio, Nicholas L ´ eonard, and Aaron Courville. Est imating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013. John Brennan, Stephen Bonner, Amir Atapour-Abarghouei, Ph ilip T Jackson, Boguslaw Obara, and Andrew Stephen McGough. Not half bad: Exploring half-preci sion in graph convolutional neural networks. In 2020 IEEE International Conference on Big Data (Big Data) , pp. 2725–2734. IEEE, 2020. Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678 , 2016. W eihan Chen, Peisong W ang, and Jian Cheng. T owards mixed-pr ecision quantization of neural networks via constrained optimization. In Proceedings of the IEEE/CVF International Conference on Computer V ision , pp. 5350–5359, 2021. Mucong Ding, Kezhi Kong, Jingling Li, Chen Zhu, John Dickers on, Furong Huang, and T om Gold- stein. Vq-gnn: A universal framework to scale up graph neura l networks using vector quantiza- tion. Advances in Neural Information Processing Systems , 34:6733–6746, 2021. Zhen Dong, Zhewei Y ao, Amir Gholami, Michael W Mahoney, and K urt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precisio n. In Proceedings of the IEEE/CVF International Conference on Computer V ision , pp. 293–302, 2019. Zhen Dong, Zhewei Y ao, Daiyaan Arfeen, Amir Gholami, Michae l W Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neu ral networks. Advances in neural information processing systems , 33:18518–18529, 2020. V ijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Y oshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982 , 2020. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathi nakumar Appuswamy, and Dharmen- dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153 , 2019. Boyuan Feng, Y uke W ang, Xu Li, Shu Y ang, Xueqiao Peng, and Y uf ei Ding. Sgquant: Squeezing the last bit on graph neural networks with specialized quantiza tion. In 2020 IEEE 32nd International Conference on T ools with Artiﬁcial Intelligence (ICTAI) , pp. 1044–1052. IEEE, 2020. Matthias Fey and Jan Eric Lenssen. Fast graph representatio n learning with pytorch geometric. arXiv preprint arXiv:1903.02428 , 2019. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol V inyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning , pp. 1263–1272. PMLR, 2017. Rafael G ´ omez-Bombarelli, Jennifer N W ei, David Duvenaud, Jos´ e Miguel Hern ´ andez-Lobato, Benjam´ ın S´ anchez-Lengeling, Dennis Sheberla, Jorge Agu ilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al´ an Aspuru-Guzik. Automatic chemical de sign using a data-driven contin- uous representation of molecules. ACS central science , 4(2):268–276, 2018. 10Published as a conference paper at ICLR 2023 Will Hamilton, Zhitao Y ing, and Jure Leskovec. Inductive re presentation learning on large graphs. Advances in neural information processing systems , 30, 2017. Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mar k A Horowitz, and William J Dally. Eie: Efﬁcient inference engine on compressed deep ne ural network. ACM SIGARCH Computer Architecture News , 44(3):243–254, 2016. W eihua Hu, Matthias Fey, Marinka Zitnik, Y uxiao Dong, Hongy u Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machi ne learning on graphs. Advances in neural information processing systems , 33:22118–22133, 2020. Linyong Huang, Zhe Zhang, Zhaoyang Du, Shuangchen Li, Hongz hong Zheng, Y uan Xie, and Nianxiong T an. Epquant: A graph neural network compression approach based on product quan- tization. Neurocomputing, 503:49–61, 2022. Sambhav Jain, Albert Gural, Michael Wu, and Chris Dick. Trai ned quantization thresholds for accurate and efﬁcient ﬁxed-point inference of deep neural n etworks. Proceedings of Machine Learning and Systems , 2:112–128, 2020. Bowen Jin, Chen Gao, Xiangnan He, Depeng Jin, and Y ong Li. Mul ti-behavior recommendation with graph convolutional networks. In Proceedings of the 43rd International ACM SIGIR Con- ference on Research and Development in Information Retriev al, pp. 659–668, 2020. Y ongcheng Jing, Y iding Y ang, Xinchao W ang, Mingli Song, and Dacheng T ao. Meta-aggregator: Learning to aggregate for 1-bit graph neural networks. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer V ision , pp. 5301–5310, 2021. Patrick Judd, Jorge Albericio, T ayler Hetherington, T or M A amodt, and Andreas Moshovos. Stripes: Bit-serial deep neural network computing. In 2016 49th Annual IEEE/ACM International Sym- posium on Microarchitecture (MICRO) , pp. 1–12. IEEE, 2016. Thomas N Kipf and Max W elling. Semi-supervised classiﬁcati on with graph convolutional net- works. arXiv preprint arXiv:1609.02907 , 2016. Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca W ehrstedt, Abhijit Bose, and Alex Peysakhovich. Pytorch-biggraph: A large scale graph embed ding system. Proceedings of Ma- chine Learning and Systems , 1:120–131, 2019. Qian Lou, Feng Guo, Lantao Liu, Minje Kim, and Lei Jiang. Auto q: Automated kernel-wise neural network quantization. arXiv preprint arXiv:1902.05690 , 2019. Mike O’Connor. Highlights of the high-bandwidth memory (hb m) standard. In Memory forum workshop, volume 3, 2014. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbu chner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks , 20(1):61–80, 2008. V ivienne Sze, Y u-Hsin Chen, Tien-Ju Y ang, and Joel S Emer. Ef ﬁcient processing of deep neural networks. Synthesis Lectures on Computer Architecture , 15(2):1–341, 2020. Shyam A T ailor, Javier Fernandez-Marques, and Nicholas D La ne. Degree-quant: Quantization- aware training for graph neural networks. arXiv preprint arXiv:2008.05000 , 2020. Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Y oshi yama, Javier Alonso Garcia, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precisio n dnns: All you need is a good parametrization. arXiv preprint arXiv:1905.11452 , 2019. Petar V eliˇ ckovi´ c, Guillem Cucurull, Arantxa Casanova, A driana Romero, Pietro Lio, and Y oshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. Hanchen W ang, Defu Lian, Y ing Zhang, Lu Qin, Xiangjian He, Y i guang Lin, and Xuemin Lin. Binarized graph neural network. W orld W ide W eb, 24(3):825–848, 2021a. 11Published as a conference paper at ICLR 2023 Junfu W ang, Y unhong W ang, Zhen Y ang, Liang Y ang, and Y uanfan g Guo. Bi-gcn: Binary graph convolutional network. In Proceedings of the IEEE/CVF Conference on Computer V ision a nd P attern Recognition , pp. 1561–1570, 2021b. Kuan W ang, Zhijian Liu, Y ujun Lin, Ji Lin, and Song Han. Haq: H ardware-aware automated quan- tization with mixed precision. In Proceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pp. 8612–8620, 2019. Cong Xie, Ling Y an, Wu-Jun Li, and Zhihua Zhang. Distributed power-law graph computing: Theoretical and empirical analysis. Advances in neural information processing systems , 27, 2014. Keyulu Xu, W eihua Hu, Jure Leskovec, and Stefanie Jegelka. H ow powerful are graph neural networks? arXiv preprint arXiv:1810.00826 , 2018. Pinar Y anardag and SVN V ishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery an d data mining , pp. 1365–1374, 2015. Hongxia Y ang. Aligraph: A comprehensive graph neural netwo rk platform. In Proceedings of the 25th ACM SIGKDD international conference on knowledge disc overy & data mining , pp. 3165– 3166, 2019. Zhilin Y ang, William Cohen, and Ruslan Salakhudinov. Revis iting semi-supervised learning with graph embeddings. In International conference on machine learning , pp. 40–48. PMLR, 2016. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and V iktor Prasanna. Graph- saint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 , 2019. Y iren Zhao, Duo W ang, Daniel Bates, Robert Mullins, Mateja J amnik, and Pietro Lio. Learned low precision graph neural networks. arXiv preprint arXiv:2009.09232 , 2020. 12Published as a conference paper at ICLR 2023 A A PPENDIX A.1 U N IF O RM QUA N T IZ AT IO N In this section, we will give a detailed introduction to the c ontent related to quantiﬁcation. A.1.1 Q UA N T IZ AT IO N PRO CE S S For a vector x, the xq is a quantized representation. Given the quantization step size s, s ∈ R+, and the quantization bitwidth b, b ∈ N+, then the uniform quantization is implemented as: ¯x = sign(x)      ⌊|x| s + 0. 5⌋, |x| < s (2b−1 − 1) 2b−1 − 1, |x| ≥ s(2b−1 − 1) . (9) The x at 32bits is mapped to the integer number set {−2b−1 + 1 , ..., 0, ..., 2b−1 − 1} where the bitwidth is #b bits, and the quantized representation can be calculated as xq = s · ¯x. For inference, ¯x can be used to compute matrix multiplication in the update ph ase or perform other computations in GNNs layers and the output of these computations then are r escaled by the corresponding s using a relatively lower cost scalar-vector multiplication. As a n illustrative example, for vectors x ∈ R3×1 and y ∈ R3×1, the quantization parameters are both s = 0 . 1, b = 5 , the process of inner product between these two vectors by integers is shown in Figure 6. Wh en the values in a vector are all non-negative, we do not need to represent the sign bit in the ﬁ xed-point representation. Therefore, the value can use #b bits to quantize instead of using the ﬁrst bit to represent th e sign bit. Then the quantization range of uniform quantization is [−s(2b − 1), s (2b − 1)]. 0.11 - 0.21 1.29 0.31  0.58 -0.27  -0.436 1 - 2 13  3 6 -3 -48   ! = 0.1  \" = 0.1Preform by float-point representation  Perform by integers representation  -48  Rescale  -48  ! \" -0.48  × = × = × = 0.11 (0.11) 0.5 1 0.1 sign ê ú ´ + = ê ú  ë û  0.21( 0.21) 0.5 2 0.1sign ê - ú - ´ + = - ê ú  ë û  5 1 0.1 (2 1) 1.5 -´ - =  Quantization process  Figure 6: An example of performing inner product by integers representation. A.1.2 G RA D IE N T IN BACK P RO PAG AT IO N Due to the ﬂoor function used in the quantization process is n ot differentiable, the gradient of xq with respect to x vanishes almost everywhere, which makes it impossible to tr ain the model by the backpropagation algorithm. Therefore, we use the straight -through estimator (Bengio et al., 2013) to approximate the gradient through the ﬂoor function, i.e., ∂L ∂x = ∂L ∂x q I|x|≤s(2b−1), where I|x|≤s(2b−1) is a indicator function, whose value is 1 when |x| ≤ s(2b − 1), and vice versa. In our paper, the quantiﬁcation parameters (s, b ) are learnable, the gradients of xq w .r.t. (s, b ) used in Eq. 3 and Eq. 4 are:   ∂x q ∂s ∂x q ∂b  =        [ 1 s (xq − x) 0 ] , |x| < s (2b−1 − 1) sign(x) [ ( 2b−1 − 1 ) 2b−1 ln (2) s ] , |x| ≥ s(2b−1 − 1) . (10) 13Published as a conference paper at ICLR 2023 T able 4: The aggregation functions and update functions for GNNs used in this paper, di denotes the degree of node i, the ε denotes a learnable constant, and α represent attention coefﬁcients. Model Aggregation function Update function GCN h(l) i = ∑ j∈N (i)∪{i} 1√di √ dj x(l−1) j x(l) i = ReLU (W (l)h(l) i + b(l)) GIN h(l) i = (1 + ε(l))x(l−1) i + ∑ j∈N (i) x(l−1) j x(l) i = MLP (l)(h(l) i , W (l), b(l)) GA T h(l) i = ∑ j∈N (i)∪{i} α (l) i,j x(l−1) j x(l) i = W (l)hl i+ b(l) T able 5: The statistics for density of adjacency matrix and t he labeled nodes in four node-level datasets. Cora CiteSeer PubMed ogbn-arxiv Density of A 0.144% 0.112% 0.028% 0.008% Labled nodes 5.17% 3.61% 0.30% 53.70% A.2 M O RE A BO U T GRA P H NE U RA L NE T WO RK S In this section, we ﬁrst give detailed information about the MPNN framework (Gilmer et al., 2017), and then provide a detailed examination of the three GNNs use d in our papers. A graph G = ( V, E) consist of nodes V = {1, ..., N } and edges E ⊆ V × V has node features X ∈ RN×F and optionally H-dimensional edge features E ∈ RE×H . The MPNN framework can be formulated by x(l) i = γ(l)(x(l−1) i , □ j∈N (i) φ(l)(x(l−1) i , x(l−1) j , e(l−1) ij )), where φ is a differentiable kernel function, □ is the aggregation function which is permutation-invarian t, and the γ is a learnable update function, xi is the features of node i and eij is the features of edge between node i and j, N (i) = {j : ( i, j ) ∈ E} , and l represents the l-th layer of the GNNs. In this paper, we focus on three typical GNN models whose forw ardpass all can be represented by the MPNN framework, Graph Convolution Network (GCN) (Kip f & W elling, 2016), Graph Iso- morphism Network (GIN) (Xu et al., 2018), and Graph Attentio n Network (GA T) (V eliˇ ckovi´ c et al., 2017). the detailed information is shown in T able 4. A.3 P RO O F S O F TH E O RE T ICA L RE S U LT S This section provides formal proof of the theoretical resul ts of our paper. A.3.1 N OTAT IO N S Here, we deﬁne the notations utilized in our proof. A = {0, 1}N×N is the adjacency matrix that indicates whether there is an edge between each pair of nodes , e.g., if there is an edge between node i and node j, then aij = 1 , otherwise, aij = 0 . Then, ˜A = A + I is the adjacency matrix for a graph 14Published as a conference paper at ICLR 2023 that is added to the self-loops. The degree matrix D = diag(d1, d 2, ..., d n), where di = ∑ j aij and the degree matrix for the graph having self-loops is ˜D = ( ˜d1, ˜d2, ..., ˜dn), where ˜di = ∑ j ˜aij. A.3.2 P RO O F S Proof 1. The gradients of the loss function with respect to the node fe atures in semi-supervised tasks are most zero. Without loss of generality, we use the GCN model as an example. From the T able 4, the graph convolution operation can be described as X(l+1) = σ( ˆAX(l)W (l)), (11) where ˆA = ˜D− 1 2 ˜A ˜D− 1 2 , is the normalized adjacency matrix, W (l) ∈ RFin×Fout is a learnable weight matrix in the l-th layer of GCN. X(l) is the input of the l-th layer and the output of the (l − 1)-th layer in GCN. σ is the non-linear activation function, e.g., ReLU. Note tha t the ˆA is an extreme sparse matrix for node-level datasets in our paper. In our training process of the model, we usenll loss as our task loss function L. Only the nodes in the train set T have labels. For the last layer of GCN, we get the node feature s to be classiﬁed by H(l+1) = softmax(X(l+1)). Then the gradient of L with respect to X(l+1) is G1 = ∇X(l+1) L = ∂L ∂H(l+1) · ∂H(l+1) ∂X(l+1) = [ lij ] ∈ RN×Fout , (12) where only the G1 i,:, i ∈ T is not zero, otherwise, G1 i,: = 0 . Then, the gradient of the loss function with respect to X(l) is G2 = ∇X(l) L = ˆAT (∇X(l+1) L ⊙ σ′( ˆAX(l)W (l)))(W (l))T . (13) For node j do not have an edge with the node i, i ∈ T , G2 j,: = 0 . T able 5 lists the density of the adjacency matrix A and the percentage of the labeled nodes in four node-level da tasets. Because the sparsity property of adjacency matrix and the nodes with trained labels only account for a tiny fraction of the graph, the gradients from the loss function f or most node features are zero. Proof 2. The normalized adjacency matrix ˆA is not needed to be quantized for the GCN model. W e take the process of XW → A(XW ) as an illustrative example, which represents ﬁrst calculat e the B = XW and then calculate AB. For the l-th layer of FP32 models, the ﬁrst stage is Bl = XlWl, and then calculate the Xl+1 = ˆABl, where Xl ∈ RN×F1 , Wl ∈ RF1×F2 and A ∈ RN×N . The step-size for Bl, Xl and Wl is SBl , SXl and SWl , respectively. And they are all diagonal matrices. The integer representations are calculated as Bl = Bl qSBl , Xl = SXl Xl q and Wl = Wl qSWl . Note that for the node-level tasks, we can obtain the SBl , SXl and SWl in advance. And for the graph-level tasks, we can obtain them through one mor e element-wise multiplication whose overhead is negligible, as the comparison in T able 6. Then th e ﬁrst stage is: Bl = Xl · Wl = ( SXl · Xl q) · (Wl q · SWl ) , (14) and there exists Bl = Bl qSBl . Therefore, the integers representation for the next stage can be calculated as: Bl q = BlS−1 Bl = ( SXl · Xl q) · (Wl q · SWl )S−1 Bl = ( SXl · Xl q) · (Wl q · (SWl S−1 Bl )) = ( SXl ⊗ (SWl S−1 Bl )) ⊙ (Xl q · Wl q) , (15) where the (SXl ⊗ (SWl S−1 Bl )) can be calculated ofﬂine. Then we obtain the ﬁxed-point repr esenta- tion Bl q for the next stage and do not introduce overhead. The process of node degree normalization after the aggregat ion process can be represented as Xl+1 = σ( ˆABl), where ˆA = D− 1 2 ˜AD− 1 2 is the normalized adjacency matrix, and σ is the 15Published as a conference paper at ICLR 2023 Figure 7: The pipeline of the quantization process on our acc elerator. non-linear activation function. D− 1 2 at the right side of ˜A can be fused into the SXl and then calculate Bl q as Eq. 15. Then the features of the (l + 1) -th layer Xl+1 can be obtained as Xl+1 = σ(D− 1 2 ˜ABl q). And there exits Xl+1 = SXl+1 X(l+1) q. Therefore, the X(l+1) q can be obtained as: X(l+1) q = S−1 Xl+1 Xl+1 = S−1 Xl+1 σ(D− 1 2 ˜ABl q) . (16) Note that the elements in diagonal matrix SXl+1 are all positive because this matrix is made up of step-size, which is always positive. Then we can obtain X(l+1) q = σ(S−1 Xl+1 D− 1 2 ˜ABl q) , where S−1 Xl+1 D− 1 2 can be obtained before inference and ˜A ∈ { 0, 1}N×N . The computation of ˜ABl q only has addition operations and the S−1 Xl+1 D− 1 2 can be obtained before inference for node-level tasks or introduce only once more element-wise multiplication to ca lculate for the graph-level tasks. The D− 1 2 at the left side is fused into the element-wise multiplicati on performed by the next layer and the D− 1 2 at the right side is fused into the element-wise multiplicat ion performed by the current layer and the element-wise multiplication is a necessary st age in the quantized model. Therefore, we can perform the node degree normalization using ﬁxed point a ddition operation instead of quantizing the normalized adjacency matrix which may introduce more qu antization error. Proof 3. The quantization process can be fused with Batch Normalizat ion operations. When GNNs have Batch Normalization (BN) Layers, the calcula tion process is as follows (Note that we have fused the mean and standard-deviation with the l earned parameters in BN): Xl+1 = BN (σ( ˆABl q)) = σ( ˆABl q)Y + Z , (17) where Y = diag(y1, y 2, ..., y F2 ) ∈ RF2×F2 , Z = ( z1, z2, ..., zF2 ) ∈ RN×F2 and zi = (θi, θ i, ..., θ i)T ∈ RN among which yi and θi are the BN parameters for the i-th dimension fea- ture of the nodes features. And there exits that Xl+1 = SXl+1 Xl+1 q. Therefore, Xl+1 q = S−1 Xl+1 Xl+1 = S−1 Xl+1 (σ( ˆABl q)Y + Z) = ( S−1 Xl+1 ⊗ Y ) ⊙ (σ( ˆABl q)) + S−1 Xl+1 Z . (18) Through Eq. 18, we can fuse the quantization of the next layer into the BN operation of the cur- rent layer, which will not introduce overhead because the BN layer itself requires ﬂoating point operations. Note that the ﬂoat point operations are also ele ment-wise. 16Published as a conference paper at ICLR 2023 T able 6: The comparison between ﬁxed-point operations and ﬂ oat-point operations for some tasks using the Nearest Neighbor Strategy. T ask GIN-RE-IB GCN-MNIST GA T -CIF AR10 GCN-ZINC Fixed-point(M) 936.96 455.69 1387.98 504.62 Float-point(M) 7.35 2.06 13.71 1.74 Ratio 0.78% 0.45% 0.98% 0.34% A.4 T H E OV E RH E A D ANA LY S IS O F NE A RE S T NE IG H BO R ST RAT E G Y Through our dedicated hardware and the optimized pipeline, we reduce the overhead introduced by the Nearest Neighbor Strategy (NNS) as much as possible. As t he pipeline is shown in Figure 7, we fuse the (NNS) with the following operations. The ﬁxed-po int results produced by the previous stage are used to ﬁrst multiply the corresponding step-size from the previous stage (an element- wise ﬂoat point multiplication) and then execute the NNS pro cess. After getting the step-size, these features are quantized immediately (an element-wise ﬂoat p oint multiplication). Therefore, through this fusion way, we do not need the extra memory to store a copy of FP32 features. In addition, the overhead of the NNS is from one more element- wise ﬂoat point multiplication and the search process. W e provide a comparison of the number of ﬂ oat-point operations and ﬁxed-point operations for different graph-level tasks in T able 6, wher e ‘Fixed-point’ denotes the ﬁxed-point op- eration, ‘Float-point’ denotes the ﬂoat-point operation a nd the ‘Ratio’ denotes the percentage of the ﬂoat-point operations in the overall process. The extra ﬂoa t-point operations introduced by NNS is only a tiny fraction of the ﬁxed-point operations. On the oth er hand, through our optimized pipeline and the comparator array used in our accelerator the latency introduced by the search process of the NNS can be overlapped. Therefore, the overhead introduced b y NNS is negligible. A.5 D ATA S E T S W e show the statistics for each dataset used in our work in T ab le 7. For datasets in node-level tasks, nodes correspond to documents and edges to citations between them. Node features are a bag-of-words representation of the document. The target is to classify each node in the graph cor- rectly. The Cora, CiteSeer and PubMed are from Y ang et al. (2016). The ogbn-arxiv, ogbl-mag and ogbn-collab are from Hu et al. (2020). The Flickr is from Zeng et al. (2019). The Reddit is from Hamilton et al. (2017). In graph-level tasks, REDDIT -BINARY(Y anardag & V ishwanathan, 2015) is a balanced dataset where each graph corresponds to a n online discussion thread and the nodes correspond to users. There would be an edge between two nodes if at least one of them responded to another’s comment. The task is then to identify whether a given graph belongs to a question/answer-based community or a discussion-based co mmunity The MNIST and CIF AR-10 datasets (Dwivedi et al., 2020) which are often used for imag e classiﬁcation tasks are transformed into graphs in which every node is represented by their super pixel and location, and the edges are constructed by Achanta et al. (2012). The task is to classify the image using its graph representation. The ZINC G ´ omez-Bombarelli et al. (2018) dataset contains graphs re presenting molecules, where each node is an atom. The task is to regress the penalized logP (also called constrained solubility in some works) of a given graph. In Figure 8, we show the in-deg ree distribution for all the datasets we use in our paper. A.6 E X P E RIM E N TA L SE T U P T o make a fair comparison, we adopt the same GNN architecture s as T ailor et al. (2020) on every task, and the FP32 baseline is also the same. For those tasks t hat T ailor et al. (2020) does not do, we adopt the same architecture as their FP32 version. For ogbn-arxiv and PubMed, we use the 17Published as a conference paper at ICLR 2023 T able 7: The statistics for each dataset used in this work. T ask Name Graphs Nodes Edges Features Classes Node-level Cora 1 2708 10556 1433 7 CiteSeer 1 3327 9104 3703 6 PubMed 1 19717 88648 500 3 ogbn-arxiv 1 169343 1166243 128 23 ogbn-mag 1 1939743 25582108 128 349 ogbl-collab 1 235868 1285465 128 – Reddit 1 232965 11606919 602 41 Flickr 1 89250 899756 500 7 Graph-level REDDIT -BINAR Y 2000 ∼429.6 ∼995.5 0 2 MNIST 70000 ∼71 ∼565 3 10 CIF AR10 60000 ∼117.6 ∼941.2 5 10 ZINC 12000 ∼23 ∼49.8 28 — architectures and FP32 results reported by Hu et al. (2020) a nd Kipf & W elling (2016) respectively. W e use standard splits for MNIST , CIF AR-10, and ZINC (Dwived i et al., 2020). For Cora, CiteSeer and PubMed, we use the splits used by Y ang et al. (2016). For REDDIT -BINA R Y , we use 10-fold cross-validation. Our data split way is also the same as DQ-I NT4. Figure 9 shows the architectures of the models used in our eva luations, including the layers, the number of hidden units, and whether to use a skip connection. Our method is implemented using PyT orch Geometric (Fey & Lenssen, 2019). W e quantize the same parts as the DQ-INT4 in all models except for the normali zed adjacency matrix in the GCN model, which we have proven that the quantization of this mat rix is not necessary in Appendix A.3.2, proof 2.. The values in the Cora and CiteSeer are all 0 or 1, therefore, we do not quantize the input features for the ﬁrst layer of the GNNs trained on the two data sets as DQ. For all quantized GNNs, we train them by Adam optimizer. The learning rate and the lea rning rate schedule are consistent with their FP32 version. In our method, the quantization par ameters (s, b ) are also learnable, so we set the learning rate for them, including the b for features, s for features, and s for weights. When initializing, the parameters of the models are initial ized as their FP32 version, the quantization bits for all nodes and weight matrixes are initialized by 4bi ts, and the step sizes for node features and weights are initialized by s ∈ N (0. 01, 0. 01) except for the graph-level tasks on GA T , where we initialize the step size by s ∈ U (0, 1). The N is normal distribution and the U is uniform distribution. And for GA T model trained on graph-level data sets, we just learn the quantization bits of the node features, while in the attention coefﬁcients com putation part, we use the exact 4bit to quantize. The batch size is 128 in all graph-level tasks. The results reported in our work for GNNs on Cora, CiteSeer and PubMed are averaged over 100 runs with different seeds, and the resu lts for ogbn-arxiv, MNIST, CIF AR-10and ZINC are averaged over ten runs. The results on REDDIT - BINARY are obtained by 10-fold cross-validation and the split seed is 12345, which is the same as DQ-INT4. All experiments in our paper ran on R TX 2080Ti GPU driven by Ubuntu 18.04. The version of the CUDA and Pytorch are 10.2 and 1.8.0, respectiv ely. A.6.1 E X P E RIM E N TA L S E T U P S F O R T H E A BL AT IO N S T U DY . The advantage of learning-based mixed-precision quantization: During the experiment of com- paring the learning bitwidth and bit assignment, we ensure t he average bits of node features of these two methods are comparable to verify the effectiveness of ou r A2Q method. As an example, if the average bit is 2.2bit when assigning the bit to nodes with dif ferent in-degrees, we will ﬁrst sort the 18Published as a conference paper at ICLR 2023 /uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (a) Cora /uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (b) CiteSeer /uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000014/uni00000017/uni00000013/uni00000014/uni00000019/uni00000013 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (c) PubMed /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (d) ogbn-arxiv /uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000019 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (e) MNIST /uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000019 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (f) CIF AR10 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000019 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (g) REDDIT -BINAR Y /uni00000014/uni00000015/uni00000016/uni00000017 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000019 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (h) ZINC Figure 8: The in-degree distribution for each dataset used i n this work. nodes by their in-degrees and then select the nodes with the t op 20% in-degrees, and quantize those by 3bit, and for the remaining nodes, we use 2bit to quantize. In the model trained by the bit assign- ment method, the bit is not learnable, and other hyperparame ters are all consistent with the model using the A2Q method. For the “GCN-Cora-mixed-precision” and “GIN-Cite Seer-mixed-precision” tasks, we use 3bit and 5bit to quantize the GNNs while keeping the average bitwidth at 4bits. In particular, we assign 5bits to those nodes with 50% top in-de grees and assign 3bits to others. 19Published as a conference paper at ICLR 2023 Figure 9: The model architectures used in our evaluations, t he head number for all GA T models on different tasks are 8. T able 8: The results comparison on GCN-PubMed and GIN-ogbn- arxiv. Accuracy A verage bits Compression Ratio Speedup PubMed GCN(FP32) 78.9±0.7% 32 1x — GCN(DQ) 62.5±2.4% 4 8x 1x GCN(ours)77.5±0.1% 1.90 16.8x 1.45x ogbn-arxiv GIN(FP32) 68.8±0.2% 32 1x — GIN(DQ) 57.6±2.2% 4 8x 1x GIN(ours)65.2±0.4% 3.82 8.4x 1.02x The power of learning the quantization parameters:For the “no-lr-bit”, we initialize the bitwidth as 4bits for all nodes features and just train the step size. F or the “no-lr-step”, we initialize the step size as previously mentioned but do not train them. For the “n o-lr”, we just initialize the bitwidth and the step size, but do not train them. Local Gradient v .s. Global Gradient:All settings of the model trained by global gradient is consistent with the model trained by local gradient method. The overhead of Nearest Neighbor Strategy:The model, without using the Nearest Neighbor Strategy, selects the quantization parameters according t o their in-degrees. Every in-degree has a corresponding group of quantization parameters. Those nod es whose in-degrees are larger than 1000 will share the same group quantization parameters. In t his way, The quantization parameters used by the nodes features can be determined as soon as the gra ph data is available, without the need for selection during the inference process, and then we can c ompare the overhead introduced by the selection process. A.7 M O RE EX P E RIM E N T S RE S U LT S This section is a complementary part about experiments resu lts to demonstrate that our A2Q quan- tization method is general and robust. 20Published as a conference paper at ICLR 2023 T able 9: The results comparison on inductive learning tasks and more graphs. T ask Acc(%) A verage bits Compression Ratio GCN-mag 30.8±0.1(FP32) 32 1x 32.7±0.4(Ours) 2.7 11.7x GCN-collab 44.8±1.1(FP32) 32 1x 44.9±1.5(Ours) 2.5 12.7x GraphSage- REDDIT 95.2±0.1(FP32) 32 1x 95.3±0.1(Ours) 3.9 8.1x GraphSage- Flickr 50.9±1.0(FP32) 32 1x 50.0±0.5%(Ours) 3.8 8.4x T able 10: Comparison with more quantization method. T ask Acc(%) A verage Bits Compression Ratio GCN-Cora 80.9±0.0(Half-pre) 16 1x 80.9±0.6(Ours) 1.7 9.40x GA T -CiteSeer 68.0±0.1(LPGNAS) 8 1x 71.9±0.7(Ours) 1.9 4.21x GraphSage-Cora 74.3±0.1(LPGNAS) 12 1x 74.5±0.2(Ours) 2.7 4.44x GraphSage- Flickr 49.7±0.3(LPGNAS) 8 1x 50.0±0.5(Ours) 3.8 2.11x /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000014/uni00000013/uni0000001a/uni0000001a /uni00000014/uni00000017/uni00000013/uni00000017 /uni00000014/uni00000017/uni0000001b/uni00000018/uni00000015/uni00000013 (a) GCN-Cora /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000015/uni00000014/uni0000001c/uni00000019 /uni00000017/uni00000017/uni00000013 /uni00000019/uni00000017/uni0000001b/uni00000013 /uni00000013 (b) GIN-Cora /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni0000001a/uni00000016 /uni00000014/uni0000001b/uni00000019/uni0000001a /uni0000001a/uni00000015/uni0000001a /uni00000017/uni00000013/uni00000014/uni00000013 (c) GA T -Cora /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000015/uni00000013/uni00000017/uni0000001b /uni00000014/uni0000001a/uni00000019/uni00000019/uni00000015 /uni0000001a/uni00000013 /uni00000013 /uni00000013 (d) GCN-PubMed /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000014/uni00000013/uni00000017/uni00000016/uni00000018 /uni0000001c/uni00000013/uni00000017/uni0000001c /uni00000015/uni00000016/uni00000015/uni00000014/uni00000013 /uni00000013 (e) GA T -PubMed /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000015/uni00000018 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni0000001a/uni0000001b/uni00000014/uni00000019/uni00000018 /uni0000001b/uni00000019/uni00000014/uni00000013/uni0000001b /uni00000017/uni00000017/uni00000016/uni0000001b/uni00000018/uni00000016/uni00000013/uni0000001c/uni00000014/uni00000014/uni00000014 (f) GCN-ogbn-arxiv /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000015/uni00000013/uni0000001c/uni0000001b/uni0000001b /uni00000014/uni00000017/uni0000001b/uni00000016/uni00000018/uni00000018 /uni00000013 /uni00000013 (g) GIN-ogbn-arxiv Figure 10: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize. (a), (b) and (c) Three GNN models train ed on Cora. (d) and (e) GCN and GA T trained on PubMed, respectively. (f) and (g) GCN and GIN trai ned on ogbn-arxiv, respectively. 21Published as a conference paper at ICLR 2023 T able 11: The effect of #m on the accuracy of quantized model, using GIN trained on REDDIT - BINAR Y as an example. The average bitwidth is 4bits. GIN(FP32): 92.2± 2.3% GIN(DQ): 81.3± 4.4% m 100 400 800 1000 1500 Accuracy 88.7±3.5% 90.6±3.8% 92.0±2.2% 92.5±1.8% 92.6±1.9% A.7.1 N O D E -L E V E L TA S K S In T able 8, we show more task results on PubMed and ogbn-arxiv. On the GA T -ogbn-arxiv task, our GPU raised the Out Of Memory error, so we do not report the resu lts on the GA T -ogbn-arxiv task. The model quantized by our A2Q method is also signiﬁcantly better than DQ-INT4, which show s that our A2Q is general. W e do not compare with DQ-INT8 because our result s are comparable with the FP32 baseline with a much larger compression ratio than D Q-INT8. W e also show the relationship between bit and average in-deg rees of nodes using the corresponding bitwidth to quantize on more tasks in Figure 10. W e present th e results of the ﬁnal layer of GNNs. The results show that the bitwidth learned by our A2Q method is also aggregation-aware, which means that our method is robust. W e also evaluate the inductive model, GraphSage, on some other node-level tasks to demonstrate the generality of our method on inductive learning tasks. Du e to the sampling operation in the GraphSage model, the subgraph input to the model varies, we a pply our nearest neighbor strategy to these tasks, i.e., GraphSage-Flickr and GraphSage-Reddit . In addition, we evaluate our method on more datasets, such as the ogbn-mag and ogbl-collab. ogbn-m ag is a heterogeneous graph and the ogbl-collab is used for the link prediction tasks. The results of our experiments are presented in T able 9, where we can see that our approach still works well and even brings some generalization performance improvement while signiﬁcantly com- pressing the model size. This also demonstrates that our Nei ghbor Nearest Strategy generalizes well on inductive models for node-level tasks. W e also compare with more quantization methods on GNNs. Zhaoet al. (2020) uses the Network Architecture Search (NAS) to search for the best quantizati on strategy for different components in the GNNs. Brennan et al. (2020) explore the use of half-preci sion (i.e., FP16) in the forward and backward passes of GNNs. T able 10 presents the comparison re sults on various tasks with these two methods. ‘Half-pre’ denotes the method in Brennan et al. (2020), and ‘LPGNAS’ denotes the method in Zhao et al. (2020). The results demonstrate that ou r method achieves better accuracy with a smaller quantization bitwidth on all tasks. A.7.2 G RA P H -L E V E L TA S K S W e propose the Nearest Neighbor Strategy to quantize the nod e features in graph-level tasks, in which the number of nodes input to models is various. In our Ne arest Neighbor Strategy, #m groups quantization parameters (s, b ) should be initialized, and we explore the effect of the value of m on the performance of the quantized model in T able 11 using the G IN trained on REDDIT -BINAR Y dataset. W e can observe that when the value of m is smaller than 800, the accuracy increases as the value of m increases. When the value of m is higher than 800, the performances of the models with different m are similar. However, the models with a larger m are more stable. Moreover, the selection of m may be related to the number of nodes input to the model. Accor ding to our experiments, we ﬁnally select m as 1000 for all graph-level tasks. T able 12 lists the comparison results on GIN-ZINC and GA T -ZI NC. On the regression tasks, our method is also signiﬁcantly better than DQ-INT4. Notably, w e do not learn different bitwidths for the nodes in ZINC datasets due to the similar topology struct ure between nodes. 22Published as a conference paper at ICLR 2023 T able 12: The results comparison on GIN-ZINC and GA T -ZINC. Modle Dataset Loss ↓ A verage bits Compression Ratio ZINC GA T(FP32) 0.455±0.006 32 1x GA T(DQ) 0.520±0.021 4 8x GA T(ours)0.495±0.006 4 8x GIN(FP32) 0.334±0.024 32 1x GIN(DQ) 0.431±0.012 4 8x GIN(ours)0.380±0.022 4 8x /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni00000016/uni00000013/uni00000015/uni00000015/uni00000015 /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013/uni00000015/uni00000014/uni00000013 /uni00000016/uni00000013/uni00000015/uni00000013/uni00000014 /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013/uni00000014 /uni00000016/uni00000013/uni00000015/uni00000015/uni00000014 /uni00000013 /uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000016/uni00000016/uni00000018/uni00000018 /uni00000015/uni00000019/uni0000001b/uni00000019/uni0000001a /uni00000013 /uni00000013 /uni00000013 (d) 4-th layer Figure 11: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GCN trained on CI F AR10. /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni00000016/uni00000013/uni00000013/uni00000016/uni00000015 /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000018/uni00000015/uni0000001c/uni0000001a /uni00000015/uni00000017/uni0000001a/uni00000016/uni00000018 /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013/uni00000016/uni00000013 /uni0000001c/uni00000013/uni00000019/uni00000017 /uni00000015/uni00000013/uni0000001c/uni00000016/uni0000001b /uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000019 /uni00000015/uni00000017/uni00000018/uni00000015 /uni00000015/uni00000019/uni00000016/uni00000019/uni00000015 /uni00000014/uni00000015/uni00000014/uni00000015/uni00000013 /uni00000013 (d) 4-th layer Figure 12: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GIN trained on CI F AR10. W e also show the relationship between bit and average in-deg ree of nodes using the correspond- ing bit to quantize for more graph-level tasks in different l ayers immediately after the aggregation phase in Figure 11-Figure 16. The quantization bitwidths le arned for graph-level tasks are also aggregation-aware. Because the difference of the in-degre es between different nodes is little in the MNIST and CIF AR10 dataset resulting in the aggregated fe atures are similar between different nodes, the relationship between learned bitwidths and the i n-degrees is irregular in some layers, e.g., the 2-nd layer in GCN trained on MNIST . 23Published as a conference paper at ICLR 2023 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni00000014/uni00000018/uni00000013/uni00000016/uni0000001c /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000014/uni00000017/uni00000013/uni0000001b/uni00000016 /uni0000001c/uni00000018/uni00000019 /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000014/uni00000017/uni00000018/uni0000001c/uni00000015 /uni00000017/uni00000017/uni0000001a/uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000014/uni00000017/uni0000001a/uni00000018/uni0000001c /uni00000015/uni0000001b/uni00000013/uni00000013 /uni00000013 (d) 4-th layer Figure 13: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GA T trained on CI F AR10. /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000017/uni0000001c/uni0000001a/uni00000016 /uni00000015/uni00000014/uni00000017/uni0000001c /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni0000001a/uni00000014/uni00000015/uni00000015 /uni00000013 /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni0000001b/uni00000014/uni00000014 /uni00000019/uni00000016/uni00000014/uni00000014 /uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000015/uni00000019/uni00000015/uni00000015 /uni00000017/uni00000018/uni00000013/uni00000013 /uni00000013 /uni00000013 /uni00000013 (d) 4-th layer Figure 14: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GCN trained on MN IST . /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni0000001c/uni00000013/uni00000014/uni0000001a /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni0000001c/uni00000013/uni00000014/uni0000001a /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013/uni00000014/uni00000013/uni0000001a /uni0000001b/uni0000001c/uni00000014/uni00000013 /uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000014/uni0000001c/uni00000019/uni0000001b /uni0000001a/uni00000013/uni00000017/uni0000001c /uni00000013 /uni00000013 (d) 4-th layer Figure 15: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GIN trained on MN IST . /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni0000001c/uni00000013/uni0000001c/uni00000016 /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000014/uni00000019/uni00000015/uni0000001b /uni0000001a/uni00000017/uni00000017/uni0000001a /uni00000014/uni0000001b/uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni0000001b/uni0000001b/uni00000014/uni00000017 /uni00000015/uni0000001a/uni0000001c/uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013/uni00000015/uni00000014 /uni0000001c/uni00000013/uni00000019/uni0000001c /uni00000016/uni00000013 (d) 4-th layer Figure 16: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GA T trained on MN IST . 24Published as a conference paper at ICLR 2023 T able 13: The impact of the depth of GNNs on quantization perf ormance. Layers 3 4 5 T ask Accu(%) A varage Bits Accu(%) A varage Bits Accu(%) A varage Bits GCN-Cora FP32 80.5±0.6 32 79.3±0.1 32 75.8±3.2 32 Ours 80.2±0.6 2.94 78.2±0.9 3.54 75.0±1.2 3.61 GIN-Cora FP32 49.4±15.8 32 37.1±13.1 32 — — Ours 54.5±12.6 3.3 36.4±11.1 3.1 — — T able 14: The comparison between the model with and without s kip connection on GCN-Cora task. Layers GCN-Cora Without skip connection With skip connection FP32 Ours FP32 Ours 3 Accu(%) 80.5±0.6 80.2±0.6 82.5±0.5 82.2±0.7 Bits 32 2.94 32 2.37 4 Accu(%) 79.3±0.1 78.2±0.9 81.9±0.7 81.5±0.3 Bits 32 3.54 32 2.63 5 Accu(%) 75.8±3.2 75.0±1.2 81.1±1.1 80.6±0.6 Bits 32 3.61 32 2.72 6 Accu(%) 73.8±1.6 73.1±1.9 80.1±0.8 80.4±0.7 Bits 32 4.62 32 2.98 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000024/uni00000055/uni0000004c/uni00000003/uni00000050/uni00000048/uni00000044/uni00000051 /uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000025/uni0000004c/uni00000057/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b /uni00000015/uni00000011/uni00000018/uni0000001a /uni00000016/uni00000011/uni00000013/uni00000017 /uni00000016/uni00000011/uni00000019/uni00000013 /uni00000017/uni00000011/uni00000016/uni00000013 /uni00000016/uni00000011/uni00000016/uni0000001b Figure 17: The average bitwidth for 2nd-5th layer in ﬁve layers GCN. /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000024/uni00000055/uni0000004c/uni00000003/uni00000050/uni00000048/uni00000044/uni00000051 /uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni00000025/uni0000004c/uni00000057/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b /uni00000015/uni00000011/uni0000001c/uni00000014 /uni00000017/uni00000011/uni00000013/uni00000013 /uni00000017/uni00000011/uni0000001a/uni0000001c /uni00000018/uni00000011/uni00000016/uni00000017 /uni00000019/uni00000011/uni00000013/uni00000017 /uni00000017/uni00000011/uni00000019/uni00000015 /uni00000014/uni00000011/uni00000015/uni00000016 /uni00000014/uni00000011/uni0000001c/uni00000018 /uni00000015/uni00000011/uni0000001b/uni00000015 /uni00000016/uni00000011/uni0000001b/uni00000014 /uni00000018/uni00000011/uni00000013/uni0000001b /uni00000015/uni00000011/uni0000001c/uni0000001b /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055 /uni00000024 /uni00000059/uni0000004a/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000056/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000056/uni0000004e/uni0000004c/uni00000053 /uni00000024 /uni00000059/uni0000004a/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000056/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000056/uni0000004e/uni0000004c/uni00000053 /uni00000034/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000056/uni0000004e/uni0000004c/uni00000053 /uni00000034/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000056/uni0000004e/uni0000004c/uni00000053 Figure 18: The average bitwidth and quan- tization error for 2nd-6th layer in six layers GCN. A.7.3 M O RE ABL AT IO N ST U DY The impact of the depth of GNNs on quantization performance: W e explore how a different number of GNN layers impacts the quantization performance o f GCN-Cora and GIN-CiteSeer. W e explore the quantization performance on 3,4,5,6 layers GCN model and 3,4 layers GIN model (the GCN and GIN used in T able 1 are 2 layers). W e did not explore the deeper GNN models because 25Published as a conference paper at ICLR 2023 T able 15: The comparison results on other aggregation funct ions. Baseline(FP32) Ours Bit Compression Ratio GIN sum 77.6±1.1% 77.8±1.6% 2.37 13.5x GIN mean 78.8±0.1% 78.5±0.6% 2.37 13.5x GIN max 78.6±1.6% 78.6±0.5% 1.97 16.2x /uni0000003e/uni00000014/uni0000000f/uni00000014/uni00000013/uni00000040 /uni0000003e/uni00000014 /uni00000014/uni0000000f/uni00000015/uni00000013/uni00000040/uni0000003e/uni00000015/uni00000014/uni0000000f/uni00000016/uni00000013/uni00000040/uni0000003e/uni00000016/uni00000014/uni0000000f/uni00000017/uni00000013/uni00000040/uni0000003e/uni00000017/uni00000014/uni0000000f/uni00000014/uni00000019/uni0000001b/uni00000040 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni000000ed/uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000024 /uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni0000000b/uni00000024/uni00000049/uni00000057/uni00000048/uni00000055/uni00000003 /uni00000024/uni0000004a/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c /uni00000036/uni00000058/uni00000050 /uni00000030/uni00000048/uni00000044/uni00000051 /uni00000030/uni00000044/uni0000005b Figure 19: The average aggregated nodes features in differe nt in-degree groups for models with different aggregation functions. the accuracy of the model decreases drastically as the number of model layers increases due to the over-smooth phenomenon in GNNs. As shown in T able 13, our method can also maintain the performance with a high compression ratio for the model with different layers compared with the FP32 model. In addition, we observe that the learned quantization bitwidth increases with the number of layers. W e analysis the average bitwidth used by 2nd to 5th layer for t he ﬁve layers GCN model in Figure 17. Our method learns a higher bitwidth for the deeper layer. Due to the over-smooth phenomenon that exists in the deep layer, the embedding features of diff erent nodes are similar in the deep layer. Therefore, we consider the deeper layer may need a higher qua ntization bitwidth to distinguish the embedding features of different nodes. The impact of skip connection on quantization performance:The ﬁrst column denoted by ‘With- out skip connection’ and the second column denoted by ‘With s kip connection’ of 18 present the comparison results for different layers GCN on Cora dataset s without skip connection and with skip connection, respectively. For the model with skip connecti on, our method is also effective. Our method learns a higher bitwidth for the deeper layer. Due to t he over-smooth phenomenon that ex- ists in the deep layer, we consider that the deeper layer may n eed a higher quantization bitwidth to distinguish the embedding features of different nodes. and the higher learned quantization bitwidth for deeper layers also alleviate quantization error. And co mpared to the quantized model with a skip connection, the learned quantization bitwidths are hi gher for the quantized model without skip connection. Figure 18 presents that the quantization error s of the model with skip connection are always higher than the model without skip connection in ever y layer which means that the model without skip connection is more sensitive to the quantizati on error. Therefore, a higher quantization bitwidth is necessary for the model without skip connection to maintain the performance. W e will add these analyses to the appendix in the revision. Scalability for models that use other aggregation functions: T o demonstrate that our method is also helpful to the GNNs using other aggregation functions r ather than the sum function, we replace the aggregation function of the GIN model, which is based on t he MPNN framework with mean and max functions, and we conduct the comparison experiment on t he Cora dataset. As shown in T able 26Published as a conference paper at ICLR 2023 T able 16: The comparison reults with the binary quantizatio n method on Cora and CiteSeer datasets. Accuracy A verage bits Compression ratio Cora GCN(FP32) 81.5±0.7% 32 1x Bi-GCN 81.2±0.8% 1 32x GCN(ours)81.4±0.7% 1.61 19.9x GIN(FP32) 77.6±1.1% 32 1x Bi-GIN 33.7±6.6% 1 32x GIN(ours)77.4±0.8% 1.92 16.7x GA T(FP32) 83.1±0.4% 32 1x Bi-GA T 31.9±0% 1 32x GA T(ours)82.6±0.5% 2.03 15.8x CiteSeer GCN(FP32) 71.1±0.7% 32 1x Bi-GCN 70.7±2.4% 1 32x GCN(ours) 70.7±0.7% 1.98 16.2x GIN(FP32) 66.1±0.9% 32 1x Bi-GIN 29.1±1.7% 1 32x GIN(ours)65.6±1.5% 2.39 13.4x GA T(FP32) 72.5±0.7% 32 1x Bi-GA T 20.6±2.6% 1 32x GA T(ours)71.0±0.7% 2.15 14.9x 19, the accuracy degradation is negligible and the compress ion ratio is high , indicating that our quantization scheme also applies to the GNNs with mean or max aggregation function. W e analyze the average features for different aggregation functions i n different in-degrees group in Figure 19. The average features of the sum and max functions are highly d ependent on in-degrees. The other insight is that the variance of the features is also highly de pendent on in-degrees. The analysis demonstrates the generality of our approach, w hich can capture differences between nodes introduced by topology information of graphs and comp ress the model size as much as possi- ble while maintaining the performance. A.7.4 C O M PA RIS O N WIT H BINA RY QUA N T IZ AT IO N ME T H O D In this section, we show the advantages of our method over the binary quantization method for GNNs. W e select the binary quantization method in W ang et al. (2021b) as our baseline. W e just ran the experiments on the node-level because the binary quanti zation method only supports node-level tasks, which is one of the drawbacks of the binary quantizati on method in GNNs. W e quantize the same part as W ang et al. (2021b) does for a fair comparison. The comparison results are shown in T able 16. The binary quantization method performs well on GCN, where the aggregation and update phases are simple. How ever, on both models, GA T and GIN, the accuracy drops signiﬁcantly compared with the FP32 baseline, which makes the deploy- ment unrealistic. However, our method is immune to this prob lem, although it has to use a higher average bit for node features which we believe is necessary f or GA T and GIN. In summary, our method outperforms the binary quantization method in two wa ys: 1. Our method can quantize more complex GNN models and ensure th e accuracy degradation is negligible compared with the FP32 baseline while achieving a high compression ratio of 13.4x- 19.9x. 2.Our method can be applied to graph-level tasks. However, the binary quantization method can not handle them. 27Published as a conference paper at ICLR 2023 Edge Buffer  Weight Buffer  Output Buffer  Input Buffer  Decode Control Unit  DRAM  Data Control signal  MAC …MAC MAC  MAC …MAC MAC  MAC …MAC MAC  MAC …MAC MAC  PE (a) 1 0 1 1 0 1 0 1Features  Weights  = 1 0 1 1023 hh h 1 0 1 1122 hh 1 0 1 1021 hh 1 0 1 1120 hh + + + = 55  0 1 0 1 1 0 1 1 + reg  <<  & (b) Figure 20: (a) The overview of our accelerator architecture . (b) An example of the bit-serial calcu- lation and the architecture of the MAC. A.7.5 A CCE L E RATO R ARCH IT E CT U RE In this section, we introduce the architecture of our hardwa re accelerator designed for GNN infer- ence. As presented in Section 3.1, we quantize each node feat ure to an appropriate precision and ﬁx the weights to 4bits. T o support mixed-precision computa tion, we adopt bit-serial multipliers at the core. Speciﬁcally, we follow the methodology in Judd et a l. (2016) to only serialize the node features. This way, it takes m cycles to complete the multiplication between an m-bit node feature with a 4bit weight, as shown in Figure 20(b). The product invo lving 2n is implemented by left-shift, i.e., for 2n × a, we can shift a left by n bits to implement the product. T o increase the computationa l throughput, we use 256 × 16 MACs which can process 256 16-dimensional features in paral lel. As shown in Figure 20(a), the compute unit is composed of 256 P rocessing Engines (PEs), each containing a row of 16 MACs. The architecture of the MAC is sho wn in Figure 20(b). The on-chip memory consists of an Edge Buffer, which stores t he adjacency matrix of graphs, a W eight Buffer, which stores the weight of the GNNs, an Input B uffer, and an Output Buffer to store the input features and the output result, and the regis ter of each MAC to store the partial sum. T o reduce data movement in the memory hierarchy, the input bu ffer and output buffer work in a swapped fashion, as the output of the current layer is the inp ut to the next layer. W e set the memory size of Input Buffer, Output Buffer, Edge Buffer, and the W ei ght Buffer to 2MB, 2MB, 256KB, and 256KB, respectively. The overview of our architecture is sh own in Figure 20(a). T o calculate Bl = XlW l, 256 consecutive rows in Xl and a column of W l are mapped onto the MAC array to compute 256 inner products in each phase. T o a chieve this, a column of W l is broadcast and shared among PEs. The results of the inner prod ucts are written to the output buffer, which can be reused to reduce the off-chip DRAM access. The ca lculation of Xl+1 = ABl is also in a inner-product manner. In this scenario, A is a sparse matrix. W e therefore represent A in the Compressed Sparse Row (CSR) format, where full zero rows or elements of A are eliminated. During inference, consecutive compressed rows of A and a column of Bl are mapped onto the MAC array in each phase. W e also sort the nodes in descending o rder according to their in-degrees, and the nodes with similar in-degrees are processed in paral lel simultaneously to alleviate the load imbalance problem when performing the aggregation operati ons. A.7.6 E N E RG Y EFFICIE N CY ANA LY S IS Our method can save energy cost signiﬁcantly from the follow ing two aspects: 1. By compressing the model size as much as possible, e.g., 18 .6x compression ratio on GCN-Cora as shown in T able 1, our method can signiﬁcantly reduce the me mory footprints. Figure 21 presents the energy table for the 45nm technology node. It shows that m emory access consumes further more energy than arithmetic operations. Therefore, the memory f ootprints domains the energy cost, and then compressing the model can save much energy cost. 2. Through our quantization method and the accelerator, themodel can perform inference using the ﬁxed-point operations instead of ﬂoat-point operations, w hich are much more energy-consuming 28Published as a conference paper at ICLR 2023 Figure 21: The energy table for 45nm technology node(Han et al., 2016; Sze et al., 2020). /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni0000002a/uni00000024 /uni00000037 /uni00000010/uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037/uni0000002a/uni00000024 /uni00000037 /uni00000010/uni00000026/uni0000002c/uni00000029 /uni00000024/uni00000035/uni00000014/uni00000013/uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000035/uni00000028/uni00000010/uni00000025/uni0000002c/uni0000002a/uni00000048/uni00000052/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051 /uni00000037 /uni00000044/uni00000056/uni0000004e/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000028/uni00000051/uni00000048/uni00000055/uni0000004a/uni0000005c/uni00000003/uni00000028/uni00000049/uni00000049/uni0000004c/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni0000005c/uni0000000b/uni00000029/uni0000002f/uni00000032/uni00000033/uni00000056/uni00000012/uni0000002d/uni0000000c /uni00000015/uni00000015/uni00000011/uni00000018× /uni00000016/uni00000016/uni00000011/uni00000014× /uni00000017/uni00000011/uni00000018× /uni00000018/uni00000011/uni00000018× /uni0000001c/uni00000011/uni00000019× /uni0000001b/uni00000011/uni00000017× /uni00000014/uni00000013/uni00000011/uni0000001a× /uni00000014× /uni00000014× /uni00000014× /uni00000014× /uni00000014× /uni00000014× /uni00000014× /uni0000002a/uni00000033/uni00000038 /uni00000024 /uni00000015 /uni00000034 Figure 22: The energy efﬁciency compared with 2080Ti GPU on various tasks. than ﬁxed-point operations. As shown in Figure 21, the 32bit ﬂoat MUL T consumes 18.5x energy compared to the 8bit int MUL T . Therefore, our method’s energ y consumption is much lower than the FP32 model. T o illustrate the advantage of our approach in terms of energy efﬁciency, we compare our accelerator with the 2080Ti GPU on various tasks. T o estimate the energy e fﬁciency of GPU, we use the nvidia- smi to obtain the power of GPU when performing the inference and m easure the inference time by time function provided by Python. Then we can get the energy cost o f GPU. W e also model the energy cost of our method on the accelerator. W e use High Band width Memory (HBM) as our off- chip storage. Then we count the number of integer operations , and ﬂoating point operations, and the number of accesses to SRAM and HBM when performing the inf erence process of the quantized models on our accelerator. Based on the data in T able 21, we es timate the energy consumed by ﬁxed- point operations and ﬂoating-point operations. The static and dynamic power of SRAM is estimated using CACTI 7.0(Balasubramonian et al., 2017). The energy o f HBM 1.0 is estimated with 7 pJ/bit as in (O’Connor, 2014). Figure 22 presents these results, wh ich shows that the the energy efﬁciency of our method is signiﬁcantly better than GPU. A.8 C O M P L E X IT Y ANA LY S IS In this section, we provide the analysis of the complexity of our proposed A2Q method, including the computational complexity and space complexity. Space Complexity:When analyzing the space complexity, we use the data size of t he node features as an approximation of the entire loaded data, including wei ghts and features, because the node features account for more than 90% of the overall memory cons umption for a GNN model. For a GNN has L layers, we assume that the input data to the ﬁrst laye r is X ∈ RN×F0 , and the dimension of the hidden features is F1. Then the dimension of the input to the 2-(L-1) layer is N × F1. After quantizing the model, the average bits of the feature maps ar e bm. The memory size includes two parts: 1. the nodes features bm[NF0 + ( L − 1)NF1]. 2. the quantization step size (a step size is a ﬂoat-point number which is 32bit) for each node 32NL. Therefore, the space complexity of the overall GNN model is as follows: M = bm[NF0 + (L − 1)NF1] + 32 NL. (19) W e can also obtain the ratio of the memory consumption of the s tep size in overall memory size: r = 32NL bm[NF0 + (L − 1)NF1]. (20) In the node-level tasks, the F0 is usually much larger than 32, e.g., 3703 in the CiteSeer dat aset. Moreover, in the graph-level tasks, we usually set m = 1000 , which is much smaller than the number of the input nodes to models, i.e., N. Therefore, alth ough our method learns the quantization step size for each node the memory overhead introduced by the quantization step size is negligible. Computational Complexity: The forward pass is divided into the aggregation and update p hases according to the MPNN framework. The aggregation phase can b e represented as Hl = ˆAXl, 29Published as a conference paper at ICLR 2023 and then the update phase calculates Xl+1 = HlW l. For ˆA ∈ RN×N , Xl ∈ RN×F1 , and W l ∈ RF1×F2 , the computational complexity of the FP32 models is O(N2F1 + NF1F2), which are all the ﬂoat-point operations. After quantizing the mod el, the ﬂoat-point matrix multiplication can be replaced by integer multiplication, and the element- wise operation, which calculates the mul- tiplication between integers and ﬂoat-point numbers accor ding to the Eq. 2. Then the computational complexity is C = OI (N2F1 + NF1F2) + OE (NF2), (21) where OI represents the complexity of the integers multiplication, whose cost is much lower than the ﬂoat-point operations, and the OE represents the complexity of the element-wise operations. Note that although we quantize each node features by different st ep size, the complexity of element-wise operation involving ﬂoat-point is the same as the DQ-INT4 be cause the number of element-wise operations is equal to the number of the elements in a feature map, i.e., N × F2. 30",
      "meta_data": {
        "arxiv_id": "2302.00193v1",
        "authors": [
          "Zeyu Zhu",
          "Fanrong Li",
          "Zitao Mo",
          "Qinghao Hu",
          "Gang Li",
          "Zejian Liu",
          "Xiaoyao Liang",
          "Jian Cheng"
        ],
        "published_date": "2023-02-01T02:54:35Z",
        "pdf_url": "https://arxiv.org/pdf/2302.00193v1.pdf"
      }
    },
    {
      "title": "GraphQNTK: Quantum Neural Tangent Kernel for Graph Data"
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a model’s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 Saarbr¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a node’s neighborhood to update the node’s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (Veliˇckovi´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a node’s i) own features and ii) neighboring nodes’ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 ± 0.84% test accuracy). In sum- mary, our contributions are as follows: • We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. • We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. • We update an existing conservation law relating the structure of gradients in GAT to GATE. • We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss ‘good’ and ‘bad’ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a node’s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as ωGAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E ⊆ V × V, where for a node v ∈ V the neighborhood is N(v) = {u|(u, v) ∈ E} and input features are h0 v. A GNN layer updates each node’s repre- sentation by aggregating over its neighbors’ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, ∀v ∈ V, (v, v) ∈ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function ϕ that is homogeneous (i.e ϕ(x) = xϕ′(x)) and consequently, ϕ(ax) = aϕ(x) for positive scalars a) such as ReLU ϕ(x) = max {x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. GAT Given input representations hl−1 v for v ∈ V, a GAT 1 layer l ∈ [L] transforms those to: hl v = ϕ   X u∈N(v) αl uv · Wl shl−1 u  , where (1) αl uv = exp \u0000 el uv \u0001 P u′∈N(v) exp \u0000 el u′v \u0001, and (2) el uv = \u0000 al\u0001⊤ · ϕ \u0000 Wl shl−1 u + Wl thl−1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1u̸=val s + 1u=val t \u0001⊤ ·ϕ \u0000 Ulhl−1 u + Vlhl−1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters θ of a layer l ∈ [L − 1] in a GAT network and their gradients ∇θL w.r.t. loss L fulfill: ⟨Wl [i,:], ∇Wl [i,:] ⟩ = ⟨Wl+1 [:,i] , ∇Wl+1 [:,i] ⟩ + ⟨al [i], ∇al [i] ⟩. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as ∆θ = ∇θL/θ for θ ̸= 0 or ∆θ = 0 for θ = 0. nl+1X j=1 Wl ij 2 ∆Wl ij = nl+2X k=1 Wl+1 ki 2 ∆Wl+1 ki + al i 2 ∆al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change ∆al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require αij/αii << 1. We use relativeαij/αii instead of αij and αii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods αij/αii << 1 and αji/αjj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing αij/αii, it automatically in- creases αji/αjj . However, we require multiple features that contribute to reducing only αij without strengthen- ing αji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms ∥a∥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require αij/αii << 1, ∀ j ∈ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l ∈ [L − 1] are conserved according to the following laws. Given Θ(θ) = ⟨θ, ∇θL⟩, it holds that: Θ(Wl [i,:]) − Θ(al+1 s [i]) − Θ(al+1 t [i]) = Θ(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Θ(al s[i]) + Θ(al t[i]) = Θ(Ul [i,:]) + Θ(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of αvv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since ∀ v ∈ V, αvv = 1 − P u∈N(v),u̸=v αuv. Thus, αvv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas αvv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. αvv = 1 and (b) neighbor-dependent that benefit from ig- noring the node’s own features, i.e.αvv = 0. In both cases, ∀ v ∈ V, P u∈N(v),u̸=v αuv + αvv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require αii ∈ [0, 1]. GATE’s atten- tion mechanism is more flexible than GAT’s in learning the level of neighborhood aggregation required for a task. a node’s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd ˝os–R´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with ∗ and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with ‡. GAT S and GAT models marked with † also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215† 97.7@166† 99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36‡ R,7 1 99.4@263 † 99.8@268† 100@104 2 61.7@2088 ∗ 52.8@341∗ 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 † 100@182† 100@1313 2 99.2@100 † 99.2@119† 99.6@79 5 64.0@7778 ∗ 99.6@239 100@45 R,8 1 88.8@9578 ∗ 98.4@3290 99.2@1755 2 90.4@2459 ∗ 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a node’s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v ∈ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /∈ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy ±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6±1.3 92 .3±1.3 96.4 ± 0.7 93.5±1.3 2 93 .5±0.7 92 .7±2.7 97.9 ± 0.8 94.6±2.1 3 88 .2±4.9 91 .8±3.4 92 .1±4.6 94.0 ± 1.5 2 2 90 .4±1.3 87 .7±1.6 93.8 ± 0.5 88.7±2.5 3 82 .2±4.5 88 .9±2.1 85 .8±2.5 93.4 ± 3.3 4 84 .0±5.0 83 .0±4.8 89.2 ± 2.3 87.8±2.4 3 3 84 .3±3.2 83 .8±2.7 87 .5±1.8 88.6 ± 2.0 4 71 .4±3.9 75 .9±7.6 89.2 ± 1.0 89.0±0.5 5 80 .2±4.8 83 .9±2.2 86 .1±0.8 87.8 ± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /∈ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argc∈[C](v ∈ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u ∈ Nk(v) rather than the node’s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodes’ own random features (left) and neighbors’ features aggregated k times (right). Figure 5: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter ϵ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATE’s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofαvv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the αvv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layer’s learned representations, is more beneficial compared to the node’s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of αvv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy ±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 ± 1.25 (5) 45 .58 ± 0.41 (10) 57 .72 ± 1.58 (5) 50 .83 ± 0.41 (5) 63 .57 ± 1.03 (10) MLP 65.12 ± 0.25 (5) 43 .26 ± 0.34 (5) 59 .44 ± 0.94 (10) 50 .74 ± 0.56 (5) 62 .67 ± 1.06 (10) MLP+GAT 70.83 ± 0.39 (5) 45 .25 ± 0.17 (10) 59 .12 ± 1.57 (10) 60 .07 ± 1.11 (5) 65 .85 ± 0.64 (10) FAGCN 67.55 ± 0.81 (5) 42 .85 ± 0.83 (10) 60 .38 ± 1.21 (5) 63 .38 ± 0.91 (10) 60 .89 ± 1.12 (5) GATE 75.55 ± 0.30 (5) 45.73 ± 0.24(10) 62.95 ± 0.71 (5) 66.14 ± 1.57 (5) 66.63 ± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of αvv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of αvv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATE’s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATE’s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 ± 0.49 52.49 ± 0.46 92.82 ± 0.90 84.62 ± 0.69 78.46 ± 1.17 GAT 80.87 ± 0.30 49.09 ± 0.63 92.01 ± 0.68 83.70 ± 0.47 77.43 ± 1.20 GATE-sep 89.78 ± 0.54 54.51 ± 0.38 94.18 ± 0.43 84.48 ± 0.57 78.20 ± 1.00 GAT-sep 88.75 ± 0.41 52.70 ± 0.62 93.91 ± 0.35 83.78 ± 0.43 76.79 ± 0.71 GT 86.51 ± 0.73 51.17 ± 0.66 91.85 ± 0.76 83.23 ± 0.64 77.95 ± 0.68 GT-sep 87.32 ± 0.39 52.18 ± 0.80 92.29 ± 0.47 82.52 ± 0.92 78.05 ± 0.93 GCN 73.69 ± 0.74 48.70 ± 0.63 89.75 ± 0.52 83.64 ± 0.67 76.09 ± 1.27 SAGE 85.74 ± 0.67 53.63 ± 0.39 93.51 ± 0.57 82.43 ± 0.44 76.44 ± 0.62 H2GCN 60.11 ± 0.52 36.47 ± 0.23 89.71 ± 0.31 73.35 ± 1.01 63.59 ± 1.46 CPGNN 63.96 ± 0.62 39.79 ± 0.77 52.03 ± 5.46 73.36 ± 1.01 65.96 ± 1.95 GPR-GNN 64.85 ± 0.27 44.88 ± 0.34 86.24 ± 0.61 72.94 ± 0.97 55.48 ± 0.91 FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 GloGNN 59.63 ± 0.69 36.89 ± 0.14 51.08 ± 1.23 73.39 ± 1.17 65.74 ± 1.19 FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 ResNet 65.88 ± 0.38 45.90 ± 0.52 50.89 ± 1.39 72.95 ± 1.06 70.34 ± 0.76 ResNet+SGC 73.90 ± 0.51 50.66 ± 0.48 70.88 ± 0.90 80.70 ± 0.97 75.81 ± 0.96 ResNet+adj 52.25 ± 0.40 51.83 ± 0.57 50.42 ± 0.83 78.78 ± 1.11 75.77 ± 1.24 Table 5: Mean test accuracy ±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 ± 0.16 (3) 79.57 ± 0.84 (12) products 80.63 ± 0.46 (3) 86.24 ± 1.01 (8) mag 32.61 ± 0.29 (2) 35.29 ± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and ˙Ismail ˙Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249–256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that αij/αii << 1 with αij/αii = exp ( eij − eii). This implies that we require eij − eii << 0 and thus aT ϕ (W (hi + hj)) − 2aT ϕ (W (hi)) << 0. Since we also require αij/αjj << 1, it follows from adding both inequalities that aT [ϕ (W (hi + hj)) − (ϕ (Whi) + ϕ (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which ∆fij ; = a[f] [ϕ (W[f, :] (hi + hj)) − (ϕ (W[f, :]hi) + ϕ (W[f, :]hi))] fulfills ∆fij << 0. Yet, note that if both ϕ (W[f, :]hi) and ϕ (W[f, :]hj) are positive or both are negative, we just get ∆fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality ϕ (W[f, :]hi) < 0 and ϕ (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]ϕ (W[f, :]hi) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hj)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hj)) also receives a negative contribution that makes αij/αjj smaller. Yet, what happens to αij/αii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [ϕ (W (hi + hj)) − 2ϕ (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]ϕ (W[f, :]hj) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hi)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hi)) and αij/αjj is reduced. Similarly, we can derive that at the same time αij/αii is increased, however. This implies that any feature that contributes to reducing ∆fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either αij/αii or αij/αjj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require αij/αjj << 1 and αji/αii << 1. It follows from adding both related inequalities that aT [ϕ (Wshi + Wthj) + ϕ (Wshj + Wthi) − ϕ ((Ws + Wt) hi) − ϕ ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ ((Ws[f, :] + Wt[f, :]) hi) − ϕ ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation α >0 denotes the slope of the leakyReLU and not the attention weights αij. 1. Case (+ − ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that ϕ is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](α −1)[Ws[f, :]hj + Wt[f, :]hi]. Since α −1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ − −−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + −): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (− − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](α −1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ − +−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 − α)Ws[f, :] (hi − hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi − hj) and the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l ∈ [L − 1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al+1 s [i], ∇al+1 s [i]L⟩ + ⟨al+1 t [i], ∇al+1 t [i]L⟩. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩ = ⟨Ul[i, :], ∇Ul[i,:]L⟩ + ⟨V l[i, :], ∇V l[i,:]L⟩. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(θ) is rescale-invariant with respect to disjoint subsets of the parametersθ1 and θ2 if for everyλ >0 we haveL(θ) = L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1L⟩ − ⟨θ2, ∇θ2L⟩ = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l ∈ [L−1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets θ1 and θ2 of the parameter set θ, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors θ1 = {x|x ∈ Wl[i, :]} θ2 = {w|w ∈ Wl+1[:, i]} ∪ {al s[i]} ∪ {al t[i]} We show that the loss of GATES remains invariant for any λ >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. The scaled network parameters are denoted with a tilde as ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i], and ˜Wl[i, j] = λWl[i, j], and the corresponding networks components scaled as a result are denoted by ˜hl u[i], ˜hl+1 v [k], and ˜αl uv. We show that the parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 − quv)al s + (quv)al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) where quv = 1 if u = v and quv = 0 if u ̸= v. For simplicity, we rewrite this as: el uv,u̸=v = (al s)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (16) el uv,u=v = (al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (17) We show that ˜αl uv = exp(˜el uv)P u′∈N(v) exp(˜eluv) = αl uv , because (18) ˜el uv,u̸=v = el uv,u̸=v , and ˜el uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of ϕ that allows ˜el uv,u=v = λ−1al s[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (20) = λ−1λal s[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (21) = el uv,u̸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors ˜el uv,u=v = λ−1al t[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (23) = λ−1λal t[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (24) = el uv,u=v. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] = ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i]. In the next layer, we therefore have ˜hl+1 v [k] = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ˜Wl[i, :] = Wl[i, :]λ, then also scaling ˜Wl+1[:, i] = Wl+1[:, i]λ−1 and ˜al+1 s [i] = al+1 s [i]λ−1 and ˜al+1 t [i] = al+1 t [i]λ−1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i] could be compensated by ˜Ul[i, :] = Ul[i, :]λ and ˜V l[i, :] = V l[i, :]λ. It follows immediately that ˜euv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, αij/αii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute ϕ in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute ϕ in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels β as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofβ indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels β. Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data β L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of α coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of αvv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of αvv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of αvv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of αvv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of ‘over-smoothing’ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines ‘over’-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. ωGAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter ω that can, in principle, switch off neighborhood aggregation by setting ω parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the node’s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: ‘When αG ij ≈ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.’ where αG ij defined in the paper can be considered analogous to αij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all αG ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of αG ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting αG ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a node’s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theωGAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      }
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a model’s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 Saarbr¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a node’s neighborhood to update the node’s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (Veliˇckovi´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a node’s i) own features and ii) neighboring nodes’ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 ± 0.84% test accuracy). In sum- mary, our contributions are as follows: • We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. • We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. • We update an existing conservation law relating the structure of gradients in GAT to GATE. • We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss ‘good’ and ‘bad’ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a node’s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as ωGAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E ⊆ V × V, where for a node v ∈ V the neighborhood is N(v) = {u|(u, v) ∈ E} and input features are h0 v. A GNN layer updates each node’s repre- sentation by aggregating over its neighbors’ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, ∀v ∈ V, (v, v) ∈ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function ϕ that is homogeneous (i.e ϕ(x) = xϕ′(x)) and consequently, ϕ(ax) = aϕ(x) for positive scalars a) such as ReLU ϕ(x) = max {x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. GAT Given input representations hl−1 v for v ∈ V, a GAT 1 layer l ∈ [L] transforms those to: hl v = ϕ   X u∈N(v) αl uv · Wl shl−1 u  , where (1) αl uv = exp \u0000 el uv \u0001 P u′∈N(v) exp \u0000 el u′v \u0001, and (2) el uv = \u0000 al\u0001⊤ · ϕ \u0000 Wl shl−1 u + Wl thl−1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1u̸=val s + 1u=val t \u0001⊤ ·ϕ \u0000 Ulhl−1 u + Vlhl−1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters θ of a layer l ∈ [L − 1] in a GAT network and their gradients ∇θL w.r.t. loss L fulfill: ⟨Wl [i,:], ∇Wl [i,:] ⟩ = ⟨Wl+1 [:,i] , ∇Wl+1 [:,i] ⟩ + ⟨al [i], ∇al [i] ⟩. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as ∆θ = ∇θL/θ for θ ̸= 0 or ∆θ = 0 for θ = 0. nl+1X j=1 Wl ij 2 ∆Wl ij = nl+2X k=1 Wl+1 ki 2 ∆Wl+1 ki + al i 2 ∆al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change ∆al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require αij/αii << 1. We use relativeαij/αii instead of αij and αii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods αij/αii << 1 and αji/αjj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing αij/αii, it automatically in- creases αji/αjj . However, we require multiple features that contribute to reducing only αij without strengthen- ing αji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms ∥a∥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require αij/αii << 1, ∀ j ∈ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l ∈ [L − 1] are conserved according to the following laws. Given Θ(θ) = ⟨θ, ∇θL⟩, it holds that: Θ(Wl [i,:]) − Θ(al+1 s [i]) − Θ(al+1 t [i]) = Θ(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Θ(al s[i]) + Θ(al t[i]) = Θ(Ul [i,:]) + Θ(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of αvv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since ∀ v ∈ V, αvv = 1 − P u∈N(v),u̸=v αuv. Thus, αvv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas αvv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. αvv = 1 and (b) neighbor-dependent that benefit from ig- noring the node’s own features, i.e.αvv = 0. In both cases, ∀ v ∈ V, P u∈N(v),u̸=v αuv + αvv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require αii ∈ [0, 1]. GATE’s atten- tion mechanism is more flexible than GAT’s in learning the level of neighborhood aggregation required for a task. a node’s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd ˝os–R´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with ∗ and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with ‡. GAT S and GAT models marked with † also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215† 97.7@166† 99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36‡ R,7 1 99.4@263 † 99.8@268† 100@104 2 61.7@2088 ∗ 52.8@341∗ 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 † 100@182† 100@1313 2 99.2@100 † 99.2@119† 99.6@79 5 64.0@7778 ∗ 99.6@239 100@45 R,8 1 88.8@9578 ∗ 98.4@3290 99.2@1755 2 90.4@2459 ∗ 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a node’s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v ∈ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /∈ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy ±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6±1.3 92 .3±1.3 96.4 ± 0.7 93.5±1.3 2 93 .5±0.7 92 .7±2.7 97.9 ± 0.8 94.6±2.1 3 88 .2±4.9 91 .8±3.4 92 .1±4.6 94.0 ± 1.5 2 2 90 .4±1.3 87 .7±1.6 93.8 ± 0.5 88.7±2.5 3 82 .2±4.5 88 .9±2.1 85 .8±2.5 93.4 ± 3.3 4 84 .0±5.0 83 .0±4.8 89.2 ± 2.3 87.8±2.4 3 3 84 .3±3.2 83 .8±2.7 87 .5±1.8 88.6 ± 2.0 4 71 .4±3.9 75 .9±7.6 89.2 ± 1.0 89.0±0.5 5 80 .2±4.8 83 .9±2.2 86 .1±0.8 87.8 ± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /∈ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argc∈[C](v ∈ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u ∈ Nk(v) rather than the node’s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodes’ own random features (left) and neighbors’ features aggregated k times (right). Figure 5: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter ϵ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATE’s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofαvv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the αvv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layer’s learned representations, is more beneficial compared to the node’s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of αvv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy ±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 ± 1.25 (5) 45 .58 ± 0.41 (10) 57 .72 ± 1.58 (5) 50 .83 ± 0.41 (5) 63 .57 ± 1.03 (10) MLP 65.12 ± 0.25 (5) 43 .26 ± 0.34 (5) 59 .44 ± 0.94 (10) 50 .74 ± 0.56 (5) 62 .67 ± 1.06 (10) MLP+GAT 70.83 ± 0.39 (5) 45 .25 ± 0.17 (10) 59 .12 ± 1.57 (10) 60 .07 ± 1.11 (5) 65 .85 ± 0.64 (10) FAGCN 67.55 ± 0.81 (5) 42 .85 ± 0.83 (10) 60 .38 ± 1.21 (5) 63 .38 ± 0.91 (10) 60 .89 ± 1.12 (5) GATE 75.55 ± 0.30 (5) 45.73 ± 0.24(10) 62.95 ± 0.71 (5) 66.14 ± 1.57 (5) 66.63 ± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of αvv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of αvv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATE’s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATE’s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 ± 0.49 52.49 ± 0.46 92.82 ± 0.90 84.62 ± 0.69 78.46 ± 1.17 GAT 80.87 ± 0.30 49.09 ± 0.63 92.01 ± 0.68 83.70 ± 0.47 77.43 ± 1.20 GATE-sep 89.78 ± 0.54 54.51 ± 0.38 94.18 ± 0.43 84.48 ± 0.57 78.20 ± 1.00 GAT-sep 88.75 ± 0.41 52.70 ± 0.62 93.91 ± 0.35 83.78 ± 0.43 76.79 ± 0.71 GT 86.51 ± 0.73 51.17 ± 0.66 91.85 ± 0.76 83.23 ± 0.64 77.95 ± 0.68 GT-sep 87.32 ± 0.39 52.18 ± 0.80 92.29 ± 0.47 82.52 ± 0.92 78.05 ± 0.93 GCN 73.69 ± 0.74 48.70 ± 0.63 89.75 ± 0.52 83.64 ± 0.67 76.09 ± 1.27 SAGE 85.74 ± 0.67 53.63 ± 0.39 93.51 ± 0.57 82.43 ± 0.44 76.44 ± 0.62 H2GCN 60.11 ± 0.52 36.47 ± 0.23 89.71 ± 0.31 73.35 ± 1.01 63.59 ± 1.46 CPGNN 63.96 ± 0.62 39.79 ± 0.77 52.03 ± 5.46 73.36 ± 1.01 65.96 ± 1.95 GPR-GNN 64.85 ± 0.27 44.88 ± 0.34 86.24 ± 0.61 72.94 ± 0.97 55.48 ± 0.91 FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 GloGNN 59.63 ± 0.69 36.89 ± 0.14 51.08 ± 1.23 73.39 ± 1.17 65.74 ± 1.19 FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 ResNet 65.88 ± 0.38 45.90 ± 0.52 50.89 ± 1.39 72.95 ± 1.06 70.34 ± 0.76 ResNet+SGC 73.90 ± 0.51 50.66 ± 0.48 70.88 ± 0.90 80.70 ± 0.97 75.81 ± 0.96 ResNet+adj 52.25 ± 0.40 51.83 ± 0.57 50.42 ± 0.83 78.78 ± 1.11 75.77 ± 1.24 Table 5: Mean test accuracy ±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 ± 0.16 (3) 79.57 ± 0.84 (12) products 80.63 ± 0.46 (3) 86.24 ± 1.01 (8) mag 32.61 ± 0.29 (2) 35.29 ± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and ˙Ismail ˙Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249–256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that αij/αii << 1 with αij/αii = exp ( eij − eii). This implies that we require eij − eii << 0 and thus aT ϕ (W (hi + hj)) − 2aT ϕ (W (hi)) << 0. Since we also require αij/αjj << 1, it follows from adding both inequalities that aT [ϕ (W (hi + hj)) − (ϕ (Whi) + ϕ (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which ∆fij ; = a[f] [ϕ (W[f, :] (hi + hj)) − (ϕ (W[f, :]hi) + ϕ (W[f, :]hi))] fulfills ∆fij << 0. Yet, note that if both ϕ (W[f, :]hi) and ϕ (W[f, :]hj) are positive or both are negative, we just get ∆fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality ϕ (W[f, :]hi) < 0 and ϕ (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]ϕ (W[f, :]hi) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hj)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hj)) also receives a negative contribution that makes αij/αjj smaller. Yet, what happens to αij/αii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [ϕ (W (hi + hj)) − 2ϕ (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]ϕ (W[f, :]hj) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hi)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hi)) and αij/αjj is reduced. Similarly, we can derive that at the same time αij/αii is increased, however. This implies that any feature that contributes to reducing ∆fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either αij/αii or αij/αjj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require αij/αjj << 1 and αji/αii << 1. It follows from adding both related inequalities that aT [ϕ (Wshi + Wthj) + ϕ (Wshj + Wthi) − ϕ ((Ws + Wt) hi) − ϕ ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ ((Ws[f, :] + Wt[f, :]) hi) − ϕ ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation α >0 denotes the slope of the leakyReLU and not the attention weights αij. 1. Case (+ − ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that ϕ is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](α −1)[Ws[f, :]hj + Wt[f, :]hi]. Since α −1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ − −−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + −): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (− − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](α −1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ − +−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 − α)Ws[f, :] (hi − hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi − hj) and the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l ∈ [L − 1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al+1 s [i], ∇al+1 s [i]L⟩ + ⟨al+1 t [i], ∇al+1 t [i]L⟩. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩ = ⟨Ul[i, :], ∇Ul[i,:]L⟩ + ⟨V l[i, :], ∇V l[i,:]L⟩. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(θ) is rescale-invariant with respect to disjoint subsets of the parametersθ1 and θ2 if for everyλ >0 we haveL(θ) = L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1L⟩ − ⟨θ2, ∇θ2L⟩ = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l ∈ [L−1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets θ1 and θ2 of the parameter set θ, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors θ1 = {x|x ∈ Wl[i, :]} θ2 = {w|w ∈ Wl+1[:, i]} ∪ {al s[i]} ∪ {al t[i]} We show that the loss of GATES remains invariant for any λ >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. The scaled network parameters are denoted with a tilde as ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i], and ˜Wl[i, j] = λWl[i, j], and the corresponding networks components scaled as a result are denoted by ˜hl u[i], ˜hl+1 v [k], and ˜αl uv. We show that the parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 − quv)al s + (quv)al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) where quv = 1 if u = v and quv = 0 if u ̸= v. For simplicity, we rewrite this as: el uv,u̸=v = (al s)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (16) el uv,u=v = (al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (17) We show that ˜αl uv = exp(˜el uv)P u′∈N(v) exp(˜eluv) = αl uv , because (18) ˜el uv,u̸=v = el uv,u̸=v , and ˜el uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of ϕ that allows ˜el uv,u=v = λ−1al s[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (20) = λ−1λal s[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (21) = el uv,u̸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors ˜el uv,u=v = λ−1al t[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (23) = λ−1λal t[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (24) = el uv,u=v. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] = ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i]. In the next layer, we therefore have ˜hl+1 v [k] = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ˜Wl[i, :] = Wl[i, :]λ, then also scaling ˜Wl+1[:, i] = Wl+1[:, i]λ−1 and ˜al+1 s [i] = al+1 s [i]λ−1 and ˜al+1 t [i] = al+1 t [i]λ−1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i] could be compensated by ˜Ul[i, :] = Ul[i, :]λ and ˜V l[i, :] = V l[i, :]λ. It follows immediately that ˜euv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, αij/αii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute ϕ in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute ϕ in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels β as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofβ indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels β. Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data β L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of α coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of αvv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of αvv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of αvv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of αvv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of ‘over-smoothing’ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines ‘over’-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. ωGAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter ω that can, in principle, switch off neighborhood aggregation by setting ω parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the node’s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: ‘When αG ij ≈ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.’ where αG ij defined in the paper can be considered analogous to αij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all αG ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of αG ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting αG ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a node’s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theωGAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      }
    },
    {
      "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method",
      "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning\nrepresentations of attributed graphs. To scale GCNs to large graphs,\nstate-of-the-art methods use various layer sampling techniques to alleviate the\n\"neighbor explosion\" problem during minibatch training. We propose GraphSAINT,\na graph sampling based inductive learning method that improves training\nefficiency and accuracy in a fundamentally different way. By changing\nperspective, GraphSAINT constructs minibatches by sampling the training graph,\nrather than the nodes or edges across GCN layers. Each iteration, a complete\nGCN is built from the properly sampled subgraph. Thus, we ensure fixed number\nof well-connected nodes in all layers. We further propose normalization\ntechnique to eliminate bias, and sampling algorithms for variance reduction.\nImportantly, we can decouple the sampling from the forward and backward\npropagation, and extend GraphSAINT with many architecture variants (e.g., graph\nattention, jumping connection). GraphSAINT demonstrates superior performance in\nboth accuracy and training time on five large graphs, and achieves new\nstate-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).",
      "full_text": "Published as a conference paper at ICLR 2020 GraphSAINT: G RAPH SAMPLING BASED INDUCTIVE LEARNING METHOD Hanqing Zeng∗ University of Southern California zengh@usc.edu Hongkuan Zhou∗ University of Southern California hongkuaz@usc.edu Ajitesh Srivastava University of Southern California ajiteshs@usc.edu Rajgopal Kannan US Army Research Lab rajgopal.kannan.civ@mail.mil Viktor Prasanna University of Southern California prasanna@usc.edu ABSTRACT Graph Convolutional Networks (GCNs) are powerful models for learning repre- sentations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use variouslayer samplingtechniques to alleviate the “neighbor explosion” problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efﬁciency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure ﬁxed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the for- ward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on ﬁve large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). 1 I NTRODUCTION Recently, representation learning on graphs has attracted much attention, since it greatly facilitates tasks such as classiﬁcation and clustering (Wu et al., 2019; Cai et al., 2017). Current works on Graph Convolutional Networks (GCNs) (Hamilton et al., 2017; Chen et al., 2018b; Gao et al., 2018; Huang et al., 2018; Chen et al., 2018a) mostly focus on shallow models (2 layers) on relatively small graphs. Scaling GCNs to larger datasets and deeper layers still requires fast alternate training methods. In a GCN, data to be gathered for one output node comes from its neighbors in the previous layer. Each of these neighbors in turn, gathers its output from the previous layer, and so on. Clearly, the deeper we back track, the more multi-hop neighbors to support the computation of the root. The number of support nodes (and thus the training time) potentially grows exponentially with the GCN depth. To mitigate such “neighbor explosion”, state-of-the-art methods use variouslayer sampling techniques. The works by Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a) ensure that only a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. Chen et al. (2018b) and Huang et al. (2018) further propose samplers to restrict the neighbor expansion factor to 1, by ensuring a ﬁxed sample size in all layers. While these methods signiﬁcantly speed up training, they face challenges in scalability, accuracy or computation complexity. ∗Equal contribution 1 arXiv:1907.04931v4  [cs.LG]  16 Feb 2020Published as a conference paper at ICLR 2020 Present work We present GraphSAINT (Graph SAmpling based INductive learning meThod) to efﬁciently train deep GCNs. GraphSAINT is developed from a fundamentally different way of minibatch construction. Instead of building a GCN on the full training graph and then sampling across the layers, we sample the training graph ﬁrst and then build a full GCN on the subgraph. Our method is thus graph samplingbased. Naturally, GraphSAINT resolves “neighbor explosion”, since every GCN of the minibatches is a small yet complete one. On the other hand, graph sampling based method also brings new challenges in training. Intuitively, nodes of higher inﬂuence on each other should have higher probability to form a subgraph. This enables the sampled nodes to “support” each other without going outside the minibatch. Unfortunately, such strategy results in non-identical node sampling probability, and introduces bias in the minibatch estimator. To address this issue, we develop normalization techniques so that the feature learning does not give preference to nodes more frequently sampled. To further improve training quality, we perform variance reduction analysis, and design light-weight sampling algorithms by quantifying “inﬂuence” of neighbors. Experiments on GraphSAINT using ﬁve large datasets show signiﬁcant performance gain in both training accuracy and time. We also demonstrate the ﬂexibility of GraphSAINT by integrating our minibatch method with popular GCN architectures such as JK-net (Xu et al., 2018) and GAT (Veliˇckovi´c et al., 2017). The resulting deep models achieve new state-of-the-art F1 scores on PPI (0.995) and Reddit (0.970). 2 R ELATED WORK A neural network model that extends convolution operation to the graph domain is ﬁrst proposed by Bruna et al. (2013). Further, Kipf & Welling (2016); Defferrard et al. (2016) speed up graph convolution computation with localized ﬁlters based on Chebyshev expansion. They target relatively small datasets and thus the training proceeds in full batch. In order to scale GCNs to large graphs, layer sampling techniques (Hamilton et al., 2017; Chen et al., 2018b; Ying et al., 2018a; Chen et al., 2018a; Gao et al., 2018; Huang et al., 2018) have been proposed for efﬁcient minibatch training. All of them follow the three meta steps: 1. Construct a complete GCN on the full training graph. 2. Sample nodes or edges of each layer to form minibatches. 3. Propagate forward and backward among the sampled GCN. Steps (2) and (3) proceed iteratively to update the weights via stochastic gradient descent. The layer sampling algorithm of GraphSAGE (Hamilton et al., 2017) performs uniform node sampling on the previous layer neighbors. It enforces a pre-deﬁned budget on the sample size, so as to bound the minibatch computation complexity. Ying et al. (2018a) enhances the layer sampler of Hamilton et al. (2017) by introducing an importance score to each neighbor. The algorithm presumably leads to less information loss due to weighted aggregation. S-GCN (Chen et al., 2018a) further restricts neighborhood size by requiring only two support nodes in the previous layer. The idea is to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN (Chen et al., 2018b) performs sampling from another perspective. Instead of tracking down the inter-layer connections, node sampling is performed independently for each layer. It applies importance sampling to reduce variance, and results in constant sample size in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. Huang et al. (2018) improves FastGCN by an additional sampling neural network. It ensures high accuracy, since sampling is conditioned on the selected nodes in the next layer. Signiﬁcant overhead may be incurred due to the expensive sampling algorithm and the extra sampler parameters to be learned. Instead of sampling layers, the works of Zeng et al. (2018) and Chiang et al. (2019) build mini- batches from subgraphs. Zeng et al. (2018) proposes a speciﬁc graph sampling algorithm to ensure connectivity among minibatch nodes. They further present techniques to scale such training on shared-memory multi-core platforms. More recently, ClusterGCN (Chiang et al., 2019) proposes graph clustering based minibatch training. During pre-processing, the training graph is partitioned into densely connected clusters. During training, clusters are randomly selected to form minibatches, and intra-cluster edge connections remain unchanged. Similar to GraphSAINT, the works of Zeng et al. (2018) and Chiang et al. (2019) do not sample the layers and thus “neighbor explosion” is avoided. Unlike GraphSAINT, both works are heuristic based, and do not account for bias due to the unequal probability of each node / edge appearing in a minibatch. Another line of research focuses on improving model capacity. Applying attention on graphs, the architectures of Veliˇckovi´c et al. (2017); Zhang et al. (2018); Lu et al. (2019) better capture neighbor features by dynamically adjusting edge weights. Klicpera et al. (2018) combines PageRank with GCNs to enable efﬁcient information propagation from many hops away. To develop deeper models, 2Published as a conference paper at ICLR 2020 “skip-connection” is borrowed from CNNs (He et al., 2015; Huang et al., 2017) into the GCN context. In particular, JK-net Xu et al. (2018) demonstrates signiﬁcant accuracy improvement on GCNs with more than two layers. Note, however, that JK-net (Xu et al., 2018) follows the same sampling strategy as GraphSAGE (Hamilton et al., 2017). Thus, its training cost is high due to neighbor explosion. In addition, high order graph convolutional layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) also help propagate long-distance features. With the numerous architectural variants developed, the question of how to train them efﬁciently via minibatches still remains to be answered. 3 P ROPOSED METHOD : GraphSAINT Graph sampling based method is motivated by the challenges in scalability (in terms of model depth and graph size). We analyze the bias (Section 3.2) and variance (Section 3.3) introduced by graph sampling, and thus, propose feasible sampling algorithms (Section 3.4). We show the applicability of GraphSAINT to other architectures, both conceptually (Section 4) and experimentally (Section 5.2). In the following, we deﬁne the problem of interest and the corresponding notations. A GCN learns representation of an un-directed, attributed graph G(V,E), where each node v ∈V has a length-f attribute xv. Let A be the adjacency matrix and ˜A be the normalized one (i.e., ˜A = D−1A, and D is the diagonal degree matrix). Let the dimension of layer-ℓinput activation be f(ℓ). The activation of node v is x(ℓ) v ∈Rf(ℓ) , and the weight matrix is W(ℓ) ∈Rf(ℓ)×f(ℓ+1) . Note that xv = x(1) v . Propagation rule of a layer is deﬁned as follows: x(ℓ+1) v = σ (∑ u∈V ˜Av,u ( W(ℓ) )T x(ℓ) u ) (1) where ˜Av,u is a scalar, taking an element of ˜A. And σ(·) is the activation function (e.g., ReLU). We use subscript “s” to denote parameterd of the sampled graph (e.g.,Gs, Vs, Es). GCNs can be applied under inductive and transductive settings. While GraphSAINT is applicable to both, in this paper, we focus on inductive learning. It has been shown that inductive learning is especially challenging (Hamilton et al., 2017) — during training, neither attributes nor connections of the test nodes are present. Thus, an inductive model has to generalize to completely unseen graphs. 3.1 M INIBATCH BY GRAPH SAMPLING 0 1 2 3 4 5 6 8 9 7 0 1 2 3 5 7 0 1 2 3 5 7 0 1 2 3 5 7 Gs = SAMPLE(G) Full GCN on Gs Figure 1: GraphSAINT training algorithm GraphSAINT follows the design philosophy of directly sampling the training graph G, rather than the corresponding GCN. Our goals are to 1. extract appropriately connected subgraphs so that little information is lost when propagating within the subgraphs, and 2. combine information of many subgraphs together so that the training process overall learns good representation of the full graph. Figure 1 and Algorithm 1 illustrate the training algorithm. Before training starts, we perform light-weight pre-processing on Gwith the given sampler SAMPLE. The pre-processing estimates the probability of a node v∈V and an edge e∈E being sampled by SAMPLE. Such probability is later used to normalize the subgraph neighbor aggregation and the minibatch loss (Section 3.2). Afterwards, 3Published as a conference paper at ICLR 2020 Algorithm 1GraphSAINT training algorithm Input: Training graph G(V,E,X); Labels Y ; Sampler SAMPLE; Output: GCN model with trained weights 1: Pre-processing: Setup SAMPLE parameters; Compute normalization coefﬁcients α, λ. 2: for each minibatch do 3: Gs (Vs,Es) ←Sampled sub-graph of Gaccording to SAMPLE 4: GCN construction on Gs. 5: {yv |v∈Vs}← Forward propagation of {xv |v∈Vs}, normalized by α 6: Backward propagation from λ-normalized loss L(yv,yv). Update weights. 7: end for training proceeds by iterative weight updates via SGD. Each iteration starts with an independently sampled Gs (where |Vs| ≪|V|). We then build a full GCN on Gs to generate embedding and calculate loss for every v∈Vs. In Algorithm 1, node representation is learned by performing node classiﬁcation in the supervised setting, and each training node vcomes with a ground truth label yv. Intuitively, there are two requirements for SAMPLE: 1. Nodes having high inﬂuence on each other should be sampled in the same subgraph. 2. Each edge should have non-negligible probability to be sampled. For requirement (1), an ideal SAMPLE would consider the joint information from node connections as well as attributes. However, the resulting algorithm may have high complexity as it would need to infer the relationships between features. For simplicity, we deﬁne “inﬂuence” from the graph connectivity perspective and design topology based samplers. Requirement (2) leads to better generalization since it enables the neural net to explore the full feature and label space. 3.2 N ORMALIZATION A sampler that preserves connectivity characteristic of Gwill almost inevitably introduce bias into minibatch estimation. In the following, we present normalization techniques to eliminate biases. Analysis of the complete multi-layer GCN is difﬁcult due to non-linear activations. Thus, we analyze the embedding of each layer independently. This is similar to the treatment of layers independently by prior work (Chen et al., 2018b; Huang et al., 2018). Consider a layer-(ℓ+ 1)node vand a layer-ℓ node u. If vis sampled (i.e., v∈Vs), we can compute the aggregated feature of vas: ζ(ℓ+1) v = ∑ u∈V ˜Av,u αu,v ( W(ℓ) )T x(ℓ) u 1 u|v = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u 1 u|v, (2) where ˜x(ℓ) u = ( W(ℓ))T x(ℓ) u , and 1 u|v ∈{0,1}is the indicator function given vis in the subgraph (i.e., 1 u|v = 0if v∈Vs ∧(u,v) ̸∈Es; 1 u|v = 1if (u,v) ∈Es; 1 u|v not deﬁned if v̸∈Vs). We refer to the constant αu,v as aggregator normalization. Deﬁne pu,v = pv,u as the probability of an edge (u,v) ∈E being sampled in a subgraph, and pv as the probability of a node v∈V being sampled. Proposition 3.1. ζ(ℓ+1) v is an unbiased estimator of the aggregation ofvin the full(ℓ+ 1)th GCN layer, ifαu,v = pu,v pv . i.e.,E ( ζ(ℓ+1) v ) = ∑ u∈V ˜Av,u ˜x(ℓ) u . Assuming that each layer independently learns an embedding, we use Proposition 3.1 to normalize feature propagation of each layer of the GCN built byGraphSAINT. Further, let Lv be the loss on vin the output layer. The minibatch loss is calculated as Lbatch = ∑ v∈Gs Lv/λv, where λv is a constant that we term loss normalization. We set λv = |V|· pv so that: E(Lbatch) = 1 |G| ∑ Gs∈G ∑ v∈Vs Lv λv = 1 |V| ∑ v∈V Lv. (3) Feature propagation within subgraphs thus requires normalization factors αand λ, which are com- puted based on the edge and node probability pu,v, pv. In the case of random node or random edge samplers, pu,v and pv can be derived analytically. For other samplers in general, closed form expression is hard to obtain. Thus, we perform pre-processing for estimation. Before training starts, 4Published as a conference paper at ICLR 2020 we run the sampler repeatedly to obtain a set of N subgraphs G. We setup a counter Cv and Cu,v for each v∈V and (u,v) ∈E, to count the number of times the node or edge appears in the subgraphs of G. Then we set αu,v = Cu,v Cv = Cv,u Cv and λv = Cv N . The subgraphs Gs ∈G can all be reused as minibatches during training. Thus, the overhead of pre-processing is small (see Appendix D.2). 3.3 V ARIANCE We derive samplers for variance reduction. Letebe the edge connectingu, v, and b(ℓ) e = ˜Av,u ˜x(ℓ−1) u + ˜Au,v ˜x(ℓ−1) v . It is desirable that variance of all estimators ζ(ℓ) v is small. With this objective, we deﬁne: ζ = ∑ ℓ ∑ v∈Gs ζ(ℓ) v pv = ∑ ℓ ∑ v,u ˜Av,u pvαu,v ˜x(ℓ) u 1 v1 u|v = ∑ ℓ ∑ e b(ℓ) e pe 1 (ℓ) e . (4) where 1 e = 1if e∈Es; 1 e = 0if e̸∈Es. And 1 v = 1if v∈Vs; 1 v = 0if v̸∈Vs. The factor pu in the ﬁrst equality is present so that ζis an unbiased estimator of the sum of all node aggregations at all layers: E(ζ) =∑ ℓ ∑ v∈VE ( ζ(ℓ) v ) . Note that 1 (ℓ) e = 1 e,∀ℓ, since once an edge is present in the sampled graph, it is present in all layers of our GCN. We deﬁne the optimal edge sampler to minimize variance for every dimension of ζ. We restrict ourselves to independent edge sampling. For each e∈E, we make independent decision on whether it should be in Gs or not. The probability of including eis pe. We further constrain ∑pe = m, so that the expected number of sampled edges equals to m. The budget mis a given sampling parameter. Theorem 3.2. Under independent edge sampling with budgetm, the optimal edge probabilities to minimize the sum of variance of eachζ’s dimension is given by:pe = m∑ e′ ∑ ℓ b(ℓ) e′  ∑ ℓ b(ℓ) e . To prove Theorem 3.2, we make use of the independence among graph edges, and the dependence among layer edges to obtain the covariance of 1 (ℓ) e . Then using the fact that sum of pe is a constant, we use the Cauchy-Schwarz inequality to derive the optimal pe. Details are in Appendix A. Note that calculating b(ℓ) e requires computing ˜x(ℓ−1) v , which increases the complexity of sampling. As a reasonable simpliﬁcation, we ignore ˜x(ℓ) v to make the edge probability dependent on the graph topology only. Therefore, we choose pe ∝ ˜Av,u + ˜Au,v = 1 deg(u) + 1 deg(v) . The derived optimal edge sampler agrees with the intuition in Section 3.1. If two nodes u, v are connected and they have few neighbors, then uand vare likely to be inﬂuential to each other. In this case, the edge probability pu,v = pv,u should be high. The above analysis on edge samplers also inspires us to design other samplers, which are presented in Section 3.4. Remark We can also apply the above edge sampler to perform layer sampling. Under the indepen- dent layer sampling assumption of Chen et al. (2018b), one would sample a connection ( u(ℓ),v(ℓ+1)) with probability p(ℓ) u,v ∝ 1 deg(u) + 1 deg(v) . For simplicity, assume a uniform degree graph (of degree d). Then p(ℓ) e = p. For an already sampled u(ℓ) to connect to layer ℓ+ 1, at least one of its edges has to be selected by the layer ℓ+ 1sampler. Clearly, the probability of an input layer node to “survive” the Lnumber of independent sampling process is ( 1 −(1 −p)d )L−1 . Such layer sampler potentially returns an overly sparse minibatch for L> 1. On the other hand, connectivity within a minibatch of GraphSAINT never drops with GCN depth. If an edge is present in layer ℓ, it is present in all layers. 3.4 S AMPLERS Based on the above variance analysis, we present several light-weight and efﬁcient samplers that GraphSAINT has integrated. Detailed sampling algorithms are listed in Appendix B. Random node sampler We sample |Vs|nodes from Vrandomly, according to a node probability distribution P(u) ∝ ˜A:,u  2 . This sampler is inspired by the layer sampler of Chen et al. (2018b). 5Published as a conference paper at ICLR 2020 Random edge sampler We perform edge sampling as described in Section 3.3. Random walk based samplersAnother way to analyze graph sampling based multi-layer GCN is to ignore activations. In such case, Llayers can be represented as a single layer with edge weights given by B = ˜AL. Following a similar approach as Section 3.3, if it were possible to pick pairs of nodes (whether or not they are directly connected in the original ˜A) independently, then we would set pu,v ∝Bu,v + Bv,u, where Bu,v can be interpreted as the probability of a random walk to start at uand end at v in Lhops (and Bv,u vice-versa). Even though it is not possible to sample a subgraph where such pairs of nodes are independently selected, we still consider a random walk sampler with walk length Las a good candidate for L-layer GCNs. There are numerous random walk based samplers proposed in the literature (Ribeiro & Towsley, 2010; Leskovec & Faloutsos, 2006; Hu & Lau, 2013; Li et al., 2015). In the experiments, we implement a regular random walk sampler (with rroot nodes selected uniformly at random and each walker goes hhops), and also a multi-dimensional random walk sampler deﬁned in Ribeiro & Towsley (2010). For all the above samplers, we return the subgraph induced from the sampled nodes. The induction step adds more connections into the subgraph, and empirically helps improve convergence. 4 D ISCUSSION Extensions GraphSAINT admits two orthogonal extensions. First, GraphSAINT can seamlessly integrate other graph samplers. Second, the idea of training by graph sampling is applicable to many GCN architecture variants: 1. Jumping knowledge(Xu et al., 2018): since our GCNs constructed during training are complete, applying skip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods (Chen et al., 2018b; Huang et al., 2018), extra modiﬁcation to their samplers is required, since the jumping knowledge architecture requires layer-ℓ samples to be a subset of layer-(ℓ−1) samples∗. 2. Attention (Veliˇckovi´c et al., 2017; Fey, 2019; Zhang et al., 2018): while explicit variance reduction is hard due to the dynamically updated attention values, it is reasonable to apply attention within the subgraphs which are considered as representatives of the full graph. Our loss and aggregator normalizations are also applicable†. 3. Others: To support high order layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) or even more complicated networks for the task of graph classiﬁcation (Ying et al., 2018b), we replace the full adjacency matrix A with the (normalized) one for the subgraph As to perform layer propagation. Comparison GraphSAINT enjoys: 1. high scalability and efﬁciency, 2. high accuracy, and 3. low training complexity. Point (1) is due to the signiﬁcantly reduced neighborhood size compared with Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a). Point (2) is due to the better inter- layer connectivity compared with Chen et al. (2018b), and unbiased minibatch estimator compared with Chiang et al. (2019). Point (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of Huang et al. (2018) and clustering of Chiang et al. (2019). 5 E XPERIMENTS Setup Experiments are under the inductive, supervised learning setting. We evaluate GraphSAINT on the following tasks: 1. classifying protein functions based on the interactions of human tissue proteins (PPI), 2. categorizing types of images based on the descriptions and common properties of online images (Flickr), 3. predicting communities of online posts based on user comments (Reddit), 4. categorizing types of businesses based on customer reviewers and friendship (Yelp), and 5. classifying product categories based on buyer reviewers and interactions (Amazon). For PPI, we use the small version for the two layer convergence comparison (Table 2 and Figure 2), since Hamilton et al. (2017) and Chen et al. (2018a) report accuracy for this version in their original papers. We use the large version for additional comparison with Chiang et al. (2019) to be consistent with its reported accuracy. All datasets follow “ﬁxed-partition” splits. Appendix C.2 includes further details. ∗The skip-connection design proposed by Huang et al. (2018) does not have such “subset” requirement, and thus is compatible with both graph sampling and layer sampling based methods. †When applying GraphSAINT to GAT (Veliˇckovi´c et al., 2017), we remove the softmax step which normalizes attention values within the same neighborhood, as suggested by Huang et al. (2018). See Appendix C.3. 6Published as a conference paper at ICLR 2020 Table 1: Dataset statistics (“m” stands formulti-class classiﬁcation, and “s” for single-class.) Dataset Nodes Edges Degree Feature Classes Train / Val / Test PPI 14,755 225,270 15 50 121 (m) 0.66 / 0.12 / 0.22 Flickr 89,250 899,756 10 500 7 (s) 0.50 / 0.25 / 0.25 Reddit 232,965 11,606,919 50 602 41 (s) 0.66 / 0.10 / 0.24 Yelp 716,847 6,977,410 10 300 100 (m) 0.75 / 0.10 / 0.15 Amazon 1,598,960 132,169,734 83 200 107 (m) 0.85 / 0.05 / 0.10 PPI (large version) 56,944 818,716 14 50 121 (m) 0.79 / 0.11 / 0.10 We open source GraphSAINT‡. We compare with six baselines: 1. vanilla GCN (Kipf & Welling, 2016), 2. GraphSAGE (Hamilton et al., 2017), 3. FastGCN (Chen et al., 2018b), 4. S-GCN (Chen et al., 2018a), 5. AS-GCN (Huang et al., 2018), and 6. ClusterGCN (Chiang et al., 2019). All baselines are executed with their ofﬁcially released code (see Appendix C.3 for downloadable URLs and commit numbers). Baselines and GraphSAINT are all implemented in Tensorﬂow with Python3. We run experiments on a NVIDIA Tesla P100 GPU (see Appendix C.1 for hardware speciﬁcation). 5.1 C OMPARISON WITH STATE-OF-THE -ART Table 2 and Figure 2 show the accuracy and convergence comparison of various methods. All results correspond to two-layer GCN models (for GraphSAGE, we use its mean aggregator). For a given dataset, we keep hidden dimension the same across all methods. We describe the detailed architecture and hyperparameter search procedure in Appendix C.3. The mean and conﬁdence interval of the accuracy values in Table 2 are measured by three runs under the same hyperparameters. The training time of Figure 2 excludes the time for data loading, pre-processing, validation set evaluation and model saving. Our pre-processing incurs little overhead in training time. See Appendix D.2 for cost of graph sampling. For GraphSAINT, we implement the graph samplers described in Section 3.4. In Table 2, “Node” stands for random node sampler; “Edge” stands for random edge sampler; “RW” stands for random walk sampler; “MRW” stands for multi-dimensional random walk sampler. Table 2: Comparison of test set F1-micro score with state-of-the-art methods Method PPI Flickr Reddit Yelp Amazon GCN 0.515 ±0.006 0.492 ±0.003 0.933 ±0.000 0.378 ±0.001 0.281 ±0.005 GraphSAGE 0.637 ±0.006 0.501 ±0.013 0.953 ±0.001 0.634 ±0.006 0.758 ±0.002 FastGCN 0.513 ±0.032 0.504 ±0.001 0.924 ±0.001 0.265 ±0.053 0.174 ±0.021 S-GCN 0.963 ±0.010 0.482 ±0.003 0.964 ±0.001 0.640 ±0.002 — ‡ AS-GCN 0.687 ±0.012 0.504 ±0.002 0.958 ±0.001 — ‡ — ‡ ClusterGCN 0.875 ±0.004 0.481 ±0.005 0.954 ±0.001 0.609 ±0.005 0.759 ±0.008 GraphSAINT-Node 0.960±0.001 0.507 ±0.001 0.962 ±0.001 0.641 ±0.000 0.782 ±0.004 GraphSAINT-Edge 0.981±0.007 0.510 ±0.002 0.966±0.001 0.653±0.003 0.807 ±0.001 GraphSAINT-RW 0.981±0.004 0.511±0.001 0.966±0.001 0.653±0.003 0.815±0.001 GraphSAINT-MRW 0.980±0.006 0.510 ±0.001 0.964 ±0.000 0.652 ±0.001 0.809 ±0.001 Table 3: Additional comparison with ClusterGCN (test set F1-micro score) PPI (large version) Reddit 2 ×512 5 ×2048 2 ×128 4 ×128 ClusterGCN 0.903 ±0.002 0.994 ±0.000 0.954 ±0.001 0.966 ±0.001 GraphSAINT 0.941±0.003 0.995±0.000 0.966±0.001 0.970±0.001 ‡Open sourced code: https://github.com/GraphSAINT/GraphSAINT ‡The codes throw runtime error on the large datasets (Yelp or Amazon). 7Published as a conference paper at ICLR 2020 0 20 40 600.4 0.6 0.8 1 Validation F1-micro PPI 0 10 20 30 40 0.44 0.46 0.48 0.5 0.52 Flickr 0 50 100 1500.9 0.92 0.94 0.96 0.98 Reddit 0 200 400 600 800 0.25 0.45 0.65 Yelp 0 200 400 0.2 0.4 0.6 0.8 Training time (second) Amazon GCN GraphSAGE FastGCN* S-GCN AS-GCN ClusterGCN GraphSAINT *: CPU execution time -RW Figure 2: Convergence curves of 2-layer models on GraphSAINT and baselines Clearly, with appropriate graph samplers, GraphSAINT achieves signiﬁcantly higher accuracy on all datasets. For GraphSAINT-Node, we use the same node probability as FastGCN. Thus, the accuracy improvement is mainly due to the switching from layer sampling to graph sampling (see “Remark” in Section 3.3). Compared with AS-GCN, GraphSAINT is signiﬁcantly faster. The sampler of AS-GCN is expensive to execute, making its overall training time even longer than vanilla GCN. We provide detailed computation complexity analysis on the sampler in Appendix D.2. For S-GCN on Reddit, it achieves similar accuracy as GraphSAINT, at the cost of over 9×longer training time. The released code of FastGCN only supports CPU execution, so its convergence curve is dashed. Table 3 presents additional comparison with ClusterGCN. We useL×f to specify the architecture, where Land f denote GCN depth and hidden dimension, respectively. The four architectures are the ones used in the original paper (Chiang et al., 2019). Again, GraphSAINT achieves signiﬁcant accuracy improvement. To train models with L> 2 often requires additional architectural tweaks. ClusterGCN uses its diagonal enhancement technique for the 5-layer PPI and 4-layer Reddit models. GraphSAINT uses jumping knowledge connection (Xu et al., 2018) for 4-layer Reddit. Evaluation on graph samplers From Table 2, random edge and random walk based samplers achieve higher accuracy than the random node sampler. Figure 3 presents sensitivity analysis on parameters of “RW”. We use the same hyperparameters (except the sampling parameters) and network architecture as those of the “RW” entries in Table 2. We ﬁx the length of each walker to2 (i.e., GCN depth), and vary the number of roots rfrom 250 to 2250. For PPI, increasing rfrom 250 to 750 signiﬁcantly improves accuracy. Overall, for all datasets, accuracy stabilizes beyond r= 750. 5.2 GraphSAINT ON ARCHITECTURE VARIANTS AND DEEP MODELS In Figure 4, we train a 2-layer and a 4-layer model of GAT (Veliˇckovi´c et al., 2017) and JK-net (Xu et al., 2018), by using minibatches of GraphSAGE and GraphSAINT respectively. On the two 4-layer architectures, GraphSAINT achieves two orders of magnitude speedup than GraphSAGE, indicating much better scalability on deep models. From accuracy perspective, 4-layer GAT-SAGE and JK- SAGE do not outperform the corresponding 2-layer versions, potentially due to the smoothening effect caused by the massive neighborhood size. On the other hand, with minibatches returned by our edge sampler, increasing model depth of JK-SAINT leads to noticeable accuracy improvement (from 0.966 of 2-layer to 0.970 of 4-layer). Appendix D.1 contains additional scalability results. 6 C ONCLUSION We have presented GraphSAINT, a graph sampling based training method for deep GCNs on large graphs. We have analyzed bias and variance of the minibatches deﬁned on subgraphs, and proposed 8Published as a conference paper at ICLR 2020 0 1,000 2,0000.4 0.6 0.8 1 Number of walkers Test F1-micro PPI Flickr Reddit Yelp Amazon Figure 3: Sensitivity analysis 100 102 104 0.93 0.94 0.95 0.96 0.97 Training time (second) Validation F1-micro GAT 100 102 104 JK-net GraphSAINT 2-layer GraphSAINT 4-layer GraphSAGE 2-layer GraphSAGE 4-layer Figure 4: GraphSAINT with JK-net and GAT (Reddit) normalization techniques and sampling algorithms to improve training quality. We have conducted extensive experiments to demonstrate the advantage of GraphSAINT in accuracy and training time. An interesting future direction is to develop distributed training algorithms using graph sampling based minibatches. After partitioning the training graph in distributed memory, sampling can be performed independently on each processor. Afterwards, training on the self-supportive subgraphs can signiﬁcantly reduce the system-level communication cost. To ensure the overall convergence quality, data shufﬂing strategy for the graph nodes and edges can be developed together with each speciﬁc graph sampler. Another direction is to perform algorithm-system co-optimization to accelerate the training of GraphSAINT on heterogeneous computing platforms (Zeng et al., 2018; Zeng & Prasanna, 2019). The resolution of “neighbor explosion” by GraphSAINT not only reduces the training computation complexity, but also improves hardware utilization by signiﬁcantly less data trafﬁc to the slow memory. In addition, task-level parallelization is easy since the light-weight graph sampling is completely decoupled from the GCN layer propagation. ACKNOWLEDGEMENT This material is based on work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract Number FA8750-17-C-0086 and National Science Foundation (NSF) under Contract Numbers CCF-1919289 and OAC-1911229. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of DARPA or NSF. REFERENCES Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard, Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolution architec- tures via sparsiﬁed neighborhood mixing. arXiv preprint arXiv:1905.00067, 2019. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. CoRR, abs/1312.6203, 2013. URL http://arxiv.org/abs/ 1312.6203. HongYun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques and applications. CoRR, abs/1709.07604, 2017. URL http://arxiv.org/abs/1709.07604. Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In ICML, pp. 941–949, 2018a. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR), 2018b. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An ef- ﬁcient algorithm for training deep and large graph convolutional networks. CoRR, abs/1905.07953, 2019. URL http://arxiv.org/abs/1905.07953. 9Published as a conference paper at ICLR 2020 Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016. Matthias Fey. Just jump: Dynamic neighborhood aggregation in graph neural networks. CoRR, abs/1904.04849, 2019. URL http://arxiv.org/abs/1904.04849. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, pp. 1416–1424, New York, NY , USA, 2018. ACM. ISBN 978-1-4503-5552-0. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30, pp. 1024–1034. 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. Pili Hu and Wing Cheong Lau. A survey and taxonomy of graph sampling. arXiv preprint arXiv:1308.5865, 2013. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558–4567, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907. Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Personalized embedding propa- gation: Combining neural networks on graphs with personalized pagerank. CoRR, abs/1810.05997, 2018. URL http://arxiv.org/abs/1810.05997. John Boaz Lee, Ryan A. Rossi, Xiangnan Kong, Sungchul Kim, Eunyee Koh, and Anup Rao. Higher- order graph convolutional networks. CoRR, abs/1809.07697, 2018. URL http://arxiv.org/ abs/1809.07697. Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 631–636. ACM, 2006. R. Li, J. X. Yu, L. Qin, R. Mao, and T. Jin. On random walk based graph sampling. In 2015 IEEE 31st International Conference on Data Engineering, pp. 927–938, April 2015. doi: 10.1109/ICDE. 2015.7113345. Haonan Lu, Seth H. Huang, Tian Ye, and Xiuyan Guo. Graph star net for generalized multi-task learning. CoRR, abs/1906.12330, 2019. URL http://arxiv.org/abs/1906.12330. Bruno Ribeiro and Don Towsley. Estimating and sampling graphs with multidimensional random walks. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pp. 390–403. ACM, 2010. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. 10Published as a conference paper at ICLR 2020 Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. CoRR, abs/1901.00596, 2019. URL http: //arxiv.org/abs/1901.00596. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, 2018a. ISBN 978-1-4503-5552-0. Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 4805–4815, USA, 2018b. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id= 3327345.3327389. Hanqing Zeng and Viktor Prasanna. GraphACT: Accelerating GCN training on CPU-FPGA hetero- geneous platforms. arXiv preprint arXiv:2001.02498, 2019. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Accurate, efﬁcient and scalable graph embedding. CoRR, abs/1810.11899, 2018. URL http: //arxiv.org/abs/1810.11899. Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated atten- tion networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018. Zhenpeng Zhou. Graph convolutional networks for molecules. CoRR, abs/1706.09916, 2017. URL http://arxiv.org/abs/1706.09916. A P ROOFS Proof of Proposition 3.1.Under the condition that vis sampled in a subgraph: E ( ζ(ℓ+1) v ) =E (∑ u∈V ˜Av,u αu,v ˜x(ℓ) u 1 u|v ) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u E ( 1 u|v ) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u P((u,v) sampled|vsampled) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u P((u,v) sampled) P(vsampled) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u pu,v pv (5) where the second equality is due to linearity of expectation, and the third equality (conditional edge probability) is due to the initial condition that vis sampled in a subgraph. It directly follows that, when αu,v = pu,v pv , E ( ζ(ℓ+1) v ) = ∑ u∈V ˜Av,u ˜x(ℓ) u 11Published as a conference paper at ICLR 2020 Proof of Theorem 3.2.Below, we use Cov (·) to denote covariance and Var(·) to denote variance. For independent edge sampling as deﬁned in Section 3.3, Cov ( 1 (ℓ1) e1 ,1 (ℓ2) e2 ) = 0,∀e1 ̸= e2. And for a full GCN on the subgraph, Cov ( 1 (ℓ1) e ,1 (ℓ2) e ) = pe −p2 e. To start the proof, we ﬁrst assume that the b(ℓ) e is one dimensional (i.e., a scalar) and denote it by b(ℓ) e . Now, Var(ζ) = ∑ e,ℓ ( b(ℓ) e pe )2 Var ( 1 (ℓ) e ) + 2 ∑ e,ℓ1<ℓ2 b(ℓ1) e b(ℓ2) e p2e Cov ( 1 (ℓ1) e ,1 (ℓ2) e ) = ∑ e,ℓ ( b(ℓ) e )2 pe − ∑ e,ℓ ( b(ℓ) e )2 + 2 ∑ e,ℓ1<ℓ2 b(ℓ1) e b(ℓ2) e p2e ( pe −p2 e ) = ∑ e (∑ ℓ b(ℓ) e )2 pe − ∑ e (∑ ℓ b(ℓ) e )2 (6) Let a given constant m= ∑ e pe be the expected number of sampled edges. By Cauchy-Schwarz in- equality: ∑ e ( ∑ ℓ b(ℓ) e ) 2 pe m= ∑ e (∑ ℓ b(ℓ) e√pe )2 ∑ e (√pe )2 ≥ (∑ e,ℓ b(ℓ) e )2 . The equality is achieved when ⏐⏐⏐ ∑ ℓ b(ℓ) e√pe ⏐⏐⏐∝√pe. i.e., variance is minimized when pe ∝ ⏐⏐⏐∑ ℓ b(ℓ) e ⏐⏐⏐. It directly follows that: pe = m ∑ e′ ⏐⏐⏐∑ ℓ b(ℓ) e′ ⏐⏐⏐ ⏐⏐⏐⏐⏐ ∑ ℓ b(ℓ) e ⏐⏐⏐⏐⏐ For the multi-dimensional case of b(ℓ) e , following similar steps as above, it is easy to show that the optimal edge probability to minimize ∑ i Var(ζi) (where iis the index for ζ’s dimensions) is: pe = m ∑ e′ ∑ ℓ b(ℓ) e′   ∑ ℓ b(ℓ) e  B S AMPLING ALGORITHM Algorithm 2 lists the four graph samplers we have integrated into GraphSAINT. The naming of the samplers follows that of Table 2. Note that the sampling parameters nand mspecify a budget rather than the actual number of nodes and edges in the subgraph Gs. Since certain nodes or edges in the training graph Gmay be repeatedly sampled under a single invocation of the sampler, we often have |Vs|<n for node and MRW samplers, |Vs|<2mfor edge sampler, and |Vs|<r ·hfor RW sampler. Also note that the edge sampler presented in Algorithm 2 is an approximate version of the independent edge sampler deﬁned in Section 3.4. Complexity (excluding the subgraph induction step) of the original version in Section 3.4 is O(|E|), while complexity of the approximate one is O(m). When m≪|E|, the approximate version leads to identical accuracy as the original one, for a given m. C D ETAILED EXPERIMENTAL SETUP C.1 H ARDWARE SPECIFICATION AND ENVIRONMENT We run our experiments on a single machine with Dual Intel Xeon CPUs (E5-2698 v4 @ 2.2Ghz), one NVIDIA Tesla P100 GPU (16GB of HBM2 memory) and 512GB DDR4 memory. The code is written in Python 3.6.8 (where the sampling part is written with Cython 0.29.2). We use Tensorﬂow 1.12.0 on CUDA 9.2 with CUDNN 7.2.1 to train the model on GPU. Since the subgraphs are sampled independently, we run the sampler in parallel on 40 CPU cores. 12Published as a conference paper at ICLR 2020 Algorithm 2Graph sampling algorithms by GraphSAINT Input: Training graph G(V,E); Sampling parameters: node budget n; edge budget m; number of roots r; random walk length h Output: Sampled graph Gs (Vs,Es) 1: function NODE (G,n) ⊿Node sampler 2: P(v) := ˜A:,v  2 /∑ v′∈V ˜A:,v′  2 3: Vs ←nnodes randomly sampled (with replacement) from Vaccording to P 4: Gs ←Node induced subgraph of Gfrom Vs 5: end function 6: function EDGE (G,m) ⊿Edge sampler (approximate version) 7: P((u,v)) := ( 1 deg(u) + 1 deg(v) ) /∑ (u′,v′)∈E ( 1 deg(u′) + 1 deg(v′) ) 8: Es ←medges randomly sampled (with replacement) from Eaccording to P 9: Vs ←Set of nodes that are end-points of edges in Es 10: Gs ←Node induced subgraph of Gfrom Vs 11: end function 12: function RW(G,r,h) ⊿Random walk sampler 13: Vroot ←rroot nodes sampled uniformly at random (with replacement) from V 14: Vs ←Vroot 15: for v∈Vroot do 16: u←v 17: for d= 1to hdo 18: u←Node sampled uniformly at random from u’s neighbor 19: Vs ←Vs ∪{u} 20: end for 21: end for 22: Gs ←Node induced subgraph of Gfrom Vs 23: end function 24: function MRW(G,n,r) ⊿Multi-dimensional random walk sampler 25: VFS ←rroot nodes sampled uniformly at random (with replacement) from V 26: Vs ←VFS 27: for i= r+ 1to ndo 28: Select u∈VFS with probability deg(u)/∑ v∈VFS deg(v) 29: u′←Node randomly sampled from u’s neighbor 30: VFS ←(VFS \\{u}) ∪{u′} 31: Vs ←Vs ∪{u} 32: end for 33: Gs ←Node induced subgraph of Gfrom Vs 34: end function 13Published as a conference paper at ICLR 2020 100 101 102 103 104 105 10−6 10−4 10−2 100 Degree P(degree ≥k) PPI Flickr Reddit Yelp Amazon Figure 5: Degree Distribution C.2 A DDITIONAL DATASET DETAILS Here we present the detailed procedures to prepare the Flickr, Yelp and Amazon datasets. The Flickr dataset originates from NUS-wide §. The SNAP website ¶collected Flickr data from four different sources including NUS-wide, and generated an un-directed graph. One node in the graph represents one image uploaded to Flickr. If two images share some common properties (e.g., same geographic location, same gallery, comments by the same user, etc.), there is an edge between the nodes of these two images. We use as the node features the 500-dimensional bag-of-word representation of the images provided by NUS-wide. For labels, we scan over the 81 tags of each image and manually merged them to 7 classes. Each image belongs to one of the 7 classes. The Yelp dataset is prepared from the rawjson data of businesses, users and reviews provided in the open challenge website∥. For nodes and edges, we scan the friend list of each user in the raw json ﬁle of users. If two users are friends, we create an edge between them. We then ﬁlter out all the reviews by each user and separate the reviews into words. Each review word is converted to a 300-dimensional vector using the Word2Vec model pre-trained on GoogleNews∗∗. The word vectors of each node are added and normalized to serve as the node feature (i.e., xv). As for the node labels, we scan the raw json ﬁle of businesses, and use the categories of the businesses reviewed by a user vas the multi-class label of v. For the Amazon dataset, a node is a product on the Amazon website and an edge (u,v) is created if products uand vare bought by the same customer. Each product contains text reviews (converted to 4-gram) from the buyer. We use SVD to reduce the dimensionality of the 4-gram representation to 200, and use the obtained vectors as the node feature. The labels represent the product categories (e.g., books, movies, shoes). Figure 5 shows the degree distribution of the ﬁve graphs. A point (k,p) in the plot means the probability of a node having degree at least kis p. C.3 A DDITIONAL DETAILS IN EXPERIMENTAL CONFIGURATION Table 4 summarizes the URLs to download the baseline codes. The optimizer for GraphSAINT and all baselines is Adam (Kingma & Ba, 2014). For all baselines and datasets, we perform grid search on the hyperparameter space deﬁned by: •Hidden dimension: {128,256,512} §http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm ¶https://snap.stanford.edu/data/web-flickr.html ∥https://www.yelp.com/dataset ∗∗https://code.google.com/archive/p/word2vec/ 14Published as a conference paper at ICLR 2020 Table 4: URLs and commit number to run baseline codes Baseline URL Commit Vanilla GCN github.com/williamleif/GraphSAGE a0fdef GraphSAGE github.com/williamleif/GraphSAGE a0fdef FastGCN github.com/matenure/FastGCN b8e6e6 S-GCN github.com/thu-ml/stochastic_gcn da7b78 AS-GCN github.com/huangwb/AS-GCN 5436ec ClusterGCNgithub.com/google-research/google-research/tree/master/cluster_gcn99021e Table 5: Training conﬁguration of GraphSAINT for Table 2 Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length Node PPI 0.01 0.0 6000 — — — Flickr 0.01 0.2 8000 — — — Reddit 0.01 0.1 8000 — — — Yelp 0.01 0.1 5000 — — — Amazon 0.01 0.1 4500 — — — Edge PPI 0.01 0.1 — 4000 — — Flickr 0.01 0.2 — 6000 — — Reddit 0.01 0.1 — 6000 — — Yelp 0.01 0.1 — 2500 — — Amazon 0.01 0.1 — 2000 — — RW PPI 0.01 0.1 — — 3000 2 Flickr 0.01 0.2 — — 6000 2 Reddit 0.01 0.1 — — 2000 4 Yelp 0.01 0.1 — — 1250 2 Amazon 0.01 0.1 — — 1500 2 MRW PPI 0.01 0.1 8000 — 2500 — Flickr 0.01 0.2 12000 — 3000 — Reddit 0.01 0.1 8000 — 1000 — Yelp 0.01 0.1 2500 — 1000 — Amazon 0.01 0.1 4500 — 1500 — •Dropout: {0.0,0.1,0.2,0.3} • Learning rate: {0.1,0.01,0.001,0.0001} The hidden dimensions used for Table 2, Figure 2, Figure 3 and Figure 4 are: 512 for PPI, 256 for Flickr, 128 for Reddit, 512 for Yelp and 512 for Amazon. All methods terminate after a ﬁxed number of epochs based on convergence. We save the model producing the highest validation set F1-micro score, and reload it to evaluate the test set accuracy. For vanilla GCN and AS-GCN, we set the batch size to their default value 512. For GraphSAGE, we use the mean aggregator with the default batch size 512. For S-GCN, we set the ﬂag -cv -cvd (which stand for “control variate” and “control variate dropout”) with pre-computation of the ﬁrst layer aggregation. According to the paper (Chen et al., 2018a), such pre-computation signiﬁcantly reduces training time without affecting accuracy. For S-GCN, we use the default batch size 1000, and for FastGCN, we use the default value 400. For ClusterGCN, its batch size is determined by two parameters: the cluster size and the number of clusters per batch. We sweep the cluster size from 500 to 10000 with step 500, and the number of clusters per batch from {1,10,20,40}to determine the optimal conﬁguration for each dataset / architecture. Considering that for ClusterGCN, the cluster structure may be sensitive to the cluster size, and for FastGCN, the minibatch connectivity may increase with the sample size, we present additional experimental results to reveal the relation between accuracy and batch size in Appendix D.3. 15Published as a conference paper at ICLR 2020 Table 6: Training conﬁguration of GraphSAINT for Table 3 Arch. Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length 2×512 MRW PPI (large) 0.01 0.1 1500 — 300 — 5×2048 RW PPI (large) 0.01 0.1 — — 3000 2 2×128 Edge Reddit 0.01 0.1 — 6000 — — 4×128 Edge Reddit 0.01 0.2 — 11000 — — Table 7: Training conﬁguration of GraphSAINT for Figure 4 (Reddit) 2-layer GAT-SAINT 4-layer GAT-SAINT 2-layer JK-SAINT 4-layer JK-SAINT Hidden dimension 128 128 128 128 AttentionK 8 8 — — Aggregation⨁ — — Concat. Concat. Sampler RW RW Edge Edge (root: 3000; length: 2) (root: 2000; length: 4) (budget: 6000) (budget: 11000) Learning rate 0.01 0.01 0.01 0.01 Dropout 0.2 0.2 0.1 0.2 Conﬁguration of GraphSAINT to reproduce Table 2 results is shown in Table 5. Conﬁguration of GraphSAINT to reproduce Table 3 results is shown in Table 6. Below we describe the conﬁguration for Figure 4. The major difference between a normal GCN and a JK-net (Xu et al., 2018) is that JK-net has an additional ﬁnal layer that aggregates all the output hidden features of graph convolutional layers 1 to L. Mathematically, the additional aggregation layer outputs the ﬁnal embedding xJK as follows: xJK = σ ( WT JK · L⨁ ℓ=1 x(ℓ) v ) (7) where based on Xu et al. (2018), ⨁is the vector aggregation operator: max-pooling, concatenation or LSTM (Hochreiter & Schmidhuber, 1997) based aggregation. The graph attention of GAT (Veli ˇckovi´c et al., 2017) calculates the edge weights for neighbor aggregation by an additional neural network. With multi-head ( K) attention, the layer- (ℓ−1) features propagate to layer-(ℓ) as follows: x(ℓ) v =  K k=1 σ   ∑ u∈neighbor(v) αk u,vWkx(ℓ−1) v   (8) where ∥is the vector concatenation operation, and the coefﬁcient αis calculated with the attention weights ak by: αk u,v = LeakyReLU (( ak)T [ Wkxu∥Wkxv ]) (9) Note that the αcalculation is slightly different from the original equation in Veliˇckovi´c et al. (2017). Namely, GAT-SAINT does not normalize αby softmax across all neighbors of v. We make such modiﬁcation since under the minibatch setting, node vdoes not see all its neighbors in the training graph. The removal of softmax is also seen in the attention design of Huang et al. (2018). Note that during the minibatch training, GAT-SAINT further applies another edge coefﬁcient on top of attention for aggregator normalization. Table 7 shows the conﬁguration of the GAT-SAINT and JK-SAINT curves in Figure 4. 16Published as a conference paper at ICLR 2020 2 3 4 5 60 2 4 6 8 GCN depth Normalized training time GraphSAINT: Reddit S-GCN: Reddit GraphSAINT: Yelp S-GCN: Yelp Figure 6: Comparison of training efﬁciency PPI Flickr Reddit Yelp Amazon 0 0.5 1 1.5 Fraction of training time Node Edge RW MRW Figure 7: Fraction of training time on sampling D A DDITIONAL EXPERIMENTS D.1 T RAINING EFFICIENCY ON DEEP MODELS We evaluate the training efﬁciency for deeper GCNs. We only compare with S-GCN, since implemen- tations for other layer sampling based methods have not yet supported arbitrary model depth. The batch size and hidden dimension are the same as Table 2. On the two large graphs (Reddit and Yelp), we increase the number of layers and measure the average time per minibatch execution. In Figure 6, training cost of GraphSAINT is approximately linear with GCN depth. Training cost of S-GCN grows dramatically when increasing the depth. This reﬂects the “neighbor explosion” phenomenon (even though the expansion factor of S-GCN is just 2). On Yelp, S-GCN gives “out-of-memory” error for models beyond 5 layers. D.2 C OST OF SAMPLING AND PRE-PROCESSING Cost of graph samplers ofGraphSAINT Graph sampling introduces little training overhead. Let ts be the average time to sample one subgraph on a multi-core machine. Let tt be the average time to perform the forward and backward propagation on one minibatch on GPU. Figure 7 shows the ratio ts/tt for various datasets. The parameters of the samplers are the same as Table 2. For Node, Edge and RW samplers, we observe that time to sample one subgraph is in most cases less than 25% of the training time. The MRW sampler is more expensive to execute. Regarding the complete pre-processing procedure, we repeatedly run the sampler for N = 50·|V| /|Vs|times before training, to estimate the node and edge probability as discussed in Section 3.2 (where |Vs|is the average subgraph size). These sampled subgraphs are reused as training minibatches. Thus, if training runs for more than N iterations, the pre-processing is nearly zero-cost. Under the setting of Table 2, pre-processing on PPI and Yelp and Amazon does not incur any overhead in training time. Pre-processing on Flickr and Reddit (with RW sampler) takes less than 40% and 15% of their corresponding total training time. Cost of layers sampler of AS-GCNAS-GCN uses an additional neural network to estimate the conditional sampling probability for the previous layer. For a node v already sampled in layer ℓ, features of layer-(ℓ−1) corresponding to all v’s neighbors need to be fed to the sampling neural network to obtain the node probability. For sake of analysis, assume the sampling network is a single layer MLP, whose weight WMLP has the same shape as the GCN weights W(ℓ). Then we can show, for a L-layer GCN on a degree-dgraph, per epoch training complexity of AS-GCN is approximately γ = (d·L) /∑L−1 ℓ=0 dℓ times that of vanilla GCN. For L = 2, we have γ ≈2. This explains the observation that AS-GCN is slower than vanilla GCN in Figure 2. Additional, Table 8 shows the training time breakdown for AS-GCN. Clearly, its sampler is much more expensive than the graph sampler of GraphSAINT. 17Published as a conference paper at ICLR 2020 Table 8: Per epoch training time breakdown for AS-GCN Dataset Sampling time (sec) Forward / Backward propagation time (sec) PPI 1.1 0.2 Flickr 5.3 1.1 Reddit 20.7 3.5 Cost of clustering of ClusterGCNClusterGCN uses the highly optimized METIS software††to perform clustering. Table 9 summarizes the time to obtain the clusters for the ﬁve graphs. On the large and dense Amazon graph, the cost of clustering increase dramatically. The pre-processing time of ClusterGCN on Amazon is more than 4×of the total training time. On the other hand, the sampling cost of GraphSAINT does not increase signiﬁcantly for large graphs (see Figure 7). Table 9: Clustering time of ClusterGCN PPI Flickr Reddit Yelp Amazon Time (sec) 2.2 11.6 40.0 106.7 2254.2 Taking into account the pre-processing time, sampling time and training time altogether, we sum- marize the total convergence time of GraphSAINT and ClusterGCN in Table 10 (corresponding to Table 2 conﬁguration). On graphs that are large and dense (e.g., Amazon), GraphSAINT achieves signiﬁcantly faster convergence. Note that both the sampling of GraphSAINT and clustering of ClusterGCN can be performed ofﬂine. Table 10: Comparison of total convergence time (pre-processing + sampling + training, unit: second) PPI Flickr Reddit Yelp Amazon GraphSAINT-Edge 91.0 7.0 16.6 273.9 401.0 GraphSAINT-RW 103.6 7.5 17.2 310.1 425.6 ClusterGCN 163.2 12.9 55.3 256.0 2804.8 D.3 E FFECT OF BATCH SIZE Table 11 shows the change of test set accuracy with batch sizes. For each row of Table 11, we ﬁx the batch size, tune the other hyperparameters according to Appendix C.3, and report the highest test set accuracy achieved. For GraphSAGE, S-GCN and AS-GCN, their default batch sizes (512,1000 and 512, respectively) lead to the highest accuracy on all datasets. For FastGCN, increasing the default batch size (from 400 to 4000) leads to noticeable accuracy improvement. For ClusterGCN, different datasets correspond to different optimal batch sizes. Note that the accuracy in Section 5.1 is already tuned by identifying the optimal batch size on a per graph basis. For FastGCN, intuitively, increasing batch size may help with accuracy improvement since the minibatches may become better connected. Such intuition is veriﬁed by the rows of 400 and 2000. However, increasing the batch size from 2000 to 4000 does not further improve accuracy signiﬁcantly. For ClusterGCN, the optimal batch size depends on the cluster structure of the training graph. For PPI, small batches are better, while for Amazon, batch size does not have signiﬁcant impact on accuracy. For GraphSAGE, overly large batches may have negative impact on accuracy due to neighbor explosion. Approximately, GraphSAGE expand 10×more neighbors per layer. For a 2-layer GCN, a size 2 ×103 minibatch would then require the support of 2 ×105 nodes from the ††http://glaros.dtc.umn.edu/gkhome/metis/metis/download ∗Default batch size ¶The training does not converge. ‡The codes throw runtime error on the large datasets (Yelp or Amazon). 18Published as a conference paper at ICLR 2020 Table 11: Test set F1-micro for the baselines under various batch sizes Method Batch size PPI Flickr Reddit Yelp Amazon GraphSAGE 256 0.600 0.474 0.934 0.563 0.428 512∗ 0.637 0.501 0.953 0.634 0.758 1024 0.610 0.482 0.935 0.632 0.705 2048 0.625 0.374 0.936 0.563 0.447 FastGCN 400∗ 0.513 0.504 0.924 0.265 0.174 2000 0.561 0.506 0.934 0.255 0.196 4000 0.564 0.507 0.934 0.260 0.195 S-GCN 500 0.519 0.462 — ¶ — ¶ — ‡ 1000∗ 0.963 0.482 0.964 0.640 — ‡ 2000 0.646 0.482 0.949 0.614 — ‡ 4000 0.804 0.482 0.949 0.594 — ‡ 8000 0.694 0.481 0.950 0.613 — ‡ AS-GCN 256 0.682 0.504 0.950 — ‡ — ‡ 512∗ 0.687 0.504 0.958 — ‡ — ‡ 1024 0.687 0.502 0.951 — ‡ — ‡ 2048 0.670 0.502 0.952 — ‡ — ‡ ClusterGCN 500 0.875 0.481 0.942 0.604 0.752 1000 0.831 0.478 0.947 0.602 0.756 1500 0.865 0.480 0.954 0.602 0.752 2000 0.828 0.469 0.954 0.609 0.759 2500 0.849 0.476 0.954 0.598 0.745 3000 0.840 0.473 0.954 0.607 0.754 3500 0.846 0.473 0.952 0.602 0.754 4000 0.853 0.472 0.949 0.605 0.756 input layer. Note that the full training graph size of Reddit is just around 1.5 ×105. Thus, no matter which nodes are sampled in the output layer, GraphSAGE would almost always propagate features within the full training graph for initial layers. We suspect this would lead to difﬁculties in learning. For S-GCN, with batch size of 500, it fails to learn properly on Reddit and Yelp. The accuracy ﬂuctuates in a region of very low value, even after appropriate hyperparameter tuning. For AS-GCN, its accuracy is not sensitive to the batch size, since AS-GCN addresses neighbor explosion and also ensures good inter-layer connectivity within the minibatch. 19",
      "meta_data": {
        "arxiv_id": "1907.04931v4",
        "authors": [
          "Hanqing Zeng",
          "Hongkuan Zhou",
          "Ajitesh Srivastava",
          "Rajgopal Kannan",
          "Viktor Prasanna"
        ],
        "published_date": "2019-07-10T21:11:13Z",
        "pdf_url": "https://arxiv.org/pdf/1907.04931v4.pdf"
      }
    },
    {
      "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection",
      "abstract": "While the self-attention mechanism has been widely used in a wide variety of\ntasks, it has the unfortunate property of a quadratic cost with respect to the\ninput length, which makes it difficult to deal with long inputs. In this paper,\nwe present a method for accelerating and structuring self-attentions: Sparse\nAdaptive Connection (SAC). In SAC, we regard the input sequence as a graph and\nattention operations are performed between linked nodes. In contrast with\nprevious self-attention models with pre-defined structures (edges), the model\nlearns to construct attention edges to improve task-specific performances. In\nthis way, the model is able to select the most salient nodes and reduce the\nquadratic complexity regardless of the sequence length. Based on SAC, we show\nthat previous variants of self-attention models are its special cases. Through\nextensive experiments on neural machine translation, language modeling, graph\nrepresentation learning and image classification, we demonstrate SAC is\ncompetitive with state-of-the-art models while significantly reducing memory\ncost.",
      "full_text": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection Xiaoya Li*1, Yuxian Meng*1, Mingxin Zhou1, Qinghong Han1, Fei Wu2 and Jiwei Li 1 1 Shannon.AI 2 Computer Science Department, Zhejiang University {xiaoya_li,yuxian_meng,mingxin_zhou,qinghong_han,jiwei_li}@shannonai.com Abstract While the self-attention mechanism has been widely used in a wide variety of tasks, it has the unfortunate property of a quadratic cost with respect to the input length, which makes it difﬁcult to deal with long inputs. In this paper, we present a method for accelerating and structuring self-attentions: Sparse Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and attention operations are performed between linked nodes. In contrast with previous self-attention models with pre-deﬁned structures (edges), the model learns to construct attention edges to improve task-speciﬁc performances. In this way, the model is able to select the most salient nodes and reduce the quadratic complexity regardless of the sequence length. Based on SAC, we show that previous variants of self-attention models are its special cases. Through extensive experiments on neural machine translation, language modeling, graph representation learning and image classiﬁcation, we demonstrate SAC is competitive with state-of-the-art models while signiﬁcantly reducing memory cost. 1 Introduction The self-attention mechanism has proved to beneﬁt a wide range of ﬁelds and tasks, such as natural language processing (Vaswani et al., 2017; Dai et al., 2019), computer vision (Xu et al., 2015; Jaderberg et al., 2015; Bello et al., 2019; Parmar et al., 2019), video classiﬁcation (Wang et al., 2018) and graph modeling (Veliˇckovi´c et al., 2018; Lee et al., 2019). The attention mechanism allows the model to attend to different parts of the input and capture the most salient features, and provides with the ability to handle dependencies between elements across long distances. Two conspicuous problems stand out with the self-attention mechanism: (1) the memory complexity is quadratic with respect to the input length, making it infeasible to handle long sequences; and (2) the self-attention operations are performed in a pre-deﬁned structure (usually fully-connected), which not only is costly but also lacks for the ability to learn the optimal attention structure optimized for the performance in the downstream tasks (Child et al., 2019; Sukhbaatar et al., 2019). These observations indicate that the fully-connected structure for self-attention operations can be replaced by a more sparse one where only speciﬁc edges are constructed for attention operations. In this paper, we present Sparse Adaptive Connection (SAC), which replaces the fully-connected structure in self-attention with a sparse graph-like structure adapted to different tasks. In SAC, we regard the input as a graph where a node could be a token in the sequence or an ﬂattened feature map of an image, and an edge between a pair of nodes represents they can attend to each other. To select edges, we propose an Edge Predictor which utilizes an LSTM model to dynamically predict pairs of nodes that represent two ends of an edge. In contrast with previous self-attention models with pre-deﬁned attention structures (edges), SAC learns to construct attention edges adaptively, which are optimized to improve task-speciﬁc performances using reinforcement learning models. We 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2003.09833v3  [cs.CL]  29 Sep 2020evaluate the proposed model on four tasks: neural machine translation, language modeling, graph representation learning and image classiﬁcation, and demonstrate that SAC learns to select the most salient nodes to attend to each other and is free from heavy memory cost while achieving strong performances. 2 Related Work Many recent researches have focused on modifying the structure of self-attention to relieve the computation and memory burden. Transformer-XL (Dai et al., 2019) enables learning long-term dependency by introducing a segment-level recurrence mechanism and a novel relative position encoding scheme (Shaw et al., 2018). (Guo et al., 2019) proposed Star Transformer, a variant of Transformer which replaces the fully-connected structure in self-attention with a star-shaped topology, where all tokens are connected with a relay node. (Child et al., 2019; Kitaev et al., 2020) suggest sparsifying Transformer by focusing only on a fraction of attention connections. (Child et al., 2019) introduced sparse factorizations of the attention matrix, which scale as O(np√n) with the sequence length, and a set of sparse attention kernels which efﬁciently compute subsets of the attention matrix. (Kitaev et al., 2020) proposed Reformer, a more efﬁcient yet complicated method for computing self-attention, where the dot-product attention is replaced by one that uses a locality-sensitive hashing, reducing the complexity from O(n2) to O(nlog n). (Ye et al., 2019) partitioned the sequence to different multi-scale spans and attended the context information from ﬁne-grain to coarse-grain as the relative distance increases. All above works rely on manually designed structures. Our work is inspired by (Sukhbaatar et al., 2019), which takes a step forward and uses a novel adaptive self- attention mechanism that can learn its optimal attention span for each head, and (Correia et al., 2019) which proposed adaptively sparse Transformer. Different from Yang et al. (2018), we use an LSTM to predict attention links which gives us ﬁner control of how sparse we want self-attention to be. Graph neural networks (GNNs) are known at learning local contextual information by encoding attribute features (Kipf and Welling, 2016; Hamilton et al., 2017b), but they are not able to explic- itly distinguish the most salient nodes from all its neighbors, neither can they directly attend to the nodes that are beyond one-hop away. Much work has investigated the effect of attention on GNNs (Veliˇckovi´c et al., 2018; Abu-El-Haija et al., 2018; Lee et al., 2018; Veliˇckovi´c et al., 2019). (Veliˇckovi´c et al., 2018) extended self-attention to graphs by enabling each node to attend over its neighbors, achieving state-of-the-art results for semi-supervised node classiﬁcation tasks. But they simply applied self-attention over graphs to all neighbors of a node, which might be a problem when dealing with large and noisy graphs where only few neighbors need to be aggregated. (Ye and Ji, 2019) proposed Sparse Graph Attention Network which uses a binary gate to control whether each edge should be engaged. However, these works lack ability to aggregate long-range dependencies in graphs, and they only consider neighbors that are one hop away. Various methods have been proposed to tackle this issue (Ye et al., 2020; Zhang et al., 2020; Pei et al., 2020). Similar to graphs, (Bello et al., 2019) introduced a novel two-dimensional relative self-attention mechanism for images and augmented convolutional operators with this self-attention method, showing systematic improvements on both image classiﬁcation and object detection tasks across a wide range of architectures. 3 Background: Self-Attention Given a set of nodes1 {e1,··· ,eN}as inputs, self-attention iteratively computes the representation of ei in the l-th layer by attending to all its neighbors N(ei), which is deﬁned as follows: ˜hl i = ∑ ej∈N(ei) αijvl−1 j , αij = softmax   ( ql−1 i )T kl−1 j√ d   and ql−1 i = WQhl−1 i , kl−1 j = WKhl−1 j , vl−1 j = WVhl−1 j (1) where dis the hidden dimension, WQ,WK,WV are learnable parameters and q,k,v correspond to queries, keys and values, respectively. The multi-head mechanism linearly projects the queries, keys 1We use the term “node’ in a broad sense of denoting any particular unit in text, images or graphs. 2and values multiple times with different learned linear projections, and then performs self-attention in parallel, after which the results are concatenated and again projected: hl i = Concat(˜hl,1 i ,··· ,˜hl,m i )WO (2) where the superscript 1,···,m denotes the head number, and WO is learnable parameters. After L iterations, we obtain the ﬁnal representation for each node hL i . 4 Sparse Adaptive Connection for Self-Attention The key point in SAC is to use to an LSTM edge predictor to predict edges for self-attention operations between nodes, where a node could be a token in the sequence or an ﬂattened feature map of an image. Self-attention operations are performed between linked nodes instead of in a fully-connected manner. The LSTM edge predictor is optimized to improve task-speciﬁc performances using reinforcement learning models. 4.1 LSTM Edge Predictor In SAC, an edge predictor is used to construct edges between nodes for self-attention operations. Suppose that we are given a set of nodes {e1,··· ,eN}with no edge between any pair of nodes when initialization, our aim is to generate edges using this edge predictor, with the total number αN for each layer, where αis a hyperparameter deciding how many edges should be constructed for each node on average. The Edge Predictor uses an LSTM model as a backbone and sequentially predicts edges. The prediction of an edge is decoupled into the prediction of the original node and the destination node pair. More formally, the input to Edge Predictor is a special token “[SOS]”, and the model proceeds to predict the original node and destination node of all edges (2αN nodes in total) for the ﬁrst layer, denoted by {y1 1,y1 2,··· ,y1 2αN}, where the superscript denoted the index of the layer and the subscript denoted the index of the predicted node. At each time step, the input to the LSTM model is the representation hyt for the node that has just been predicted. Then it is combined with the previously constructed representation gt to obtain gt+1 representing the current time-step using LSTMs, and gt+1 is used to predict the following node using the softmax function. The projection matrix before softmax W shares embeddings with node representations, where each column wi is the vector representation for node ei. The probability of predicting node yt+1 given gt+1 is thus given by: p(yt+1 = ei) = exp (gT t+1 ·wi)∑ jexp (gT t+1 ·wj) (3) This process is repeated 2αN times. After the end of αN edge predictions, we update the representa- tion for each node based on self-attention as will be detailed in Section 4.1 for different tasks, and proceed to the next layer. For node predictions in the following layer, the initial input now becomes hidden state for the last time-step of the previous layer. The entire process is repeated Ltimes, where Ldenotes the number of self-attention layers and the resulted nodes in layerlare {yl 1,yl 2,··· ,yl 2αN}. Compared to separately predicting edges for each node, this approach is more ﬂexible and gives us ﬁner control of the total number of edges we would like to construct. More importantly, this process is aware of previous constructed edges, both in the current layer and previous layers. The recurrent edge predictor is shown in Figure 1(b). We implement it using a single-layer LSTM model. Once having constructed all edges for each layer, we can immediately obtain the set of neighbors N(en i) for each node ei in the n-th layer. Self-attention operations with multi-head mechanism are then performed on its neighbors for each node. For text tasks, we regard each token as a node. For graph-like structures, we treat nodes in the original graph as nodes. For images, the input (H,W,F in) dimensional sensor is reshaped to a HW ×Fin matrix, where each row can be thought as a node by our deﬁnition. 4.2 Distance Encoding The input graph intrinsically displays some degree of structures. For example, in a sequence of natural language tokens, the relative distance between two tokens in the sequence or the corresponding parse 3𝑒𝑖 𝑒1 𝑒2 𝑒3 𝑒4 i really like cats Edge  Predictor 𝑒𝑖 𝑒1 𝑒2 𝑒3 𝑒4 Self- Attention Self- Attention (a) (b) e1 e3 e3 e2 e2 e4 Distance Encodings 𝐠6 ×( )+ Node Encodings 𝐰1,𝐰2,𝐰3,𝐰4 𝐯2,𝐯0,𝐯1,𝐯-1 layer 𝑛 Figure 1: An illustration of the proposed Sparse Apdative Connection. (a) shows the process of SAC to construct edges and then perform self-attention on these edges (Red is for text and green is for graphs). (b) shows the edge prediction process of (a) with distance encodings. When predicting time-step 6, the word embeddings are added with distance encodings. tree encodes structural information. As another example, in the task of node representation learning in graphs (Yang et al., 2016b; Veliˇckovi´c et al., 2017; Xu et al., 2018), the graph originally comes with the node edges. The LSTM edge predictor described above ignores this structure. To leverage original structural information, we propose distance encodings to incorporate graph structure into the edge predictor. Distance encodings only affect the destination node predictions. In contrast to only using node embedding matrix W, we add an extra distance matrix V that encodes distance information to the original projection matrix W, giving V + W. Each column in V is its corresponding distance representation to the current original node. For example in Figure 1, at time-step 6, when two edges (e1,e3),(e3,e2), and one origin node e2 have been generated, we are to use g5 ∈Rd, the output of the LSTM model at time-step 5, to predict the node at time-step 6. According to the original structure, the distance between e2 (the current origin node) and all the nodes e1,e2,e3,e4 by far are 2, 0, 1, and -1 respectively, where -1 means inability to reach. The distance vectors are thus v2,v0,v1,v−1, which are vectors of size Rd to be learned. Intuitively, this process also discourages generating duplicate edges and leverages the original structural information. In contrast to Veliˇckovi´c et al. (2017) where attention operations are only performed between nodes with literal edges in the original graph, SAC offers the ﬂexibility in leveraging the original graph structure and inﬂuence from the training signals. Additionally, SAC allows for more convenient information exchange between similar nodes that are far away in terms of distance in the original graph structure, which is because the connection construction stage has the ability to connect any pair nodes in the graph. This ability potentially leads to better performances. 4.3 Training and Test Directly training the edge predictor is impractical since we have no access to the ground-truth edges. We use REINFORCE, which is an instance of a broader class of policy gradient methods for optimization. The main idea is to use reinforcement learning to discover the best edge connections for self-attention operations. Each action ais the node predicted by edge predictor. Let Θ denote parameters of the edge predictor and Φ denote the parameters of the main network which maps an input to its ﬁnal label based on a pre-deﬁned self-attention structure. Under the framework of reinforcement learning, we ask the edge predictor to maximize its reward R(Θ), which is the log probability of predicting the correct label, e.g., for neural machine translation the reward Ris the average log probability of golden target tokens; for image classiﬁcation, the reward the log probability of the correct label. Consider the simple case where different attention layers use the same node connections, by sampling a sequence of nodes from the edge predictor, we are able to update the parameters in edge predictor using policy gradients: ∇J(Θ) = 2αN∑ i=1 ∇log p(ai|a1:i−1; Θ)(R(Θ) −b) (4) 4layer 𝑛 layer 𝑛 −1 Vanilla Self-attention Transformer-XL Seg-Length=2 BT-Transformer layer 𝑛 +1 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 Adaptive Span S=2 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 𝑒1 𝑒2 𝑒3 𝑒4 Figure 2: Connection of SAC to other methods for computing self-attention. where bdenotes the baseline which is the average of the previous rewards. Φ is updated directly based on the log-likelihood. At test time, edges are decoded using beam search. We use a beam size of 5 for all models. 4.4 Variants of Edge Predictor The vanilla version of the Edge Predictor can be further regulated, simpliﬁed or expanded with prior knowledge for preferable graph structures. All layers sharing the same structure To reduce the computational cost and RL search space, we can enforce the edge structure to be the same for all layers, where the process is only executed once instead of Ltimes. We adopt this strategy for all settings to reduce the search space. All nodes connected in each layer To enforce each node to be connected in each layer, for each node ei, it is repeatedly fed to the predictor αtimes as the original node, and we only predict the destination node. The graph can be either directed graph or undirected graph, depending on how we want self-attention to be computed. Different heads attending to different contexts (head adaptive for short) Sukhbaatar et al. (2019) shows that it is beneﬁcial if different heads attend to different spans (some focusing on the recent history, while others focusing the whole available context). We can also augment the model by assigning each head with a edge predictor, providing the ﬂexibility that different heads can attend to different chunks of context. We sequentially predict all input and output nodes for each head, and the prediction of 2αN nodes are repeated Htimes. In this way, the prediction model for the current head is aware of the information of all previous heads. A head speciﬁc embedding is appended to the node embedding in LSTMs to let the model be aware of the current head. Since this strategy signiﬁcantly increases the search space in RL, we empirically ﬁnd that it helps some settings, but not always. 4.5 Connection to Existing Methods In this subsection, we describe the connection between SAC and previous variants of self-attentions, and show that these variants computing self-attention can be obtained through SAC if we slightly modify the edge predictor. For ease of exposition, we use EP(e) ={(ei,ej)}to denote the collection of all edges for self-attention operations. Connection to vanilla self-attention (Vaswani et al., 2017) The vanilla self-attention links each pair of nodes, where EP(e) can be described as EP(e) ={(ei,ej)|i∈[1,N]}. Connection to Transformer-XL (Dai et al., 2019) Transformer-XL treats the text in a segment- by-segment style. Self-attention operations are performed between nodes within the same segment. EP(e) can be described as EP(e) ={(ei,ej)|i∈[1,N]; j ∈Segment(i)}. Connection to Adaptive Span Transformer (Sukhbaatar et al., 2019) Adaptive Span Trans- former learns an optimal attention span for each head. Suppose the span size assigned to head tis s, then EP(e,t) can be described by: EP(e)={(ei,ej)|i∈[1,N]; j = i,i −1,··· ,i −s+ 1;span = t}. Connection to BP-Transformer (Ye et al., 2019) BP-Transformer constructed a tree-like graph by adding span nodes apart from token nodes. There are 2N −1 nodes in total, where N is the sequence length. In BP-Transformer, each token (leaf) node attends to each span (non-leaf) node that includes it, which we refer to as Ancestor(ei) for node ei. It is easy to prove that a leaf node is 5Model H B edges dev test test (heads) (blocks) (BLEU) (BLEU) (cased sacreBLEU) Transformer Base (Vaswani et al., 2017) 8 6 N2 25.8 27.3 BP base (Ye et al., 2019) 28.1 27.6 Reversible base (Kitaev et al., 2020) 28.0 27.4 SAC base 8 6 2 N 17.4 18.3 17.8 SAC base 8 6 5 N 25.6 27.0 26.2 SAC base 8 6 10 N 26.0 27.7 27.0 SAC base 8 6 15 N 25.6 27.4 26.8 SAC base 16 6 10 N 26.2 28.1 27.6 SAC base 16 12 10 N 26.4 28.4 27.8 Transformer big (Vaswani et al., 2017) 16 6 N2 26.4 28.4 Reversible big (Kitaev et al., 2020) 29.1 28.4 SAC Large 16 6 10 N 26.7 28.9 28.1 SAC Large 16 18 10 N 26.9 29.4 28.6 SAC Large (dependency) 16 18 10 N 26.9 29.5 28.8 Table 1: BLEU scores on the newstest2013 for development and newstest2014 for test for WMT English-German. N denotes the length of the input sequence. associated with ⌊log2 N⌋non-leaf nodes (and thus attends to ⌊log2 N⌋nodes). Therefore, we have EP(e)={(ei,ej)|i∈[1,N]; j ∈Ancestor(ei)}. 5 Experiments 5.1 Machine Translation We use the encoder-decoder model (Bahdanau et al., 2014; Vaswani et al., 2017) as the backbone for machine translation. For the encoder, SAC constructs αN edges for each layer and self-attention operations are performed between connected nodes. For the decoder, masked attention (Vaswani et al., 2017) is applied. Speciﬁcally, given a newly generated target node, it can attend to all source nodes, dummy nodes, target nodes that come beforehand, but not target nodes that come afterwards. We again use SAC to construct edges between the newly generated node and the preceding nodes, where the input node to the edge predictor is forced to be the newly generated node, and the output node is limited to preceding nodes and the dummy nodes. Following Vaswani et al. (2017); Ott et al. (2018); Kitaev et al. (2020), we used the standard WMT 2014 English-German dataset to test the proposed model. The dataset consists of about 4.5 million sentence pairs. Sentences are encoded using BPE (Sennrich et al., 2016), which has a shared source target vocabulary of about 37000 tokens. For fair comparison, we used the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 and ϵ= 10−9 for all models. Label smoothing (Szegedy et al., 2016) with ϵ= 0.1 is applied for all models. For the base setup, following Vaswani et al. (2017), the dimensionality of inputs and outputs dmodel is set to 512, and the inner-layer has dimensionality dff is set to 2,048. For big models, dmodel is set to 1,024 and dff is set to 4,096. Models are run on 8 NVIDIA V100 GPUs. Results are shown in Table 1. As we gradually increase the number of edges for each layer (from 2 to 5 to 10 to 15 per node), we can see that the performance ﬁrst increases, reaching the highest with αset to 10, and then decreases. This means that performing attention operations between all pairs is not only unnecessary, but can hurt the performance. Memory saved from sparse connections allow for more heads to perform attentions and deeper networks with more blocks, leading to better performances over vanilla transformers. We also implement a dependency-based model, in which English sources were ﬁrst parsed using Stanford Dependency parser (Chen and Manning, 2014). Relative positions between nodes in the dependency trees are encoded in distance encodings of the edge predictor. The introduction of dependency parser for attention construction introduces +0.14 BLEU score boost. We did not observe signiﬁcant performance boost from the head-adaptive strategy, and thus omit their performances. 6Method Enwiki8 Text8 Params Trans (Al-Rfou et al., 2019) 1.11 1.18 44M Trans-XL (Dai et al., 2019) 1.06 - 41M Adaptive(Sukhbaatar et al., 2019) 1.02 1.11 39M BPT (Ye et al., 2019) 1.02 1.11 38M SAC (basic) 1.02 1.07 39M SAC (head adaptive) 1.00 1.06 39M Table 2: Performances on language modeling datasets. 5.2 Language Modeling We use character-level language modeling datasets to evaluate SAC’s ability to handle long-term dependencies. We use Enwiki8 (Mahoney, 2011) and Text8 (Mahoney, 2011) for evaluation and report the values of BPC for different models. We use the Transformer decoder architecture as the backbone. We compare SAC with other variations of transformers to ﬁt long sequences into the model, including the vanilla Transformer (Al-Rfou et al., 2019), which splits the whole sequence into smaller segments, and only trains the model within each segment and ignore the rest; Transformer-XL (Dai et al., 2019) that adopts a recurrence mechanism to cache the memory of previous segments; adaptive span model (Sukhbaatar et al., 2019) that assigns different heads with different text spans in an adaptive fashion; and the BP-Transformer (Ye et al., 2019) that splits the sequence using binary trees. For SAC, αis set to 256 for each node. The relatively small memory cost allows the model to look at a maximum context of 50k characters. Input dimensionality is set to 512, and the inner-layer dimensionality 2,048. Following (Sukhbaatar et al., 2019), we use Adagrad for optimization, with a batch size of 64 and ﬁxed learning rate of 0.07 and 32k warm-up steps. Results are shown in Table2. As can be seen, SAC-basic outperforms the other Transformers by 0.04 bcp on Text8 while signiﬁcantly reducing the memory usage for large attention spans. For Enwiki8, it ties with the best BPT model, achieving 1.02 bcp score. The improvement validates the importance modeling long-term dependencies with limited available memory. We also ﬁnd that, in the language modeling tasks, the head-adaptive strategy helps, 5.3 Representation Learning in Graphs We test the performance of the proposed model on both transductive and inductive benchmark datasets. For the transductive setup, we used the three standard citation network benchmarks, Cora, Citeseer and Pubmed (Sen et al., 2008). In the transductive setup, we used the Protein-protein interaction dataset (PPI) (Zitnik and Leskovec, 2017). The training algorithm has access to all of the nodes’ feature vectors and labels, and predictions are performed on the test nodes. The detailed descriptions for Cora, Citeseer, Pubmed and PPI are found in the Appendix due to the space limit. The difference between SAC and (Veliˇckovi´c et al., 2017) is that the latter performs self-attention operations between nodes that are connected though graph edges, while SAC perform self-attention operations between nodes linked by the edge predictor. For fast convergence, we initialize SAC using the pretrained attention model (Veliˇckovi´c et al., 2017), where attention links are just edges in the original graph. Then we start exploring edge construction across all nodes. the number of attention heads is ﬁxed to 8 and the number of blocks is set to 12. We experiment different values of α, i.e, [5, 10, 50, 100] unless the memory usage reaches limitation. We train all models with Adam (Kingma and Ba, 2014) and early stopping on the validation set. The initial learning rate is treated as a hyper-parameter trained on the validation set. Following (Veliˇckovi´c et al., 2017), we run 100 epochs in total and use an early stopping strategy on the both the cross-entropy loss and accuracy for transductive tasks and micro-F1 for inductive tasks. Each experiment is repeated three times and we report the mean value. Results are shown in Table 3. We note that SAC achieves signiﬁcant performance boosts over existing methods across all four datasets, i.e., outperforms our implemented GAT +1.8, +1.1, +0.7 and +1.1 respectively on Cora, Citeseer, Pubmed and PPI. The explanation for SAC’s advantage is as follows: graph node representation learning concerns about both label propagation and relatedness between nearby nodes in the vector space, the latter of which is what GCN handles. As veriﬁed in many 7Available data Method Cora Citeseer Pubmed PPI A DeepWalk (Perozzi et al., 2014) 67.2 43.2 65.3 – X,A DGI (Veliˇckovi´c et al., 2019) 82.3 71.8 76.8 63.8 X,A GraphSAGE (Hamilton et al., 2017a) – – – 50.2 X,A,Y SemiEmb (Weston et al., 2012) 59.0 59.6 71.7 – X,A,Y Planetoid (Yang et al., 2016a) 75.7 64.7 77.2 – X,A,Y Chebyshev (Defferrard et al., 2016) 81.2 69.8 74.4 – X,A,Y GCN (Kipf and Welling, 2016) 81.5 70.3 70.0 – X,A,Y MoNet (Monti et al., 2017) 81.7 – 78.8 – X,A,Y SGC (Wu et al., 2019) 81.0 71.9 78.9 – X,A,Y AdaLNet (Liao et al., 2019) 80.4 68.7 78.1 – X,A,Y SGAT (Ye and Ji, 2019) 84.2 68.2 77.6 96.6 X,A,Y CurvGN-n (Ye et al., 2020) 82.7 72.1 79.2 – X,A,Y GAT (Veliˇckovi´c et al., 2017) 83.0 72.5 79.0 97.3 X,A,Y SAC 84.8 73.8 79.7 98.4 X,A,Y SAC (head adaptive) 84.7 74.0 80.1 98.4 Table 3: Summary of results in terms of classiﬁcation accuracies on transductive tasks (Cora, Citeseer and Pubmed) or micro-averaged F1 score on inductive tasks (PPI). In the ﬁrst column, we report the kind of data available to each method during training (X: features, A adjacency matrix, Y: labels). CIFAR100 ImageNet GFlops top1 top5 Params GFlops top1 top5 Params WideResNet 10.4 80.3 95.0 36.3M ResNet50 8.2 76.4 93.1 25.6M Bello et al. (2019) 10.9 81.6 95.2 36.2M 8.3 77.7 93.8 25.8M SAC 11.0 82.2 95.4 36.2M 8.3 78.5 94.2 25.9M SAC (head adaptive) 11.0 82.4 95.5 36.2M 8.3 78.7 94.3 25.9M Table 4: Results of image classiﬁcation on CIFAR-100 using the Wide-ResNet 28-10 Zagoruyko and Komodakis (2016) as the backbone and on ImageNet using the ResNet-50 He et al. (2016) model. recent works Liu et al. (2018); Wang and Leskovec (2020), combining both facets leads to better performances. The attention edge prediction stage in SAC fosters information exchange between nodes that are not directly linked in graph but similar in terms of label propagation. SAC actually offers the probability in bridging the aspects, leading to better performances. 5.4 Image Classiﬁcation Augmenting convolution models with self-attention (Bello et al., 2019; Parmar et al., 2019; Hu et al., 2019; Wang et al., 2019) provides the model with the ability to capture global contexts in an image and has yielded gains in several vision tasks such as image classiﬁcation and objective detection. We follow the protocols in (Bello et al., 2019), i.e. incorporating relative position embeddings for self-attention operations and augmenting each ResNet (Zagoruyko and Komodakis, 2016; He et al., 2016) block with self-attentions. To handle the prohibitive memory cost, (Bello et al., 2019) performs self-attention operations starting from the last layer, which has the smallest spatial dimension, until memory constraints are hit. This ad-hoc strategy is replaced by SAC. Following (Bello et al., 2019), we conduct experiments on CIFAR-100 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009). For CIFAR-100, we use the Wide-ResNet-28-10, the architecture of which comprises 3 stages of 4 residual blocks each using two 3×3 convolutions. We augment each convolution of all residual blocks with the number of attention heads set to 16. For ImageNet, we use ResNet-50, the block of which consists of 1×1, 3×3, 1×1 convolutions where the last pointwise convolution expands the number of ﬁlters and the ﬁrst one contracts the number of ﬁlters. We tune αin range {5,10,20}. Results are shown in Table 4. As can be seen, the proposed SAC model signiﬁcantly outperforms the attention model in (Bello et al., 2019) with the only modiﬁcation of automatic edge construction. Speciﬁcally, the top-1 score increases from 81.6 to 82.4 for CIFAR-100 and from 77.7 to 78.7 for ImageNet. The improvement validates the importance of performing necessary attention operations under memory limit. 86 Conclusion In this work, we propose Sparse Adaptive Connection — a sparse connection method to accelerate and structure the self-attention mechanism that adapts to various downstream tasks. We use an LSTM edge predictor to construct edges for self-attention operations, which gives us control of how sparse we want self-attention to be by setting the sparse coefﬁcient α. We demonstrate that SAC is competitive with state-of-the-art models on neural machine translation, language modeling, graph classiﬁcation and image classiﬁcation, while reducing memory costs. Broader Impact Accelerating fully-connected self-attention has been a research trend in recent years. Vanilla self- attention models, such as Transformers and BERT, are not able to process extremely long text, where text must be in advance segmented into pieces and then can be individually modelled. The lack of adequate context leads to poor performances in generating long, coherent and ﬂuent text. The goal of our proposed method, SAC, is to provide a way of relieving the computation burden of vanilla self-attention by automatically searching for the best attention patterns. We believe SAC has great potentials to generate high-quality long text. While there is risk of abuse, like generating fake news, the value of SAC is generally safe and weighs more than abuse to the whole society. Acknowledgement We thank all reviewers for their insightful comments. We also want to thank Zihao Ye for his helpful suggestions on evaluations, along with suggestions on learning head-speciﬁc policies. References Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi. 2018. Watch your step: Learning node embeddings via graph attention. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 9180–9190. Curran Associates, Inc. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 3159–3166. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V . Le. 2019. Attention augmented convolutional networks. In The IEEE International Conference on Computer Vision (ICCV). Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar. Association for Computational Linguistics. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins. 2019. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2174–2184, Hong Kong, China. Association for Computational Linguistics. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2978–2988, Florence, Italy. Association for Computational Linguistics. 9Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In D. D. Lee, M. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3844–3852. Curran Associates, Inc. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee. Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-transformer. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1315–1325, Minneapolis, Minnesota. Association for Computational Linguistics. Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017a. Inductive representation learning on large graphs. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 1024–1034. Curran Associates, Inc. William L. Hamilton, Rex Ying, and Jure Leskovec. 2017b. Representation learning on graphs: Methods and applications. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778. Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. 2019. Local relation networks for image recognition. In Proceedings of the IEEE International Conference on Computer Vision , pages 3464–3473. Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. 2015. Spatial trans- former networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2017–2025. Curran Associates, Inc. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Thomas N. Kipf and Max Welling. 2016. Semi-supervised classiﬁcation with graph convolutional networks. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efﬁcient transformer. In International Conference on Learning Representations. Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. John Boaz Lee, Ryan Rossi, and Xiangnan Kong. 2018. Graph classiﬁcation using structural attention. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, page 1666–1674, New York, NY , USA. Association for Computing Machinery. Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention graph pooling. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3734–3743, Long Beach, California, USA. PMLR. Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. 2019. Lanczosnet: Multi-scale deep graph convolutional networks. In International Conference on Learning Representations. Arthur Liberzon, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. Molecular signatures database (msigdb) 3.0. Bioinformatics, 27(12):1739– 1740. Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. 2018. Learning to propagate labels: Transductive propagation network for few-shot learning. 10Matt Mahoney. 2011. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text. html. F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. 2017. Geometric deep learning on graphs and manifolds using mixture model cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5425–5434, Los Alamitos, CA, USA. IEEE Computer Society. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 1–9, Brussels, Belgium. Association for Computational Linguistics. Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. 2019. Stand-alone self-attention in vision models. In Advances in Neural Information Processing Systems 32, pages 68–80. Curran Associates, Inc. Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. 2020. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14, page 701–710, New York, NY , USA. Association for Computing Machinery. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classiﬁcation in network data. AI magazine, 29(3):93–93. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany. Association for Computational Linguistics. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331–335, Florence, Italy. Association for Computational Linguistics. C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. 2016. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, Los Alamitos, CA, USA. IEEE Computer Society. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In International Conference on Learning Representations. Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. 2019. Deep graph infomax. In International Conference on Learning Representations. Hongwei Wang and Jure Leskovec. 2020. Unifying graph convolutional neural networks and label propagation. Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. 2019. Eca-net: Efﬁcient channel attention for deep convolutional neural networks. arXiv preprint arXiv:1910.03151. 11Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jason Weston, Frédéric Ratle, Hossein Mobahi, and Ronan Collobert. 2012. Deep Learning via Semi-supervised Embedding, pages 639–655. Springer Berlin Heidelberg, Berlin, Heidelberg. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6861–6871, Long Beach, California, USA. PMLR. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research, pages 2048–2057, Lille, France. PMLR. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536. Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016a. Revisiting semi-supervised learning with graph embeddings. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 40–48, New York, New York, USA. PMLR. Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. 2016b. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861. Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Ruslan Salakhutdi- nov, and Yann LeCun. 2018. Glomo: Unsupervisedly learned relational graphs as transferable representations. Yang Ye and Shihao Ji. 2019. Sparse graph attention networks. Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. 2020. Curvature graph network. In International Conference on Learning Representations. Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. 2019. Bp-transformer: Modelling long-range context via binary partitioning. Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks. In Proceedings of the British Machine Vision Conference (BMVC), pages 87.1–87.12. BMV A Press. Kai Zhang, Yaokang Zhu, Jun Wang, and Jie Zhang. 2020. Adaptive structural ﬁngerprints for graph attention networks. In International Conference on Learning Representations. Marinka Zitnik and Jure Leskovec. 2017. Predicting multicellular function through multi-layer tissue networks. Bioinformatics, 33(14):i190–i198. 12A Graph Datasets For the transductive setup, we used the three standard citation network benchmarks, Cora, Cite- seer and Pubmed (Sen et al., 2008). We followed the transductive setup adopted in (Yang et al., 2016b; Veliˇckovi´c et al., 2017; Xu et al., 2018), where nodes correspond to documents and edges to (undirected) citations. Cora contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. Citeseer contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. Pubmed contains 19717 nodes, 44338 edges, 3 classes and 500 features per node. For the inductive setup, we used the Protein-protein interaction dataset (PPI) (Zitnik and Leskovec, 2017), which aims at classifying protein roles such as cellular functions and gene ontology in various protein-protein interaction (PPI) graphs, where each graph corresponds to a different human tissue. Critically, testing graphs remain completely unobserved during training. The dataset has 56.9K nodes, 806.2 edges with 121 classes. The average number of nodes per graph is 2372. Each node has 50 features that are composed of positional gene sets, motif gene sets and immunological signatures. There are 121 labels for each node set from gene ontology, collected from the Molecular Signatures Database (Liberzon et al., 2011), and a node can have several labels simultaneously. 13barack obama is an american politician and attorney who served as the 44th president of the united states from 2009 to 2017 . a member of the democratic party , he was the first african-american president of the united states . he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004 . barack obama is an american politician and attorney who served as the 44th president of the united states from 2009 to 2017 . a member of the democratic party , he was the first african-american president of the united states . he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004 . 0.0 0.2 0.4 0.6 0.8 1.0 Figure 3: Predicted self-attention links for the text barack obama is an american politician and attorney who served as the 44th president of the president of the united states from 2009 to 2017. as a member of the democratic party, he was the ﬁrst african-american president of the united states. he previously served as a u.s. senator from illinois from 2005 to 2008 and an illinois state senator from 1997 to 2004. 14",
      "meta_data": {
        "arxiv_id": "2003.09833v3",
        "authors": [
          "Xiaoya Li",
          "Yuxian Meng",
          "Mingxin Zhou",
          "Qinghong Han",
          "Fei Wu",
          "Jiwei Li"
        ],
        "published_date": "2020-03-22T07:58:44Z",
        "pdf_url": "https://arxiv.org/pdf/2003.09833v3.pdf"
      }
    },
    {
      "title": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers",
      "abstract": "Recently, Transformer networks have redefined the state of the art in many\nNLP tasks. However, these models suffer from quadratic computational cost in\nthe input sequence length $n$ to compute pairwise attention in each layer. This\nhas prompted recent research into sparse Transformers that sparsify the\nconnections in the attention layers. While empirically promising for long\nsequences, fundamental questions remain unanswered: Can sparse Transformers\napproximate any arbitrary sequence-to-sequence function, similar to their dense\ncounterparts? How does the sparsity pattern and the sparsity level affect their\nperformance? In this paper, we address these questions and provide a unifying\nframework that captures existing sparse attention models. We propose sufficient\nconditions under which we prove that a sparse attention model can universally\napproximate any sequence-to-sequence function. Surprisingly, our results show\nthat sparse Transformers with only $O(n)$ connections per attention layer can\napproximate the same function class as the dense model with $n^2$ connections.\nLastly, we present experiments comparing different patterns/levels of sparsity\non standard NLP tasks.",
      "full_text": "O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers Chulhee Yun MIT chulheey@mit.edu Yin-Wen Chang Google Research NY yinwen@google.com Srinadh Bhojanapalli Google Research NY bsrinadh@google.com Ankit Singh Rawat Google Research NY ankitsrawat@google.com Sashank J. Reddi Google Research NY sashank@google.com Sanjiv Kumar Google Research NY sanjivk@google.com Abstract Recently, Transformer networks have redeﬁned the state of the art in many NLP tasks. However, these models suffer from quadratic computational cost in the input sequence length n to compute pairwise attention in each layer. This has prompted recent research into sparse Transformers that sparsify the connections in the attention layers. While empirically promising for long sequences, funda- mental questions remain unanswered: Can sparse Transformers approximate any arbitrary sequence-to-sequence function, similar to their dense counterparts? How does the sparsity pattern and the sparsity level affect their performance? In this paper, we address these questions and provide a unifying framework that captures existing sparse attention models. We propose sufﬁcient conditions under which we prove that a sparse attention model can universally approximate any sequence-to- sequence function. Surprisingly, our results show that sparse Transformers with only O(n) connections per attention layer can approximate the same function class as the dense model with n2 connections. Lastly, we present experiments comparing different patterns/levels of sparsity on standard NLP tasks. 1 Introduction Transformer networks [28] and their variants [31] have played a key role in the recent advancement of the state of the art in many natural language processing tasks, such as machine translation [28], language modeling [ 23, 24], and question answering [ 10, 17, 31]. The key component of these networks is the self-attention layer [ 1, 18], which updates the embeddings of the input tokens based on their context. Naturally, the self-attention layer also plays the key role in the analysis of Transformers [3, 4, 12, 20, 33]; for example, Yun et al. [33] show that Transformers can approximate any continuous sequence-to-sequence functions (i.e., universal approximation), by proving that self-attention layers can compute contextual mappings of the input embeddings. On the other hand, the self-attention layer is also the main bottleneck in scaling these models. It involves computation of pairwise inner products between input tokens, which results in quadratic computational complexity O(n2) in the length of the input sequence n. To mitigate this issue, researchers have developed methods to sparsify the pairwise interactions/connections in self-attention layers to reduce the computational complexity and/or improve model interpretability, and have shown successful empirical results on tasks with long sequence lengths [ 2, 6, 8, 9, 11, 16, 22, 25, 26, 32, 34, 35]. For example, Child et al. [6] propose sparse Transformers for sequence generation. One of the sparsity patterns considered in [6] is the STRIDED pattern, where the sparse attention layers alternate between two patterns: each token attends to only i) wlocal neighbors, and then ii) one after 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2006.04862v2  [cs.LG]  19 Dec 2020every wtokens in a strided manner. By choosing w= O(√n), they propose sparse attention layers with O(n3/2) connections and show improvements on both speed and performance over the dense Transformer. In the existing results, the rule of thumb for designing sparsity patterns (e.g.,STRIDED ) is connectivity; the intuition is that if each token can attend to the other tokens in multiple “hops,” then the resulting sparse Transformers do not lose much expressive power. However, there has been no formal justiﬁcation for this intuition. How does sparsifying the interaction in the self-attention layers affect the model’s expressive power and ability to learn? What are the sparsity levels at which the model still retains its rich expressive power, and how is it affected by the sparsity pattern? Such fundamental questions about sparse attention models still remain unanswered. 1.1 Summary of contributions In this paper, we take the ﬁrst step towards a theoretical understanding of sparse Transformers. • We propose a uniﬁed framework to analyze sparse Transformers, which generalizes the existing approaches that sparsify attention layers (§ 3.1). • We propose a set of intuitive conditions on the sparsity pattern (Assumption 1) and the probability map (Assumption 2). Then, in Theorem 1, we show that Sparse Transformers, of ﬁxed width and arbitrary depth, satisfying these conditions are universal approximators of any continuous sequence-to-sequence functions for any given ﬁxed sequence length (§ 3.2 and § 3.3). • We next show some examples of existing sparse Transformers [2, 6, 8, 9, 11, 34, 35] that satisfy these conditions, and hence have universal approximability (§ 3.4). Surprisingly, we show that there are sparse Transformers with only O(n) connections per self-attention layer (instead of n2) that have enough expressive power to approximate arbitrary continuous functions (Corollary 2). • We report experimental results on standard NLP tasks using sparse Transformers, comparing different sparsity patterns/levels (§ 5). 2 Preliminaries and related works In this section, we summarize the notation we will use throughout the paper, give a brief overview of Transformers, and then discuss existing efforts to sparsify the self-attention mechanism. 2.1 Notation For a positive integer a, we denote [a] = {1,2,...,a }. For any vector v ∈Rd, let vj denote its j-th coordinate. For any matrix A ∈Rd×n, let Aj denote its j-th column, and ASdenote the submatrix consisting of columns of A in an index set S⊆ [n]. We use ∥A∥p to denote the entry-wise ℓp norm of A. Let σS[·] be the softmax operator, which takes a matrix as input and applies softmax operation to each column of the matrix, which results in a column stochastic matrix. 2.2 Transformers and their universal approximation power A Transformer network, consisting of multiple layers of Transformer blocks, implements a sequence- to-sequence function that maps Rd×n to Rd×n. A Transformer Block (TB) consists of two layers: a self-attention layer and a token-wise feed-forward layer, and both layers have an identity skip connection. More concretely, for an input X ∈Rd×n consisting of d-dimensional embeddings of n tokens, a Transformer block consists of the following two layers: Attn(X) = X + WO   Head1(X) ... Headh(X)  ; Head i(X) = Wi VX ·σS[(Wi KX)TWi QX] (1a) TB(X) = Attn(X) + W2 ·ReLU(W1 ·Attn(X)), (1b) where WO ∈Rd×mh, Wi V,Wi K,Wi Q ∈Rm×d, W2 ∈Rd×r,and W1 ∈Rr×d. Although our analysis and experiments rely on bias vectors, we omit those in (1) for simplicity. To endow the network with information about the position of input tokens, it is common to add a positional embedding E ∈Rd×n to the input X before feeding it to the network. The positional 2embedding E can be ﬁxed [28] or trainable [10]; we consider the latter. Using a trainable E, Th,m,r is deﬁned to be a class of functions of the form X ↦→t(X + E), where tis a composition of any number of Transformer blocks with hattention heads of head size m, and hidden layers of width r. Thus, Th,m,r is a class of Transformers with a ﬁxed width while the depth can be arbitrary. Further, let Fbe the class of continuous functions f : D →Rd×n deﬁned on any compact domain D ⊂Rd×n, where continuity is deﬁned with respect to the entry-wise ℓp norm (1 ≤p <∞). Yun et al. [33, Theorem 3] show that T2,1,4 can universally approximate F. More precisely, for any f ∈F , ϵ >0 and 1 ≤p <∞, there exists a function g ∈T 2,1,4 such that dp(f,g) := ( ∫ D ∥f(X) −g(X)∥p pdX)1/p ≤ϵ. Our goal in this paper is to study, in a similar manner, the expressive power of sparse Transformers. 2.3 Sparse Transformers As seen in Eq. (1a), the self-attention layer involves computing the inner product between each pair of tokens, which we will refer to as theattention score matrix Ai := (Wi KX)TWi QX ∈Rn×n. This leads to quadratic computational complexity in n, which makes it expensive to apply Transformers to tasks with long sequence lengths. One popular approach to mitigate this problem is to sparsify the self-attention layers. We sub-classify sparse Transformers into three categories and summarize them below. For a more extensive summary, please see a recent survey [27]. The ﬁrst category reduces computation by making Ai sparse in a pre-determined manner. Each token in the sequence only attends to a ﬁxed smaller set of other tokens instead of the whole sequence [2, 6, 22]. In some papers, auxiliary tokens are added to improve connectivity between existing tokens while maintaining sparsity [11, 32]. One drawback of these approaches is that the sparsity pattern is independent of input, so it cannot adapt to the data. To remedy this issue, [26] proposes to learn local attention span from data. In a concurrent paper, Zaheer et al. [34] propose the BIGBIRD sparsity pattern which falls into this category. For BIGBIRD , the authors show its theoretical properties such as universal approximation and Turing completeness, as well as its superior empirical performance. We note that our paper focuses on universal approximation for abroader class of sparse Transformers, by proposing a unifying framework to analyze them. The second category studies making Ai sparse after the full Ai has been computed [8, 9, 35]. Here, the focus is not on the computational gain via sparsity, because the full score matrix Ai has to be computed ﬁrst; rather, the goal here is to make attention layers more interpretable, as well as to improve performance. This line of works modiﬁes σS in (1a) to other probability maps, by using top-kelements or adopting sparser variants such as sparselin-gen or α-entmax [15, 21]. Compared to the ﬁrst category, this approach has an advantage that sparsity patterns are adaptive to data. The last category attempts to get the best of both worlds. This line of works tries to learn sparsity patterns from data using extra components predicting the connection between tokens, e.g., k-means clustering [25], LSTM [16], or locality-sensitive hashing [14]. This way, one can adaptively determine the sparsity patterns before computing the score matrix. However, the drawback of this approach is that one needs extra computation to train/run these additional components, which may be expensive. 3 Universal approximation theorem for sparse Transformers In this section, we derive a unifying framework to study sparse Transformers. We then propose a set of conditions on the sparse self-attention layers, and prove that the sparse Transformers satisfying theses conditions are universal approximators of any continuous sequence-to-sequence functions. Finally, we show some examples of existing sparse Transformers that satisfy these conditions. 3.1 A unifying framework for sparse Transformers We modify the Transformer block in (1) to the following sparse Transformer block (STB): SAttnl(X) = X + WO   SHead1,l(X) ... SHeadh,l(X)  , SHeadi,l(X)k = Wi VXAl k ·ρ[(Wi KXAl k )TWi QXk] STBl(X) = SAttnl(X) + W2 ·ReLU(W1 ·SAttnl(X)), (2) 3where the sets Al k ⊆[n], for k ∈[n] and l ∈[p], deﬁne the psparsity patterns (formally deﬁned below), which are indexed by l∈[p]. Moreover, the parameter dimensions stay the same as in (1). Note that there are three main modiﬁcations from the dense Transformer. • (Cycling blocks) There are superscripts l ∈[p] added to the symbols such as SAttn. Unlike dense Transformers, some sparse Transformers cycle through pdifferent patterns. For example, the STRIDED pattern [6] described in § 1 alternates between two different patterns, which corresponds to p= 2. We add the superscript lto include such cases in our formulation. We assume that the layers in a sparse Transformer cycle through STB1,..., STBp. • (Sparsity patterns) Note that SHeadi,l(X)k denotes the k-th column of the i-th sparse attention head. Unlike dense Transformers, the inner product of the k-th query vector Wi QXk is taken only with Wi KXAl k , the key vectors of tokens in the set Al k ⊆[n]. Hence, instead of all n tokens, the k-th token computes attention scores with only tokens in Al k. For l∈[p], we refer to the collection of the index sets {Al k}k∈[n], or simply {Al k}, as a sparsity pattern. As a result, SHeadi,l(X)k is a linear combination of columns in Wi VXAl k , rather than the whole sequence. • (Probability map) After computing the attention score matrix, the dense Transformer (1) uses the softmax operator σS to get a column stochastic matrix. In the sparse Transformers, we generalize σS to ρ. The probability map ρis any map that takes a matrix as input and outputs a column stochastic matrix. As a sanity check, by choosing p = 1 , A1 k = [ n] for all k ∈[n], and ρ = σS, we recover the dense Transformer (1). Note also that the sparse Transformer formulation covers the ﬁrst and second categories of existing results discussed in § 2.3. The ﬁrst category corresponds to choosing a predetermined sparsity pattern(s) {Al k}, while setting ρ= σS. The second category corresponds to opting for a probability map ρother than softmax σS, while maintaining A1 k = [n] for all k∈[n]. In this paper, we assume for simplicity that all sparse attention heads SHead1,l,..., SHeadh,l in a single layer have identical sparsity patterns {Al k}. However, since our result only requires two sparse attention heads per layer (as we will see in Theorem 1), our result can be easily extended to the case that allows multiple sparsity patterns in a single layer. Similar to Th,m,r in § 2.2, we deﬁne the class of functions represented by sparse Transformers. We hide the dependence of this class on the sparsity patterns and probability map to simplify the notation. STh,m,r := {X ↦→t(X + E) |tis a composition of cycling sparse Transformer blocks STBl, each with hheads of head size mand hidden layer size r, and positional embedding E ∈Rd×n is trainable}. (3) 3.2 Conditions on sparsity patterns and probability map In this section, we deﬁne a set of conditions on the sparsity patterns {Al k}and the probability map ρ that ensures that the sparse Transformer universally approximate the function class F(cf. § 2.2). For k∈[n] and the index sets {Al k}l∈[p], we deﬁne a sequence of sets {St k}t≥1 in a recursive way: S1 k := A1 k, St k := ⋃ j∈A(t−1) mod p+1 k St−1 j . The set St k is the set of all tokens that the k-th token can directly/indirectly attend to, after tsparse attention layers with sparsity patterns cycling through {A1 k},{A2 k},..., {Ap k}. We now state our conditions on sparsity patterns. Assumption 1. The sparsity patterns {Al k}satisfy the following: 1. For all k∈[n] and l∈[p], we have k∈Al k. 2. There exists a permutation γ : [n] →[n] such that, for all i∈[n−1], γ(i) ∈⋃p l=1 Al γ(i+1). 3. There exists a ﬁnite s∈N such that s= min{u|Su k = [n] for all k∈[n]}. 4Assumption 1.1 is equivalent to saying that every token always attends to itself. Assumption 1.2 re- quires that there is a chain ofdirect connections that covers allntokens; note that the set⋃p l=1 Al γ(i+1) is the set of all tokens that the γ(i+ 1)-th token directly attends to. To elaborate more about the chain, consider a directed graph with nvertices corresponding to the ntokens. For any j ∈⋃p l=1 Al k, we add a directed edge j →k. Given a graph constructed this way, Assumption 1.2 requires that the graph has a Hamiltonian path γ(1) →γ(2) →···→ γ(n). Assumption 1.3 requires that after s sparse attention layers, every token can attend to all the other tokens, either directly or indirectly. As we discuss in § 3.4, the statements in Assumption 1 are natural enough to be satisﬁed by many existing sparsity patterns studied in the literature. In fact, Assumption 1.3 is necessary for universal approximation. If p= 1, n= 2, A1 1 = {1}and A1 2 = {1,2}, then the ﬁrst token never attends to the second, so this sparse Transformer cannot approximate a function whose ﬁrst output token is dependent on both input tokens. The other two assumptions are required in parts of our proof, which involve “propagating information” over all the tokens in a sequential manner. We now state the assumption on the probability map ρ[·]. For this, we deﬁne σH[·] to be the hardmax operator, which outputs the one-hot representation of the arg max entry for each column of the input matrix. Since ρis a column-wise operator that outputs a column-stochastic matrix, we state the assumption for the operation of ρon a single column. Assumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying vj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑ j̸=j∗ρ[tv]j ≤η. Assumption 2 requires that, for inputs that have some margin between the unique maximum entry and the other entries, ρ[·] can closely approximate the behavior of the hardmax operator by scaling its input by a positive factor t. This assumption is satisﬁed by softmax σS and other sparse variants such as sparselin-gen and α-entmax, as we show in § B of the supplementary material. It is straightforward to check that the dense Transformer, which corresponds to p= 1, A1 k = [n], and ρ[·] = σS[·] in our framework, satisﬁes both Assumptions 1 and 2. 3.3 Sparse Transformers are universal approximators The key justifying intuition for adopting sparse attention layers is that, if each token can attend to the other tokens in multiple hops1, then these models do not lose too much expressive power. However, turning this intuition into a rigorous analysis is not straightforward. Moreover, recent results show that limited width can render universal approximation impossible even with arbitrary depth [13, 19], highlighting the challenges in analyzing sparse (limited “width”) Transformers. We now state our main theorem, which shows that if the sparsity patterns{Al k}and the probability map ρsatisfy Assumptions 1 and 2, sparse Transformers with h= 2 attention heads of size m= 1, and hidden layer width r = 4 are universal approximators of continuous sequence-to-sequence functions on any compact domain (recall that Fdenotes the class of such continuous functions). Theorem 1. Consider any f ∈F , and the class of sparse Transformers ST2,1,4 (cf. (3)) with the underlying sparse attention layers satisfying Assumptions 1 and 2. Then, for any ϵ >0 and 1 ≤p< ∞, there exists a function g∈ST 2,1,4 such that dp(f,g) := (∫ D ∥f(X) −g(X)∥p pdX )1/p ≤ϵ. As discussed earlier, dense Transformers satisfy Assumptions 1 and 2, which means that Theorem 1 subsumes the existing result [33] for dense Transformers. We note that the required h, m, and rin Theorem 1 are independent of d, n, or the sparsity patterns. We provide a high-level proof sketch of Theorem 1 in § 4.1. There, we also discuss how many layers are sufﬁcient for ϵ-approximation of f, and show that Theorem 1 requires only ptimes more self-attention layers than Yun et al. [33]. We would like to emphasize that Theorem 1 provides the ﬁrst formal evidence that well-designed sparse attention layers do not limit Transformer’s universal approximation power. In § 3.4, we show a surprising fact that some existing sparse self-attention layers with only O(n) connections (as opposed to n2 in regular self-attention layers) retain enough expressive power to approximate F. Combined with the number of layers analyzed in § 4.1, this means that our analysis reduces the 1Note that this corresponds to our Assumption 1.3. 5connections per layer from n2 to O(n), with only ptimes more attention layers. This advantage of sparse Transformers over their dense counterpart becomes even stronger with increasing sequence length n, providing a theoretical support for the adoption of sparsity for tasks with long sequence lengths. On a ﬁnal note, Theorem 1 views the sequence length nas a ﬁxed constant. Hence, our result does not contradict a recent paper by Hahn [12] which studies the limitation of Transformers for varying n. Also, our analysis applies to the encoder part of the Transformer network [28]. 3.4 Analysis of existing sparse Transformers By Theorem 1, any sparse Transformer that satisﬁes our Assumptions 1 and 2 has universal approxi- mation ability. In this section, we give some examples of such sparse Transformers. Child et al. [6] propose two kinds of 2-step sparsity patterns (i.e., p= 2) for sequence generation tasks, namely STRIDED and FIXED patterns. We consider the extension of their auto-regressive patterns (i.e., attending only to past tokens) to the whole sequence. In the STRIDED pattern, a token ﬁrst attends to its wneighbors and then attends to one token after every wtokens in a strided manner. The sparsity pattern for the k-th token reads A1 k = [n] ∩{k−⌈w/2⌉,...,k −1,k,k + 1,...,k + ⌊w/2⌋}, A2 k = [n] ∩{...,k −2w,k −w,k,k + w,k + 2w,... }. (4) In the FIXED pattern, we divide the token into segments of length w. A token in a segment has access to other tokens in the same segment, and then the last tokens of the other segments: A1 k = [n] ∩{⌈k/w⌉·w−w+ 1,..., ⌈k/w⌉·w}, A2 k = [n] ∩({k}∪{w,2w,3w,... }) . (5) The STRIDED and FIXED patterns satisfy both Assumption 1 and 2 for all values of w. Speciﬁcally, Assumption 1.3 holds with s= 2, because any token can directly/indirectly access all the tokens in two hops. As for Assumption 1.2, the identity permutation γ(i) = isufﬁces to satisfy the assumption for both patterns. By choosing w = O(√n), sparse Transformers with the STRIDED and FIXED patterns achieve universal approximation power with O(n3/2) connections per attention layer. Guo et al. [11] consider the STAR sparsity pattern where they add an auxiliary relay token that attends to all the tokens, and the other tokens attend only to 2w neighboring tokens and the relay token. There is only one sparsity pattern, so p= 1. The S TAR sparsity pattern can be written as A1 k={n}∪ { (i−1) mod (n−1) + 1 |i∈{k−w,...,k + w} } for k∈[n−1], A1 n=[n], (6) where w≥1. For any ﬁxed w, this sparse Transformer has O(n) connections per attention layer, and it satisﬁes both assumptions. Speciﬁcally, Assumption 1.2 is satisﬁed with the identity permutation, i.e., γ(i) = (i) for i∈[n]. Since any token can access other tokens within two hops, Assumption 1.3 is satisﬁed with s = 2 . This demonstrates that O(n) connections per layer sufﬁce for sparse attention layers to have universal approximation power. One can similarly check that the sliding window sparsity patterns with/without global attention, proposed in Longformer [2], also satisfy the assumptions with O(n) connections. For the BIGBIRD sparsity pattern [34], it is also straightforward to check that a combination of its window attention and global attention satisﬁes Assumption 1 with O(n) connections. We state this interesting observation as a corollary below. Corollary 2. There exist sparse Transformers withO(n) connections per self-attention layer that are universal approximators in the sense of Theorem 1. Recall that another line of results that replaces softmax σS with sparse variants ρ[8, 9, 35] also ﬁts into our formulation, with p = 1 and A1 k = [n]. As we show in § B, these alternative ρ’s satisfy Assumption 2. Thus, by Theorem 1, these models also have the universal approximation property. 4 Proof sketch and discussion 4.1 Sketch of proof of Theorem 1 Now, we sketch the proof of Theorem 1, which consists of three steps. Throughout the proof, we assume without loss of generality that D ⊂[0,1)d×n. Step 1. In the ﬁrst step, we approximate f ∈F with a piecewise constant function. Towards this, consider a class of piecewise constant functionsF(δ) that map D to Rd×n, where δ >0 and δ−1 is an 6integer. Any function in F(δ) maps cubes of the form G+ [0,δ)d×n to matrices AG ∈Rd×n, where G ∈{0,δ,..., 1−δ}d×n. We approximate f with a function f ∈F(δ) such that dp(f,f) ≤ϵ/2, by choosing small enough δ. We defer the statement and the proof to § C of the supplementary material. Step 2. We then approximate f ∈F(δ) with a sparse Transformer network with a slightly modiﬁed architecture. In this architecture, we replace ReLU in the feed-forward layer with any piecewise linear activation φ ∈Φ, where Φ denotes the class of (possibly discontinuous) piecewise linear functions with three pieces. We also replace ρin the sparse attention layer with the hardmax σH operator. We refer to the function class represented by the modiﬁed sparse Transformer as ST h,m,r . By a careful construction, Lemma 3 shows that any f ∈F(δ) can be exactly represented by the modiﬁed Transformer. To this end, we ﬁrst carefully choose the positional embedding E. We then quantize the inputs using feed-forward layers (Lemma 6), construct a contextual mapping using self-attention layers to map the quantized inputs to unique “ids” (Lemma 7), and then construct a value mapping with feed-forward layers to map the ids to desired output values (Lemma 8). See § D and § E in the supplementary material for details. Lemma 3. For any f ∈F(δ), there exists g∈ST 2,1,1 such that f(X) = g(X) for all X ∈D. Step 3. The ﬁnal step is to approximate the function g ∈ ST 2,1,1 with a sparse Transformer g ∈ ST2,1,4. This is done by approximating φ and σH with ReLU and ρ, respectively, while carefully bounding the accumulation of errors introduced by the approximation. See § F in the supplementary material for the details. Lemma 4. For g∈ST 2,1,1 in Lemma 3, there exists g∈ST 2,1,4 such that dp(g,g) ≤ϵ/2. Combining these three steps, we establish that dp(f,g) ≤dp(f,f) + dp(f,g) + dp(g,g) ≤ϵ. How many layers are sufﬁcient? In § D, Lemmas 6–8 show that we need dn δ sparse Transformer blocks (2) for quantization, p(n−1) δd + sfor the contextual mapping, and n δdn for the value mapping. Recall that pis from (2), sis from Assumption 1, and δis from Step 1 above. In comparison, § C of [33] shows that the dense counterpart requires dn δ , n δd + 1, and n δdn Transformer blocks (1) for the three corresponding lemmas. Note two observations: 1) The value mapping dominates the depth, and its depth requirements are identical for the two cases; and 2) For contextual mappings (where the attention layers are used), we need roughly ptimes more layers for sparse models. Recall from § 3.4 that pis usually a small constant. These observations mean that sparse Transformers can achieve universal approximation using depth of the same order in d, nand δas the dense Transformers. 4.2 Key challenges in the proof While the high level outline of the proof is similar to the one for dense Transformers [33], the proof in [33] crucially relies on having all connections for computing attention in each layer, which we do not have in sparse Transformers. The sparsity in attention mechanism and the choice of general probability map ρpose nontrivial challenges in the proof. We highlight the key differences below. Establishing the Step 2 of the dense result [33] relies on constructing a contextual mapping using attention layers. A contextual mapping is a function that maps tokens in different sequences to unique values, thereby allowing Transformers to distinguish the same token appearing in different contexts. A crucial ingredient in the construction of such a mapping is a shift operation implemented with two attention heads in an attention layer. This shift operation involves each token taking the maximum and minimum over the entire sequence, which obviously cannot be done with sparse Transformers as it would require each token to attend to all the other tokens in the sequence. We circumvent this issue by carefully choosing the positional embedding E dependent on γ(cf. Assumption 1.2), and ensuring that a similar shift operation is applied in a desired order even under sparsity. As the ﬁnal phase of the contextual mapping in [33], a single attention layer shifts the entire sequence by the maximum over the sequence. Again, this cannot be directly implemented due to sparsity. Using Assumption 1.3, we instead prove that by stacking ssparse layers, one can successfully implement a similar operation that shifts the entire sequence by the maximum over the whole sequence, up to some controlled errors. This way, we overcome the difﬁculties posed by the sparsity and construct a new version of contextual mappings. The details can be found in § E.2 of the supplementary material. Moreover, the proof of Step 3 in [33] uses the simple fact that softmax can approximate hardmax arbitrarily closely. Since we do not restrict ourselves to softmax and generalize the probability map, 7Table 1: Accuracy on the synthetic copying task. Percentages in parentheses mark the sparsity levels. STRIDED FIXED STAR RANDOM Depth UNION (87%) MULTIHEAD (93%) SEQUENTIAL (93%) UNION (87%) MULTIHEAD (93%) SEQUENTIAL (93%) (87%) (90%) 1-layer 0.82% 0.82% 0.80% 7.04% 0.76% 0.80% 1.53% 33.14% 2-layer 100.00% 100.00% 81.24% 69.26% 56.45% 96.01% 29.70% 63.41% 3-layer 100.00% 100.00% 100.00% 99.98% 99.08% 98.58% 42.18% 70.29% 4-layer 100.00% 100.00% 100.00% 100.00% 99.64% 100.00% 83.57% 95.49% a more careful argument is required. Since there are many layers in the network g, it turns out that approximating it with an original sparse Transformer in ST2,1,4 requires carefully controlling the approximation errors accumulated over layers. The proof of Lemma 4 in § F of the supplementary material shows that this is indeed possible by utilizing Assumption 2. 5 Experiments We now present our experimental study comparing different design and implementation choices, including sparsity patterns and levels, on four tasks: i) a synthetic copying task, ii) language modeling, iii) translation, and iv) GLUE tasks. Our goal is to understand the effect of such choices while employing sparse Transformers to the tasks with small sequence lengths, complementing the existing results for sparse Transformers on long sequence tasks. 5.1 Experiment Settings We consider four sparsity patterns: STRIDED (4), FIXED (5), STAR (6) and RANDOM . The ﬁrst three patterns are proposed in [6] and [11]; we test them for different values of w. In case of the RANDOM pattern, given a sparsity level, we make connections uniformly at random. Following [6], STRIDED and FIXED patterns are tested for three different head conﬁgurations: i) SEQUENTIAL , where the sparse attention layers alternate between {A1 k}and {A2 k}, as described in the previous sections; ii) UNION , where all sparse attention layers use the sparsity pattern{A1 k∪A2 k}; and iii) MULTIHEAD , where half of the attention heads in every attention layer use {A1 k}and the other half use {A2 k}. Note that, given the same sequence length, UNION is less sparse than the other two conﬁgurations. Thus, to ensure fair comparisons, we compare different conﬁgurations based on their sparsity levels. We use maximum sequence length 256 in all our experiments, except 128 for GLUE tasks. For the copying task, we experiment with only one sparse Transformer block (cf. Eq (2)), with varying numbers of attention layers with 4 attention heads. For language modeling and translation, we use the Tensor2Tensor [29] framework and employ 12-block and 6-block (respectively) Transformers with 8 attention heads per block. For GLUE tasks, we experiment with the BERTBASE model. For more details of the setup, see § G of the supplementary material. 5.2 Results Copying task. We consider a synthetic copying task proposed in [ 14], where the input sequence has the format 0s0s, where s is a 127 length sequence of symbols in [0,127]. The models have to predict (copy) the second part, given the ﬁrst half of the input. This task tests the ability of sparse Transformers to communicate the information. Table 1 presents the results for this task. Except for the STAR and RANDOM patterns, we can see that the networks learn to copy the sequences with four sparse attention layers. One possible explanation for the bad performance of STAR is that, except for the relay token, it only attends to local neighbors while the task requires to copy distant tokens. Language modeling. We conduct the language modeling experiments on the One Billion Word Benchmark [5] which has almost one billion tokens and a vocabulary of more than 800K unique tokens. In Figure 1a, we plot the perplexity against the sparsity level. We observe that the STRIDED pattern and the STAR achieve the best performance across all sparsity levels. For both the STRIDED and FIXED patterns, the UNION conﬁguration shows the best performance. Translation. For the translation task, we train the model on WMT18 English-Czech (en-cs) dataset and test it on the Newstest 2015 dataset. We plot the BLEU score against the sparsity level in Figure 1b. We apply the same sparsity pattern to both the encoder and the decoder. The STRIDED 8(a) One Billion Benchmark  (b) WMT en-cs Figure 1. Comparison of sparsity patterns and different head conﬁgurations on the One Billion Benchmark (a language modeling task) and WMT en-cs (a translation task). Note that the number of connections in the attention layers goes down as we increase the sparsity level. (a) MNLI  (b) XNLI Figure 2. Comparison of sparsity patterns and different head conﬁgurations on the MNLI and XNLI (sentence-pair classiﬁcation tasks), using the BERTBASE model. and FIXED patterns with UNION conﬁguration show the best scores, which are similar to the dense attention. The U NION conﬁguration is also the least sensitive to the sparsity levels. GLUE Tasks. We experiment with the BERTBASE model and report results on two sentence-pair classiﬁcation tasks: MNLI [30] (Figure 2a) and XNLI [7] (Figure 2b). We plot the average accuracy of three runs on the dev set against the sparsity level. Additional results of the CoLA and MRPC tasks are reported in § H of the supplementary material. Discussion. In all tasks, the RANDOM pattern performs worse than the deterministic patterns, demonstrating the need for a careful design of sparsity patterns. Overall, our experiments suggest that the design of the optimal sparsity patterns is heavily dependent on speciﬁc tasks. For example, the STAR pattern shows the best performance on the language modeling task, while having trouble with copying, translation, and BERT experiments. Among the three head conﬁgurations tested for STRIDED and FIXED , the UNION performs the best in language modeling and translation but suffers in BERT tasks. In translation experiments, we see an interesting trend that the performance of MULTIHEAD conﬁguration improves as sparsity increases. We conjecture that this is due to the fact that in STRIDED and FIXED , we have |A1 k|= O(w) and |A2 k|= O(n/w) (cf. Eqs (4) and (5)), so the sparsest choice of w= O(√n) is the one with the best “balance” between|A1 k|and |A2 k|. 6 Conclusion Recently, sparse Transformers have received a lot of attention as they enable more efﬁcient/faster attention mechanisms for the tasks with very long sequence lengths. We take an initial step to provide a theoretical understanding of these models. We provide a unifying framework that captures existing sparse attention models, and prove a universal approximation theorem for sparse Transformers which holds under intuitive conditions on sparsity patterns and probability maps. We also carry out experiments comparing different sparsity patterns and levels on standard NLP tasks. We hope that this work will shed light on the understanding of sparsity in attention layers, and provide guidance for the design of sparse attention models. 9Broader Impact This work studies theoretical aspects of a class of widely used neural network models in NLP and related areas. Since we do not propose a new method nor a new dataset, we expect that the impact of this work on ethical aspects and future societal consequences will be small, if any. Other than that, this work brings new insights into the sparsity in attention models, hence may make an impact on the study of faster and more efﬁcient NLP models. Acknowledgments and Disclosure of Funding CY acknowledges partial support as a graduate Research Assistant from the NSF Grant (CAREER 1846088). CY also acknowledges Korea Foundation for Advanced Studies for their support. References [1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015. [2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document Transformer. arXiv preprint arXiv:2004.05150, 2020. [3] Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. arXiv preprint arXiv:2002.07028, 2020. [4] Gino Brunner, Yang Liu, Damián Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. On identiﬁability in Transformers. arXiv preprint arXiv:1908.04211, 2019. [5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005. [6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse Transformers. arXiv preprint arXiv:1904.10509, 2019. [7] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018. [8] Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse Transformers. arXiv preprint arXiv:1909.00015, 2019. [9] Baiyun Cui, Yingming Li, Ming Chen, and Zhongfei Zhang. Fine-tune BERT with sparse self-attention mechanism. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3539–3544, 2019. [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional Transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [11] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. Star-Transformer. arXiv preprint arXiv:1902.09113, 2019. [12] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156–171, 2020. [13] Jesse Johnson. Deep, skinny neural networks are not universal approximators. In International Conference on Learning Representations, 2019. [14] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient Transformer. arXiv preprint arXiv:2001.04451, 2020. 10[15] Anirban Laha, Saneem Ahmed Chemmengath, Priyanka Agrawal, Mitesh Khapra, Karthik Sankaranarayanan, and Harish G Ramaswamy. On controllable sparse alternatives to softmax. In Advances in Neural Information Processing Systems, pages 6422–6432, 2018. [16] Xiaoya Li, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. Sac: Accelerating and structuring self-attention via sparse adaptive connection. arXiv preprint arXiv:2003.09833, 2020. [17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [18] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention- based neural machine translation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1412–1421, Lisbon, Portugal, September 2015. Association for Computational Linguistics. [19] Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approxi- mation. arXiv preprint arXiv:2006.08859, 2020. [20] Jorge Pérez, Javier Marinkovi´c, and Pablo Barceló. On the Turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019. [21] Ben Peters, Vlad Niculae, and André FT Martins. Sparse sequence-to-sequence models. arXiv preprint arXiv:1905.05702, 2019. [22] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019. [23] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. Technical Report, OpenAI, 2018. [24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. Technical Report, OpenAI, 2019. [25] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse attention with routing Transformers. arXiv preprint arXiv:2003.05997, 2020. [26] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in Transformers. arXiv preprint arXiv:1905.07799, 2019. [27] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv preprint arXiv:2009.06732, 2020. [28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, pages 5998–6008, 2017. [29] Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, et al. Tensor2tensor for neural machine translation. arXiv preprint arXiv:1803.07416, 2018. [30] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 1112–1122. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1101. [31] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. [32] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-Transformer: Modelling long-range context via binary partitioning. arXiv preprint arXiv:1911.04070, 2019. 11[33] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are Transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2020. [34] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2020. [35] Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, and Xu Sun. Ex- plicit sparse Transformer: Concentrated attention through explicit selection. arXiv preprint arXiv:1912.11637, 2019. [36] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015. 12A Outline and notation The supplementary material is organized as follows. First, § B proves that the softmax operator as well as its sparse versions indeed satisfy Assumption 2. Next, § C provides formal statements of Step 1 in the proof sketch (§ 4.1). The outline of proof of Lemma 3 (Step 2 in the proof sketch) is presented in § D, followed by a separate section (§ E) proving the three key sublemmas in the proof. The proof of Step 3, Lemma 4, is given in § F. Lastly, § G and § H present the detailed setup of our experiments and additional experiment results, respectively. We next review some of the notation and also introduce additional notation used throughout the supplementary material. For a positive integer a, let [a] := {1,...,a }. For a,b,c ∈R where b−a> 0 is an integer multiple of c> 0, we write [a: c: b] := {a,a + c,a + 2c,...,b −c,b}. For any matrix A ∈Rd×n, let Aj denote its j-th column, and ASdenote the submatrix consisting of columns of A in the index set S⊆ [n]. We also use Ai,j to denote its (i,j)-th entry. Let 1 {·}be the 0-1 indicator for an event. Let 1n ∈Rn be a vector whose components are all 1. B Sparse probability maps satisfy Assumption 2 In this section, we show that the softmax operatorσS as well as the probability maps ρused to replace softmax in the existing approaches, namely softmax with only top-kinputs [35], sparselin-gen [9], and α-entmax [8], all satisfy Assumption 2. We restate the assumption for reader’s convenience: Assumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying vj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑ j̸=j∗ρ[tv]j ≤η. As in the assumption, we only consider the operation of these probability maps on a single vector, as they are applied column-wise. For each of the probability maps, we will show that for any ζ >0 and η∈(0,1], we can choose t> 0 that satisﬁes the conditions of Assumption 2. B.1 Softmax & softmax with top- kinputs Given an input vector v ∈Rn, the j-th coordinate of the output of softmax σS[v] is deﬁned as σS[v]j := exp(vj)∑n i=1 exp(vi). We assume without loss of generality that the entry ofv is in decreasing order, where the ﬁrst two entries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence of t> 0 such that σS[tv]1 = exp(tv1)∑n i=1 exp(tvi) ≥1 −η. Then, ∑n j=2 σS[tv]j ≤ηfollows. Now, since vi ≤v1 −ζfor i∈[2 : n], note that σS[tv]1 = exp(tv1)∑n i=1 exp(tvi) ≥ exp(tv1) exp(tv1) + (n−1) exp(tv1 −tζ) = 1 1 + (n−1) exp(−tζ). Since 1 1+(n−1) exp(−tζ) is an increasing function in t >0, one can increase tsufﬁciently large to make it greater than 1 −η. The same argument holds for the softmax with top-kinputs, used in [35]. By the assumption on v, entries v1,...,v k are the top kcomponents. Thus, ρ[tv]1 ≥ 1 1 + (k−1) exp(−tζ) ≥1 −η can be satisﬁed by choosing large enough t> 0. B.2 Sparselin-gen We now consider the case where ρis sparselin-gen [15], which was used to sparsify the attention score matrices in [9]. Given a regularization parameter λ∈[0,1), the sparselin-gen used in [9] is deﬁned as ρ[v] := arg min p∈∆n−1 ∥p −v∥2 −λ∥p∥2 , 13where ∆n−1 := {p ∈Rn |p ≥0,∑n i=1 pi = 1}is the probability simplex. Then, the solution for optimization problem above can be written as ρ[v]j = max { 0,vj −τ(v) 1 −λ } , for j ∈[n], where τ : Rn →R is a threshold function that chooses the threshold τ(v) such that ∑n j=1 ρ[v]j = 1. Now, assume without loss of generality that the entry of v is in decreasing order, where the ﬁrst two entries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence of t> 0 such that ρ[tv]1 ≥1 −η. This is done by choosing t= 1−η ζ . To see this, notice that if vj’s are in decreasing order, then ρ[v]j are also in decreasing order. Now consider ρ[tv]1 = max { 0,tv1 −τ(tv) 1 −λ } , ρ[tv]2 = max { 0,tv2 −τ(tv) 1 −λ } . If ρ[tv]2 = 0, then ρ[tv]j = 0 for all j = 3,...,n , and ρ[tv]1 = 1 ≥1 −η. If ρ[tv]2 >0, then ρ[tv]1 −ρ[tv]2 = tv1 −τ(tv) 1 −λ −tv2 −τ(tv) 1 −λ = t(v1 −v2) 1 −λ ≥t(v1 −v2) ≥tζ = 1 −η. B.3 α-entmax Next, we consider the case where ρis α-entmax [21], which was used to sparsify the attention score matrices in [8]. Given a parameter α≥1, the α-entmax is deﬁned as ρ[v] := arg max p∈∆n−1 pTv + Hα(v), where ∆n−1 is the probability simplex and Hα is the Tsallis continuous family of entropies Hα(v) := { 1 α(α−1) ∑ jvj −vα j α> 1, −∑ jvjlog vj α= 1. As shown in [8], the solution of α-entmax is equal to softmax if α= 1, and otherwise (α> 1) it is given in the form ρ[v]j = [ max{0,(α−1)vj −τ(v)} ] 1 α−1 , for j ∈[n], where τ : Rn →R is a threshold function that chooses the threshold τ(v) such that ∑n j=1 ρ[v]j = 1. Since softmax (α= 1) is already covered above, we focus on α> 1. Again, assume without loss of generality that the entry of v is in decreasing order, where the ﬁrst two entries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence of t> 0 such that ρ[tv]1 ≥1 −η. This is done by choosing t= 1/ζ(α−1). Note that (α−1)t(v1 −v2) ≥1 due to our choice of t. Then, we will show that with such a t, ρ[tv]1 = 1 must hold. For the sake of contradiction, suppose not: ρ[tv]1 <1. Then, by monotonicity of ρ[tv]j, we have ρ[tv]2 >0. This means ρ[tv]2 = [ (α−1)tv2 −τ(tv) ] 1 α−1 >0, in particular, we have (α−1)tv2 −τ(tv) >0. However, recall that (α−1)t(v1 −v2) ≥1, which implies (α−1)tv1 −τ(tv) >1. This results in ρ[tv]1 = [ (α−1)tv1 −τ(tv) ] 1 α−1 >1, thus contradicting ρ[tv]1 <1. Therefore, ρ[tv]1 = 1 must hold. C Details of the Step 1 in the proof sketch (§ 4.1) We start by formally deﬁning the function class F(δ). F(δ) := { Z ↦→ ∑ G∈Gδ AG1 { Z ∈G + [0,δ)d×n} |Z ∈D,AG ∈Rd×n } , where Gδ := {0,δ,..., 1 −δ}d×n. We now state and prove the lemma. 14Lemma 5. For any f ∈F and ϵ >0, there exists a small enough δ >0 such that there exists f ∈F(δ) such that dp(f,f) ≤ϵ/2. Proof Since f : D → Rd×n is a continuous function on a compact domain, it is uniformly continuous. Also, continuity is deﬁned with respect to entry-wise ℓp norm which is equivalent to entry-wise ℓ∞norm, uniform continuity leads to ∀ϵ> 0,∃δ >0 such that ∀X,Y ,∥X −Y ∥∞<δ =⇒ ∥f(X) −f(Y )∥p <ϵ/2. Then, suppose we create a set of cube grid pointsGδ := {0,δ,..., 1−δ}d×n, and deﬁne a piece-wise constant approximation f(X) = ∑ G∈Gδ f(G)1 { X ∈G + [0,δ)d×n} . Note that for any X ∈G + [0,δ)d×n we have ∥X −G∥∞<δ, so we have f(X) −f(X)  p = ∥f(X) −f(G)∥p <ϵ/2. This implies that dp(f,f) = (∫ D f(X) −f(X) p p )1/p ≤ϵ/2, ﬁnishing the proof of the lemma. D Proof of Lemma 3 (Step 2 in § 4.1) In this section, we describe in further details how modiﬁed sparse Transformers (the class ST 2,1,1 ) are able to exactly express arbitrary piecewise constant functions in F(δ). We show that we can compute a contextual mapping of the entire input sequences without relying on dense self-attention layers. The token-wise feed-forward layers then transform these contextual mappings to the desired output sequence. To give a high level summary of the proof, we want to show that given a piece-wise constant function f ∈F(δ), there exists a modiﬁed Transformer network g∈ST 2,1,1 that exactly represents f. Recall ﬁrst that the function class ST 2,1,1 has an additive positional embedding matrix E ∈Rd×n that is added to input before the input is fed to the network. We start by choosing the positional embedding E and construct a Transformer network that implements quantization of the input, contextual mapping of the quantized input, and value mapping of the context ids. 1. Choose the positional embedding E according to γin Assumption 1.2. After addition, each column of the input Xk + Ek are in disjoint intervals. 2. Given the input X + E, a series of modiﬁed feed-forward layers quantizes it so that each entry of the quantized input has a value in {0,δ,...,n −δ}(Lemma 6). 3. Next, a series of modiﬁed sparse self-attention layers takes the quantized input H and implement a contextual mapping qsuch that, for different quantized input sequences H and H′, all the elements in q(H) and q(H′) are distinct (Lemma 7). 4. Finally, a series of modiﬁed feed-forward layers maps each element in the context id q(H) to the desired output value of f ∈Fat the input X (Lemma 8). We defer the proofs of Lemmas 6, 7, and 8 to a separate section: see § E. Before discussing the details of each step, we note that although a Transformer network stacks self-attention and feed-forward layers in an alternate manner, we can use a series of arbitrary number of the same layers, thanks to skip connections. The outline of the proof is similar to [ 33], but key component in their proof called selective shift operation relies on the fact that each token can attend to the entire sequence; this is not true in sparse Transformers, which poses a nontrivial challenge. We overcome this issue by a more careful construction of the positional embedding E and sparse self-attention layers. 15D.1 Choosing the positional embedding Recall from Assumption 1.2 that there exists a permutation γ : [n] →[n] such that for all i∈[n−1], γ(i) is one of the tokens that the γ(i+ 1)-th token directly attends to. Using this permutation γ, we choose the columns of positional embedding E in the following way: Eγ(1) = (n−1)1n, and Eγ(i) = (i−2)1n, for i∈[2 : n] As a result, theγ(1)-th column ofX+E will be in the range[n−1,n)d, and similarlyXγ(i)+Eγ(i) ∈ [i−2,i −1)d for i∈[2 : n]. This means that the entries corresponding to different tokens lie be in disjoint intervals of the form [j,j + 1), where j ∈[0 : n−1]. D.2 Quantization by feed-forward layers Note from the previous step that each entry of X + E must be in [0,n). Next, we quantize this interval [0,n) of input using to a set of δ-grid points {0,δ,...,n −δ}. This allows us to deal with ﬁnite set of values, which proves useful in the later stages of the proof. The next lemma shows that the quantization can be carried out using a seried of the modiﬁed feed-forward layers. Lemma 6. Consider a entry-wise quantization map gent q : R →R: gent q (t) = {kδ if kδ ≤t< (k+ 1)δ, k∈[0 : n/δ−1], t otherwise. There exists a function gq : Rd×n ↦→Rd×n composed of dn δ token-wise feed-forward layers with r= 1 and an activation φ∈Φ, which implements the entry-wise quantization gent q to each entry of its input. D.3 Contextual mapping by sparse self-attention layers After the input X + E is quantized, the output of gq must be in the following set Hδ ⊂Rd×n: Hδ := {G + E ∈Rd×n |G ∈Gδ}, where Gδ := {0,δ,..., 1 −δ}d×n was deﬁned to be the δ-cubic grid points of [0,1)d×n. Using this ﬁnite set of sequences, we construct a contextual mapping that maps each sequence in Hδ to unique numbers. Recall that the sparse attention layer has psparsity patterns that rotate in cycles, and Assumption 1.3 assumes that one token directly/indirectly access all the other tokens after ssuch sparse attention layers. We now state the lemma. Lemma 7. Assume that n≥2, and δ−1 is an integer satisfying δ−1 ≥2. Suppose that the sparse self-attention layers (h = 2,m = 1) satisfy Assumption 1 and employ the hardmax σH operator, and that the positional embedding E was chosen as described in § D.1. Then, there exist a function gc : Rd×n →Rd×n composed of p(n−1) δd + ssparse self-attention layers, and a vector u ∈Rd, such that q(H) := uTgc(H) satisﬁes the following properties: 1. For any H ∈Hδ, the entries of q(H) are all distinct. 2. For any H,H′∈Hδ such that H ̸= H′, all entries of q(H), q(H′) are distinct. This contextual mapping maps each unique sequence/context into different context ids, enabling the network to distinguish the same token appearing in different sequences. D.4 Value mapping by feed-forward layers After the contextual mapping, we use the token-wise feed-forward layers to map each different context ids to the desired output value of the target function f. More speciﬁcally, recall the function gc from Lemma 7. For any H ∈Hδ, we need to map the output gc(H) of Lemma 7 to the desired function value f(H −E) (recall that H is the quantized input after adding E to X, so we need to subtract E). This is done by implementing a token-wise value mapping using the feed-forward layers. 16Lemma 8. There exists a function gv : Rd×n → Rd×n composed of n(1 δ)dn token-wise feed- forward layers (r = 1) with an activation φ′∈Φ such that gv is deﬁned by a token-wise function gtkn v : Rd →Rd on each column, gv(Z) = [ gtkn v (Z1) ··· gtkn v (Zn) ] , where for all H ∈Hδ and k∈{1,...,n }, gtkn v (gc(H)k) = f(H −E)k. D.5 Finishing the proof Given Lemmas 6, 7, and 8, one can easily check that for any G ∈Gδ := {0,δ,..., 1 −δ}d×n and any input value X ∈G + [0,δ)d×n, we have gv ◦gc ◦gq(X + E) = gv ◦gc(G + E) = [ gtkn v (gc(G + E)1) gtkn v (gc(G + E)2) ··· gtkn v (gc(G + E)n) ] = [ f(G)1 f(G)2 ··· f(G)n ] = f(G) = f(X). Therefore, we have constructed a modiﬁed sparse Transformer networkg(X) := gv ◦gc ◦gq(X +E) that satisﬁes g(X) = f(X) for all X ∈D, hence proving Lemma 3. E Proof of Lemmas 6, 7, and 8 E.1 Proof of Lemma 6 The proof goes as follows. Using n δ token-wise feed-forward layers, we implement the quantization function gent q that quantizes the ﬁrst row of the input. Then we stack another n δ layers to quantize the second row, and so on. For the ﬁrst row, we add n/δlayers of the following form, for k∈[0 : n/δ−1]. Z ↦→Z + e(1)φ((e(1))TZ −kδ1T n), φ(t) = {0 t< 0 or t≥δ, −t 0 ≤t<δ, where e(1) ∈Rd is the ﬁrst canonical basis vector e(1) = (1,0,..., 0). Each layer quantizes Z1,: in [kδ,kδ + δ) to kδ, without modifying other intervals or other rows of Z. Note that the activation φis a piecewise linear function with three pieces; hence, φ∈Φ. Therefore, the layers satisfy the deﬁnition of modiﬁed feed-forward layers. We can now repeat the same construction for the d−1 remaining rows. E.2 Proof of Lemma 7 In order to construct a network gc that implements the contextual mapping, we ﬁrst introduce two operations referred to as the sparse selective shift operation and all-max-shift operation, implemented by at most two (modiﬁed) sparse attention heads of head size 1. Then, we proceed to stack layers implementing the selective shift operations and all-max-shift operations, and prove that these layers map input H ∈Hδ to unique context ids. E.2.1 Preliminaries Sparse selective shift operation. Given any vector u ∈Rd, ﬁrst consider the following function implementable with a sparse attention head with head size 1 and sparsity pattern {Al k}k∈[n]. For k∈[n], the function ψl : Rd×n →R1×n computes each of its output column in the following way: ψl(Z; bQ)k := uTZAl k σH[(uTZAl k )T(uTZk −bQ)] = { maxj∈Al k uTZj if uTZk >bQ, minj∈Al k uTZj if uTZk <bQ. One can consider a sparse self-attention layer that consists of two such heads, with bQ <b′ Q: Ψl(Z; c,bQ,b′ Q) := Z + [ ce(1) −ce(1)][ψl(Z; bQ) ψl(Z; b′ Q) ] . 17The (1,k)-th entry of Ψl(Z; c,bQ,b′ Q) reads Ψl(Z; c,bQ,b′ Q)1,k = Z1,k + c(ψl(Z; bQ)k −ψl(Z; b′ Q)k) = { Z1,k + c(maxj∈Al k uTZj −minj∈Al k uTZj) if bQ <uTZk <b′ Q, Z1,k if uTZk /∈[bQ,b′ Q]. This means that for input columns Zk satisfying uTZk ∈(bQ,b′ Q) only, Ψl shifts up the ﬁrst entry of Zk by the difference of maximum and minimum values of uTZj over the sparsity pattern j ∈Al k, while leaving other columns intact. By choosing bQ and b′ Q properly, we can selectively modify certain columns without touching other columns; we refer to this operation Ψl as the sparse selective shift operation, and we will see later that this is indeed the key ingredient of our proof. In fact, this operation is a sparse version of the selective shift operation used in [ 33]. Since Al k is usually only a small subset of [n], one cannot calculate the maximum and minimum of uTZj over the whole sequence, as done in [33]. Instead, we use Assumption 1.2 and a more careful choice of E to get around the restriction posed by sparsity. All-max-shift operation. Suppose the input Z ∈Rd×n satisﬁes uTZ >0 entry-wise, for a vector u ∈Rd. Then, the all-max-shift operation Ωl : Rd×n →Rd×n is a sparse self-attention layer that consists of one attention head: Ωl(Z; c) = Z + ce(1)ψl(Z; 0). The (1,k)-th entry of Ωl(Z; c) reads Ωl(Z; c)1,k = Z1,k + cψl(Z; 0)k = Z1,k + cmax j∈Al k uTZj. So, for each column k, the all-max-shift operation shifts up the ﬁrst entry of Zk by the maximum value of uTZj over the sparsity pattern j ∈Al k. Unlike the selective shift operation, the all-max-shift operation is applied to all the columns. Column ids. Recall that the any input to this step is in Hδ := {G + E ∈Rd×n |G ∈Gδ := [0 : δ: 1 −δ]d×n}. Because of the way E is chosen according to the permutation γin Assumption 1.2, for any H ∈Hδ we have Hγ(1) ∈[n−1 : δ: n−δ]d, Hγ(i) ∈[i−2 : δ: i−1 −δ]d for all i∈[2 : n]. Now consider u := (1,δ−1,δ−2,...,δ −d+1). It is easy to check that for any H ∈Hδ, the map Hk ↦→uTHk is one-to-one, and uTHγ(1) ∈ [ (n−1) d−1∑ i=0 δ−i : δ: (n−1) d−1∑ i=0 δ−i + δ−d+1 −δ ] , uTHγ(i) ∈ [ (i−2) d−1∑ i=0 δ−i : δ: (i−2) d−1∑ i=0 δ−i + δ−d+1 −δ ] , for i∈[2 : n]. (7) Hence, for each column Hk, the inner product uTHk is in an interval disjoint from the other columns. Thus, uTHk can be thought as a “column id” that identiﬁes the column’s original input valueGk as well as its position k. Note furthermore that for any H ∈Hδ, uTHγ(2) <uTHγ(3) <··· <uTHγ(n) <uTHγ(1). (8) E.2.2 Construction of layers Given these preliminaries, we now describe our construction of gc. Recall from Assumption 1.2 that the permutation γsatisﬁes γ(i−1) ∈⋃p l=1 Al γ(i) for i∈[2 : n]. From this, for i∈[2 : n] we let 18li ∈[p] be any index such that γ(i−1) ∈Ali γ(i). For simplicity of notation, let zk := uTHk for k∈[n] and ∆ = ∑d−1 i=0 δ−i. Next, starting from i= 2, we want to sequentially stack δ−d sparse selective shift operations Ψli(·; δ−d,b −δ/2,b + δ/2), in increasing order of b∈ [ (i−2)∆ : δ: (i−2)∆ + δ−d+1 −δ ] . That is, we want to add sparse attention layers with sparsity patterns Ali γ(i) that apply the selective shift operation to each possible value of zγ(i). Recall that the sparsity patterns have to cycle from A1 k to Ap k, so we have to place other remaining p−1 sparsity patterns (whose indices are not li) in between the Ψli layers. This can be done by setting all the other sparse attention layers to be the identity. This way, we stack a total of pδ−d sparse attention layers for i= 2, another pδ−d for i= 3, and so on, up to i= n. After these layers, we further stack sall-max-shift operations. For i= 1,...,s , we add all-max-shift operations of the form Ω(i−1) mod p+1(·; 2snδ−nd−1). Here, the superscript (i−1) mod p+ 1 is there to make sure that we cycle through the sparsity patterns from 1 to p, until we stack slayers in total. This ﬁnishes the construction of our function gc composed of p(n−1) δd + ssparse self-attention layers. E.2.3 Selective shift operations We now explain how these stacked self-attention layers implement a contextual mapping. This subsection will consider the selective shift operations part; all-max-shift operations are described in the next subsection. Suppose that after the input H ∈Hδ is processed through the ﬁrst p(n−1) δd layers, we get ˜H ∈Rd×n at the output. We will show at the end of this subsection that the map H ↦→uT˜Hγ(n) is a one-to-one map for column γ(n), so the selective shift operations compute a “unique id” for each possible input sequenceH ∈Hδ. First selective shift. First consider the ﬁrst pδ−d layers. Omitting layers that are identity, they are essentially selective shift operations Ψl2 (·; δ−d,b −δ/2,b + δ/2) for b ∈[0 : δ : δ−d+1 −δ]. Since [0 : δ : δ−d+1 −δ] is the set of possible values of zγ(2), these layers perform selective shift operation on the γ(2)-th column without changing the other columns. Each possible value of Hγ(2) undergoes one and only shift operation (by the corresponding layer with b= uTHγ(2)), by which the (1,γ(2))-th entry of the input is updated. Recall by Assumption 1.2 thatγ(1) ∈Al2 γ(2), and that zγ(1) and zγ(2) are the maximum and minimum over the whole sequence z1,...,z n (see (8)). By Assumption 1.1 we also have γ(2) ∈Al2 γ(2). Since both γ(1) and γ(2) are in Al2 γ(2), the maximum and minimum value ofzj := uTHj’s overj ∈Al2 γ(2) are zγ(1) and zγ(2), respectively. Therefore, the (1,γ(2))-th entry of the input matrix is shifted up as follows: ˜H1,γ(2) := H1,γ(2) + δ−d(zγ(1) −zγ(2)). Let ˜Hγ(2) be the γ(2)-th column after the shift operation has shifted H1,γ(2) to ˜H1,γ(2). Then, deﬁne ˜zγ(2) := uT˜Hγ(2) = zγ(2) + δ−d(zγ(1) −zγ(2)). Note that ˜zγ(2) >zγ(1) because zγ(2) + δ−d(zγ(1) −zγ(2)) >zγ(1) ⇔(δ−d −1)(zγ(1) −zγ(2)) >0, which is true. Therefore, ˜zγ(2) becomes the new maximum among the current values zγ(1),˜zγ(2),zγ(3),...,z γ(n), and the new minimum element is zγ(3). Second selective shift. We now consider the nextpδ−d layers, which are essentially Ψl3 (·; δ−d,b− δ/2,b + δ/2) for b∈[∆ : δ: ∆ +δ−d+1 −δ]. They apply the shift operation to the γ(3)-th column. Since we have γ(2),γ(3) ∈Al3 γ(3), the shift operation similarly yields ˜zγ(3) := zγ(3) + δ−d(˜zγ(2) −zγ(3)) = zγ(3) + δ−d(zγ(2) −zγ(3)) + δ−2d(zγ(1) −zγ(2)). 19We can also show ˜zγ(3) >˜zγ(2), because zγ(3) + δ−d(˜zγ(2) −zγ(3)) >˜zγ(2) ⇔(δ−d −1)(˜zγ(2) −zγ(3)) >0. So after this operation ˜zγ(3) and zγ(4) are the new maximum and minimum over the updated sequence zγ(1),˜zγ(2),˜zγ(3),zγ(4),...,z γ(n). Repeating the process. The same process continues. The next pδ−d layers shifts the γ(4)-th columns and results in ˜zγ(4) which is greater than ˜zγ(3). After the ﬁrst p(n−1)δ−d layers, all columns except γ(1)-th column have been shifted, resulting in zγ(1),˜zγ(2),..., ˜zγ(n) satisfying (n−1)∆ ≤zγ(1) <˜zγ(2) <··· <˜zγ(n). (9) Let us denote the output of the p(n−1)δ−d-th layer as ˜H. Selective shifts implement a one-to-one map. Next, we prove that the map from H ∈Hδ to ˜zγ(n) := uT˜Hγ(n) = zγ(n) + n−1∑ i=1 δ−id(zγ(n−i) −zγ(n+1−i)) is one-to-one. Recall that for each column Hk, the map Hk ↦→ uTHk =: zk is one-to-one. Also, permutation of columns is one-to-one, which implies that it sufﬁces to show that the map[zγ(1) ... z γ(n) ] ↦→˜zγ(n) is one-to-one. Suppose we have two sequences [zγ(1) ... z γ(n) ] and [z′ γ(1) ... z ′ γ(n) ] that map to the same value of ˜zγ(n) = ˜z′ γ(n). Then, 0 = ˜zγ(n) −˜z′ γ(n) = zγ(n) −z′ γ(n) + n−1∑ i=1 δ−id(zγ(n−i) −zγ(n+1−i) −z′ γ(n−i) + z′ γ(n+1−i)). Suppose zγ(n) ̸= z′ γ(n). Since they both lie inside [(n−2)∆ : δ: (n−2)∆ + δ−d+1 −δ], we have −δ−d+1 + δ≤zγ(n) −z′ γ(n) ≤δ−d+1 −δ. Note that all the terms other than zγ(n) −z′ γ(n) are of “coarser resolution.” For example, the ﬁrst term δ−d(zγ(n−1) −zγ(n) −z′ γ(n−1) + z′ γ(n)) in the summation can only take values 0,δ−d+1,−δ−d+1,2δ−d+1,−2δ−d+1,... , so it can never cancel the difference zγ(n) −z′ γ(n) and make the sum ˜zγ(n) −˜z′ γ(n) zero. This implies that zγ(n) = z′ γ(n) must hold. Next, suppose zγ(n−1) ̸= z′ γ(n−1). Since we have zγ(n) = z′ γ(n), −δ−2d+1 <δ−d(zγ(n−1) −zγ(n) −z′ γ(n−1) + z′ γ(n)) = δ−d(zγ(n−1) −z′ γ(n−1)) <δ−2d+1. But similarly, any other terms in the summation have coarser resolution than δ−2d+1, so they cannot cancel the difference δ−d(zγ(n−1) −z′ γ(n−1)). Thus zγ(n−1) = z′ γ(n−1) must hold. Repeating the same argument up to γ(1) proves that the two sequences must be equal: [zγ(1) ... z γ(n) ] =[z′ γ(1) ... z ′ γ(n) ] . This proves that the map H ↦→˜zγ(n) is one-to-one and ˜zγ(n) can be seen as the unique id for the input sequence H ∈Hδ. E.2.4 All-max-shift operations Next, we explain the operation of the sall-max-shift layers. Recall from Assumption 1.3 that any token can attend to all the other tokens after ssteps, either directly or indirectly. Also recall from the last subsection that the input to the ﬁrst all-max-shift layer is ˜H, and the maximum entry of uT˜H is ˜zγ(n), the unique id for input H. From the statement of Lemma 7, the output after the s all-max-shift operations for input H is denoted as gc(H). In this subsection, we show that through s all-max-shift operations, the maximum ˜zγ(n) will propagate to all tokens and be a “dominant” term, which determines the interval that uTgc(H) lies in. As a result, we can show Properties 7.1 and 7.2 of gc at the end. 20Some preliminaries. Note that the unique id ˜zγ(n) has the following upper bound: ˜zγ(n) := zγ(n) + n−2∑ i=1 δ−id(zγ(n−i) −zγ(n+1−i)) + δ−(n−1)d(zγ(1) −zγ(2)) ≤zγ(n) + δ−d n−2∑ i=1 (zγ(n−i) −zγ(n+1−i)) + δ−(n−1)d(zγ(1) −zγ(2)) = zγ(n) + δ−d(zγ(2) −zγ(n)) + δ−(n−1)d(zγ(1) −zγ(2)) = δ−(n−1)dzγ(1) −(δ−(n−1)d −δ−d)zγ(2) −(δ−d −1)zγ(n) ≤δ−(n−1)dzγ(1) ≤δ−(n−1)d((n−1)∆ + δ−d+1 −δ) ≤δ−(n−1)d(n−1 + δ)(δ−d −1) ≤δ−nd −δ (10) where we used ∆ := ∑d−1 i=0 δ−i = δ−d−1 δ−1−1 ≤δ−d −1. A similar bound ˜zγ(i) ≤nδ−id −δ (11) also holds from a similar derivation. Next, recall from Assumption 1.3 the deﬁnitions S1 k := A1 k, St k := ⋃ j∈A(t−1) mod p+1 k St−1 j , and that there exists s≥1 such that, for all k∈[n], Ss k = [n]. Finally, the following inequality will be useful throughout: for any integer s≥1, (2s+ 1 2s ) ≤ (2s+ 1 2s )2 ≤···≤ (2s+ 1 2s )s ≤2. (12) Let us now describe the operation that the all-max-shift layers Ω(i−1) mod p+1(·; 2snδ−nd−1), i = 1,...,s , carry out. First all-max-shift. The input to the ﬁrst all-max-shift layer is ˜H. Let the output of the layer be M1. Recall that uT˜H consists of values zγ(1),˜zγ(2),..., ˜zγ(n), which are all strictly greater than 0 and strictly less than nδ−nd (by (10)). So, for each column k∈[n], the layer update reads M1 1,k := ˜H1,k + 2snδ−nd−1 max j∈A1 k uT˜Hj = ˜H1,k + 2snδ−nd−1uT˜Hj1 k , where j1 k := arg maxj∈A1 k uT˜Hj. After the update, uTM1 k is “dominated” by2snδ−nd−1uT˜Hj1 k , meaning that for any k,k′∈[n], uT˜Hj1 k <uT˜Hj1 k′ =⇒ uTMk <uTMk′. This is because the minimum gap between different values of uT˜Hj1 k is at least δ, and we have uT˜Hk <nδ−nd <2snδ−nd−1 ·δ, so if uT˜Hj1 k <uT˜Hj1 k′, that solely determines the order uTMk <uTMk′ because uT˜Hk cannot reverse it. Also, by the deﬁnition of j1 k, for any index set B∈ [n] we have max i∈B uT˜Hj1 i = max j∈⋃ i∈BA1 i uT˜Hj. (13) If s≥2, we move on to the second layer. Second all-max-shift. At the second all-max-shift, we have sparsity patterns A1 mod p+1 k . Let us the output of this layer as M2. For each column k∈[n], the layer update reads M2 1,k := M1 1,k + 2snδ−nd−1 max j∈A1 mod p+1 k uTM1 j = M1 1,k + 2snδ−nd−1uTM1 j2 k , 21where j2 k := arg maxj∈A1 mod p+1 k uTM1 j. If we look at the update more closely, we can apply (13) and get uTM2 k = uT˜Hk + 2snδ−nd−1uT˜Hj1 k + 2snδ−nd−1(uT˜Hj2 k + 2snδ−nd−1 max i∈A1 mod p+1 k uT˜Hj1 i ) = uT˜Hk + 2snδ−nd−1(uT˜Hj1 k + uT˜Hj2 k ) + (2snδ−nd−1)2 max j∈S2 k uT˜Hj. Again, the last term dominates the rest of the terms in uTM2 k, because the minimum gap between different values of maxj∈S2 k uT˜Hj is at least δ, and uTM2 k −(2snδ−nd−1)2 max j∈S2 k uT˜Hj = uT˜Hk + 2snδ−nd−1(uT˜Hj1 k + uT˜Hj2 k ) <(1 + 4snδ−nd−1)nδ−nd ≤(1 + 4s)n2δ−2nd−1 ≤(2snδ−nd−1)2 ·δ= 4s2n2δ−2nd−1. The last inequality holds due to inequality (12), because (2s+ 1 2s )2 ≤2 ⇔1 + 4s≤4s2 is true for s≥2. Remaining all-max-shifts. If s≥3, we move on to the third layer, which outputs M3. Similarly, we can show that uTM3 k is dominated by (2snδ−nd−1)3 maxj∈S3 k uT˜Hj because the rest of the terms in uTM3 k is strictly upper-bounded uTM3 k −(2snδ−nd−1)3 max j∈S3 k uT˜Hj <(1 + 3·2snδ−nd−1 + 3 ·(2snδ−nd−1)2)nδ−nd−1, which can then be shown to be smaller than (2snδ−nd−1)3 ·δ: (1 + 3·2snδ−nd−1 + 3·(2snδ−nd−1)2)nδ−nd ≤(1 + 6s+ 12s2)n3δ−3nd−2 ≤8s3n3δ−3nd−3 ·δ. The last inequality is due to the fact that 1 + 6s+ 12s2 ≤8s3 for s≥3, which can derived from (12). Repeating this process, after all slayers we get Ms, and uTMs k is dominated by (2snδ−nd−1)smax j∈Ss k uT˜Hj = (2snδ−nd−1)smax j∈[n] uT˜Hj = (2snδ−nd−1)s˜zγ(n). This is because the remaining terms in uTMs k can be strictly upper-bounded uTMs k −(2snδ−nd−1)s˜zγ(n) < (s−1∑ i=0 (s i ) (2snδ−nd−1)i ) nδ−nd, which is then dominated by the smallest difference possible in (2snδ−nd−1)s˜zγ(n): (s−1∑ i=0 (s i ) (2snδ−nd−1)i ) nδ−nd ≤ (s−1∑ i=0 (s i ) (2s)i ) (nδ−nd−1)s−1nδ−nd = ((1 + 2s)s −(2s)s)(nδ−nd−1)s ·δ≤(2snδ−nd−1)s ·δ. The last inequality used (1 + 2s)s −(2s)s ≤(2s)s, derived from (12). E.2.5 Verifying Properties 7.1 and 7.2 After these all-max-shift operations, we deﬁne the output Ms of the last all-max-shift layers to be the output of the function gc for input H, i.e., gc(H) := Ms. Property 7.1 requires that for any H ∈Hδ, all the components uTgc(H) need to be distinct. This is true, because for each column of uTgc(H), we have uTgc(H)k mod 2snδ−nd = uT˜Hk. 22This is because anything added by the all-max-shift operations is an integer multiple of 2snδ−nd, and uT˜Hk <nδ −nd <2nδ−nd for all k. Recall that ˜H is the input matrix for the ﬁrst max-shift operation, and that the components of uT˜H are zγ(1),˜zγ(2),..., ˜zγ(n), which were shown to be distinct by (9). Since uTgc(H)k produce distinct outputs for a mod operation, they themselves have to distinct. This proves Property 7.1. Also, by the “domination” argument in the previous subsection, the outputgc(H) has the property that for any column, uTgc(H)k lies inside an interval determined by ˜zγ(n), the unique id for the input H: uTgc(H)k ∈ [ (2snδ−nd−1)s˜zγ(n),(2snδ−nd−1)s(˜zγ(n) + δ) ) , and these intervals do not overlap because any different values of ˜zγ(n) must differ by at least δ. This means that for any input H,H′∈Hδ, the components in uTgc(H) and uTgc(H′) lie in disjoint intervals. Together with Property 7.1, this proves Property 7.2. E.3 Proof of Lemma 8 To prove this lemma, we implement a token-wise function that maps gtkn v (gc(H)k) = f(H −E)k, for all H ∈Hδ and k ∈[n]. From the construction of Lemma 7, there are n|Hδ|= n δdn distinct values of uTgc(H)k, and different values of uTgc(H)k differ by at least δ. The implementation of gtkn v can be done by stacking feed-forward layers so that each layer maps one unique number to the corresponding output column. More precisely, choose any H ∈Hδ. For each of the nvalues of uTgc(H)k, we add one feed- forward layer of the form Z ↦→Z+(f(H−E)k−gc(H)k)φ′(uTZ−uTgc(H)k1T n), φ′(t) = {0 t< −δ/2 or t≥δ/2, 1 −δ/2 ≤t<δ/ 2. This layer updates any column j of its input Z that satisﬁes uTgc(H)k −δ/2 ≤ uTZj < uTgc(H)k + δ/2, without modifying any other columns that are out of this range. We stack these layers for all possible values ofH ∈Hδ. After n δdn such layers, we get the desired function gv that satisﬁes gv(Z) = [ gtkn v (Z1) ··· gtkn v (Zn) ] , where for all H ∈Hδ and k∈[n], gtkn v (gc(H)k) = f(H −E)k. F Proof of Lemma 4 (Step 3 in § 4.1) In this section, we describe how the modiﬁed sparse Transformer networkg∈ST 2,1,1 constructed in Lemma 3 can be approximated with an original sparse Transformer network g∈ST 2,1,4. Recall that gis a “modiﬁed” sparse Transformer network, which employ the hardmax σH operators in place of ρ operators in sparse self-attention layers and piecewise linear activations φ∈Φ instead of ReLUs in feed-forward layers. The goal of this lemma is to approximate the functiong= gv ◦gc ◦gq ∈ST 2,1,1 with a standard sparse Transformer g = ˜gv ◦˜gc ◦˜gq ∈ST 2,1,4 with accuracy dp(g,g) ≤ϵ/2. As the construction of gconsists of three steps, we will approximate each of them step by step. The whole intuition behind the proof is that as long as we are considering Lp approximation, we can approximate σH and φ∈Φ as closely as we want with ρand ReLUs, respectively. However, as the proof will show, controlling the aggregated error over layers is not a trivial job. F.1 Approximating the quantization function gq (Lemma 6) We ﬁrst consider approximating gq from Lemma 6 with a standard feed-forward layer counterpart, ˜gq. Recall from § E.1 that the modiﬁed feed-forward layers used in gq are of the form Z ↦→Z + e(i)φ((e(i))TZ −kδ1T n), φ(t) = {0 t< 0 or t≥δ, −t 0 ≤t<δ, (14) 23for i∈[d] and k ∈[0 : n/δ−1]. Note that the activation φ∈Φ can be closely approximated by three ReLUs: ˜φα(t) := −ReLU(t) + 1 αReLU(t−(1 −α)δ) −1 −α α ReLU(t−δ) =    0 t≤0 or t≥δ, −t 0 ≤t≤(1 −α)δ, 1−α α (t−δ) (1 −α)δ≤t≤δ, where 0 < α <1. Note that ˜φα(t) = φ(t) except for an interval ((1 −α)δ,δ), and by shrinking α >0 this interval can be made arbitrarily small. Consider approximating the layers (14) with standard feed-forward layers, by replacing φwith its approximation ˜φα. Let the resulting function be ˜gq ∈ST 2,1,3. Then, it is easy to check that gq(X + E) = ˜gq(X + E) holds if all coordinates of X ∈[0,1)d×n are in the intervals of the form [kδ,(k+ 1 −α)δ] for some k ∈[0 : n/δ−1]; i.e., the intervals in which ˜φα perfectly approximates φ. The Lebesgue measure of the set of such inputs X is ((1 −α)δ)nd × 1 δnd = (1 −α)nd, and this can be made arbitrarily close to 1 by makingαsmall. As a result, “most” of the inputX ∈D satisﬁes gq(X + E) = ˜gq(X + E) ∈Hδ, while a small fraction (of measure at most 1 −(1 −α)nd) can map to some other values. For most of the remaining of the proof, we will consider the fraction of inputs mapped correctly to Hδ and bound their approximation error. We will come back to the 1 −(1 −α)nd fraction at the end of the proof. F.2 Approximating the contextual mapping gc (Lemma 7) Let us now consider approximating the contextual mapping gc in Lemma 7, constructed using the hardmax σH operators, with the standard sparse self-attention layers employing ρoperator. We will call the approximation ˜gc. Recall that ρsatisﬁes Assumption 2: Assumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying vj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑ j̸=j∗ρ[tv]j ≤η. This means that ρcan closely approximate σH in the sense that whenever the input vector v to the ρoperator has a maximum element vj∗ by some margin ζ, then the j∗-th component of the output ρ[tv] is close to 1, while the other components of ρ[tv] are close to 0. Recall that gc consists of two parts. The ﬁrst part is a composition of sparse selective shift operations, and the second is a composition of all-max-shift operations. We will ﬁrst examine how “errors” are introduced when σH is replaced with ρin both operations, discuss how the errors accumulate, and show how to choose the right ζand ηto control the errors in the approximation ˜gc. Errors introduced by ρ: Sparse selective shift operation. Recall that the key component in both the selective shift operation and all-max-shift operation is the sparse attention head ψl(·), which computes its k-th column as the following: ψl(Z; bQ)k := uTZAl k σH[(uTZAl k )T(uTZk −bQ)] = { maxj∈Al k uTZj if uTZk >bQ, minj∈Al k uTZj if uTZk <bQ. Now suppose we replaced σH with ρsatisfying Assumption 2. Suppose each entry in uTZ differs at least by δ, which is true in the construction of gc. We choose ζ = δ/2 and some 0 <η <1, and corresponding t> 0. Then, replace σH[·] with ρ[t·] and deﬁne ˜ψl(Z; bQ)k := uTZAl k ρ[t(uTZAl k )T(uTZk −bQ)]. If uTZk >bQ, it is easy to check that ˜ψl(Z; bQ)k satisﬁes (1 −η) max j∈Al k uTZj + η min j∈Al k uTZj ≤˜ψl(Z; bQ)k ≤max j∈Al k uTZj. (15) 24Similarly, if uTZk <bQ, we have min j∈Al k uTZj ≤˜ψl(Z; bQ)k ≤(1 −η) min j∈Al k uTZj + ηmax j∈Al k uTZj. Now consider the approximate sparse selective shift operator ˜Ψl, implemented with ˜ψl. For bQ <b′ Q, we deﬁne ˜Ψl(Z; c,bQ,b′ Q) := Z + [ ce(1) −ce(1)] [ ˜ψl(Z; bQ) ˜ψl(Z; b′ Q) ] . For any column Zk satisfying bQ <uTZk <b′ Q, we have (1 −2η) ( max j∈Al k uTZj −min j∈Al k uTZj ) ≤˜ψl(Z; bQ)k −˜ψl(Z; b′ Q)k ≤max j∈Al k uTZj −min j∈Al k uTZj, and for any column Zk satisfying uTZk /∈[bQ,b′ Q], we get |˜ψl(Z; bQ)k −˜ψl(Z; b′ Q)k|≤ η ( max j∈Al k uTZj −min j∈Al k uTZj ) . Recall that for the hardmax σH version, we had ψl(Z; bQ)k −ψl(Z; b′ Q)k = { maxj∈Al k uTZj −minj∈Al k uTZj if bQ <uTZk <b′ Q, 0 if uTZk /∈[bQ,b′ Q]. From this observation, the approximation error ˜Ψl−Ψl of the selective shift operator on the (j,k)-th entry of the output can be bounded as follows: ˜Ψl(Z; c,bQ,b′ Q)j,k −Ψl(Z; c,bQ,b′ Q)j,k ∈    [−2cηDl k,0] if j = 1,uTZk ∈(bQ,b′ Q), [−cηDl k,cηDl k] if j = 1,uTZk /∈[bQ,b′ Q], {0} if j ̸= 1, where we used Dl k := maxj∈Al k uTZj −minj∈Al k uTZj for simplicity. Errors introduced by ρ: All-max-shift operation. Next, we examine the approximation error of the all-max-shift operation introduced by replacement of σH with ρ. Let us deﬁne the approximate all-max-shift operation ˜Ωl: ˜Ωl(Z; c) = Z + ce(1) ˜ψl(Z; 0). From (15), we can check that the approximation error ˜Ωl −Ωl of the all-max-shift operation is bounded as ˜Ωl(Z; c)j,k −Ωl(Z; c)j,k ∈ {[−cηDl k,0] if j = 1, {0} if j ̸= 1. Errors in selective shift operations. Given these approximation error bounds of single operations, we now analyze the accumulation of errors through multiple layers. We ﬁrst consider the ﬁrst pδ−d self-attention layers in gc. Recall that they consist of selective shift layersΨl2 (·; δ−d,b−δ/2,b+δ/2) for b∈[0 : δ: δ−d+1 −δ] and (p−1)δ−d identity layers. A natural way to approximate these layers with standard self-attention layers is to use approximate layers ˜Ψl2 (·; δ−d,b −δ/2,b + δ/2), with sufﬁciently large t> 0. As we have seen above, there is no error introduced by ρexcept for the ﬁrst row. Thus, we will analyze the approximation error of ˜Ψl2 (·; δ−d,b −δ/2,b + δ/2) for the ﬁrst row only. Let us remind the readers how the ﬁrst selective shift operation (done by the ﬁrst pδ−d layers) originally worked in gc. The input to gc is H, and we deﬁne zk := uTHk and ∆ = ∑d−1 i=0 δ−i. Recall from Eqs. (7) and (8) in § E.2 that 0 ≤zγ(2) <zγ(3) <··· <zγ(n) <zγ(1) ≤(n−1)∆ + δ−d+1 −δ <nδ−d 25and zγ(2) ∈[0 : δ: δ−d+1 −δ], so zγ(2) will undergo the selective shift by one of the self-attention layers, which updates the (1,γ(2))-th entry of the input. Let ˜Hγ(2) be the updated value of the column and ˜zγ(2) := uT˜Hγ(2). The new sequence satisﬁes ∆ ≤zγ(3) <··· <zγ(n) <zγ(1) <˜zγ(2) <nδ−2d, where the strict upper bound on ˜zγ(2) is from Eq. (11). In case of the approximation ˜Ψl2 , we have seen that the error depends on the gap between maximum and minimum of uTZj’s, and this gap may grow larger as error accumulates; in the worst case, it may grow exponentially. To see this, suppose a0 and b0 are the maximum and minimum value of uTZj’s, and they go through a selective shift operation, but they do not belong to the range of the operation (bQ,b′ Q). Then, a0 and b0 will be updated to a1 and b1, which are bounded by a1 ≤a0 + δ−dη(a0 −b0), b1 ≥b0 −δ−dη(a0 −b0). After the next layer, we get a2 ≤a1 + δ−dη(a1 −b1) ≤a0 + δ−dη(a0 −b0) + δ−dη(1 + 2δ−dη)(a0 −b0), b2 ≥b1 −δ−dη(a1 −b1) ≥b0 −δ−dη(a0 −b0) −δ−dη(1 + 2δ−dη)(a0 −b0). Similarly, after ksuch layers, we get ak ≤a0 + (a0 −b0)δ−dη k−1∑ i=0 (1 + 2δ−dη)i, bk ≥b0 −(a0 −b0)δ−dη k−1∑ i=0 (1 + 2δ−dη)i, showing that the gap ak −bk may grow exponentially in the worst case: ak −bk ≤(1 + 2δ−dη)k(a0 −b0). In the error-less case (σH), for any input sequence H, the maximum possible difference between maximum and minimum of uTH is bounded above by nδ−d, and after one selective shift operation was done on the γ(2)-th column, the difference is then bounded by nδ−2d. Therefore, the worst-case possible error introduced by ρ is bounded above by the sum of the worst-case errors calculated assuming that we started off with max-min difference nδ−2d. Using this observation, the error on each ﬁrst-row entry of the sequence after the ﬁrst pδ−d layers is bounded above by 2nδ−2d ·δ−dη δ−d−1∑ i=0 (1 + 2δ−dη)i, (16) where a factor of 2 is introduced because when the selective shift operation is applied to the γ(2)-th column, it may introduce an error which is twice the magnitude of the error introduced to the other columns. We want to make (16) smaller than δ 8n. By Assumption 2, we can always choose t> 0 that satisﬁes the assumption for ζ = δ 2, and η= 1 2 δ2dlog ( 1 + δ2d˜δ 8n2 ) >0, where ˜δ:= min { δ,21−1/pϵ n1/p } . Using such t, we can control the total accumulated error by the ﬁrst pδ−d selective shift operations below ˜δ 8n: 2nδ−2d ·δ−dη δ−d−1∑ i=0 (1 + 2δ−dη)i ≤2nδ−3dη(1 + 2δ−dη)δ−d −1 (1 + 2δ−dη) −1 = nδ−2d    1 + log ( 1 + δ2d˜δ 8n2 ) δ−d   δ−d −1  ≤nδ−2d ( exp log ( 1 + δ2d˜δ 8n2 ) −1 ) = nδ−2dδ2d˜δ 8n2 = ˜δ 8n. 26Therefore, after the ﬁrst pδ−d selective shift layers, the accumulated error for each entry of the ﬁrst row is at most ˜δ/8n. We can also apply similar arguments to the remaining selective shift layers. For example, for the j-th set of pδ−d selective shift layers where the operation is done on γ(j+ 1)-th column of the input, the gap between the maximum and the minimum, including the accumulated error from previous layers, is bounded above by nδ−(j+1)d. Therefore, for this set of layers, the maximum accumulated error is bounded by 2nδ−(j+1)d ·δ−dη δ−d−1∑ i=0 (1 + 2δ−dη)i. So, choosing t> 0 that satisﬁes Assumption 2 for η = δ 2 and η = 1 2 δ2dlog(1 + δ(j+1)d˜δ 8n2 ), we can control the accumulated error introduced by the pδ−d layers below δ 8n: 2nδ−(j+1)d ·δ−dη δ−d−1∑ i=0 (1 + 2δ−dη)i ≤2nδ−(j+2)dη(1 + 2δ−dη)δ−d −1 (1 + 2δ−dη) −1 ≤nδ−(j+1)d    1 + log ( 1 + δ(j+1)d˜δ 8n2 ) δ−d   δ−d −1  ≤ ˜δ 8n. In total, the accumulated error by the ﬁrst p(n−1)/δd layers, which correspond to the selective shift operation part of the construction, is at most (n−1)˜δ 8n ≤ ˜δ 8 . Errors in all-max-shift operations. For all-max-shift operations, we approximate the hardmax σH all-max-shift operations Ωl(Z; nδ−nd) with its ρ-counterparts, ˜Ωl(Z; nδ−nd). We can similarly bound the accumulated error in the all-max-shift operations. Recall from § E.2 that during the whole series of all-max-shift operations, the maximum entry in the sequence is upper-bounded by (2snδ−nd−1)snδ−nd and minimum entry is lower-bounded by(n−1)∆. Therefore, the gap between the max and min elements, taking into consideration the errors from selective shift operations, is bounded from above by (2snδ−nd−1)snδ−nd. Then, using a similar argument as the select shift operation layers, the maximum error is bounded above by (2snδ−nd−1)snδ−nd ·nδ−ndη s−1∑ i=0 (1 + nδ−ndη)i, and we want to make it smaller than ˜δ 8 . By Assumption 2, we can always choose t> 0 that satisﬁes the assumption for ζ = δ 2, and η= δnd sn log ( 1 + δs(nd+1)+nd˜δ 2s+3ssns+1 ) >0. Using such t, we can control the total accumulated error by the ﬁrst pδ−d selective shift operations below ˜δ 8 : (2snδ−nd−1)snδ−nd ·nδ−ndη s−1∑ i=0 (1 + nδ−ndη)i ≤(2snδ−nd−1)snδ−nd ·nδ−ndη(1 + nδ−ndη)s −1 (1 + nδ−ndη) −1 = (2snδ−nd−1)snδ−nd    1 + log ( 1 + δs(nd+1)+nd˜δ 2s+3ssns+1 ) s   s −1   ≤(2snδ−nd−1)snδ−ndδs(nd+1)+nd˜δ 2s+3ssns+1 = ˜δ 8. 27So far, we have analyzed the total accumulated error of approximating the contextual mapping function gc (constructed with hardmax σH) with an approximation ˜gc (constructed with ρ). We have seen that for any input H ∈Hδ, the approximation error can be controlled so that the error by the selective shift operation part is at most ˜δ/8 and the all-max-shift operation part is at most ˜δ/8. Therefore, the total error of the (j,k)-th entry can be bounded as ˜gc(H)j,k −gc(H)j,k ∈ { [− ˜δ 4 , ˜δ 4 ] j = 1, {0} j ̸= 1, for any H ∈Hδ. F.3 Approximating the value mapping gv (Lemma 8) We now consider the approximation of the value mappinggv with standard feed-forward layers. In gv, we implemented the function with layers of the form Z ↦→Z+(f(H−E)k−gc(H)k)φ′(uTZ−uTgc(H)k1T n), φ′(t) = {0 t< −δ/2 or t≥δ/2, 1 −δ/2 ≤t<δ/ 2. Since the output of contextual mapping gc(H) and its approximation ˜gc(H) differ in only the ﬁrst row and by ˜δ/4 ≤δ/4, one can approximate each layer in gv by replacing φ′with an approximation ˜φ′, implementable with four ReLU’s: ˜φ′(t) =    0 t< −δ/2 or t≥δ/2, 4 δt+ 2 −δ/2 ≤t< −δ/4, 1 −δ/4 ≤t<δ/ 4, −4 δt+ 2 δ/4 ≤t<δ/ 2. Let ˜gv be the approximation of gv constructed this way. Because the error on ˜gc is bounded by ˜δ/4, the error on the ﬁnal output ˜gv is also bounded by ˜δ/4. That is, for any H ∈Hδ, ˜gv(˜gc(H))j,k −gv(gc(H))j,k ∈ { [− ˜δ 4 , ˜δ 4 ] j = 1, {0} j ̸= 1. Hence, using ˜δ:= min { δ,21−1/pϵ n1/p } , we have ∥˜gv(˜gc(H)) −gv(gc(H))∥p p ≤n (˜δ 4 )p ≤1 2 (ϵ 2 )p , for all H ∈Hδ. F.4 Finishing the proof Recall from § F.1 that the approximated quantization function ˜gq maps most of the input X ∈D to H ∈Hδ, and a small fraction of them (of measure at most 1 −(1 −α)nd) to something else. Note now that the original function g = gv ◦gc ◦gq and the approximation g = ˜gv ◦˜gc ◦˜gq are both bounded, so there is a global constant Bsuch chat ∥g(X + E) −g(X + E)∥p ≤Bfor all X ∈D. We can divide the integral overD to two disjoint sets. The ﬁrst one D1 := {X ∈D |˜gq(X + E) ∈ Hδ}is the set of input X mapped to Hδ by ˜gq, and the other is its complement D2 = D \\D1. dp(g,g)p := ∫ D ∥g(X + E) −g(X + E)∥p pdX = ∫ D1 ∥g(X + E) −g(X + E)∥p pdX + ∫ D2 ∥g(X + E) −g(X + E)∥p pdX ≤1 2 (ϵ 2 )p + (1 −(1 −α)nd)Bp. One can make α close enough to 1 so that the second term is less than 1 2 (ϵ 2 )p . This makes dp(g,g) ≤ϵ/2, hence ﬁnishing the proof. 28G Experimental setup G.1 Copying task We generated the synthetic dataset for the copying task. The input sequence to the copying task has the format 0s0s, where s is a 127 length sequence of symbols randomly sampled from the range of [0,127]. The training set contains 100K sequences, while the testing set contains 10K sequences. We implement the copying task as a masked-LM [10] style prediction task by masking all the tokens in the second half of the sequence. For the test examples, each masked token is predicted independently. For the results reported in § 5, we experiment with bidirectional models, where each token can attend to both previous and future tokens. The maximum sequence length is n= 256, and we use embedding dimension d= 256. The model has 1 to 4 attention layers with h= 4 attention heads of size m= 64, followed by a feed-forward hidden layer of size r= 512. We train the model with the AdamW optimizer with weight decay and no dropout. We train the model using 3,000 warmup steps and a total of 500K training steps. The learning rate is 1e−4. We use the batch size 1,024 on 8 TPUv3 chips. For all sparsity patterns other than the RANDOM pattern, we choose the segment length wto be 16 for all patterns. This segment length results in the sparsest level for the STRIDED and FIXED patterns. In Table 1, we include the sparsity level as a reference. For this task, we report the prediction accuracy for all the tokens. G.2 Language modeling For the language modeling task, we train on the One Billion Word Benchmark [5] which contains almost one billion tokens and a vocabulary of more than 800K tokens. We use the Transformer model in the Tensor2Tensor framework [29]. We use a 12-block (cf. (2)) Transformer, with embedding dimension d = 256, maximum sequence length n = 256, number of heads h= 8, head size m= 64, and feed-forward hidden layer size r = 1024. Since language modeling task is auto-regressive (attending to only past tokens) in nature, we evaluate the (sparse) attention score matrices and mask them to be an upper-triangular matrix. We train the model with the Adafactor with weight decay. We train the model using 10K warmup steps and a total of 240K steps. We use the batch size 4,096 on 8 TPUv2 chips. For this task, we report the perplexity. G.3 Translation For the translation task, we train on the WMT18 en-cs datasets (Europarl v7, Common Crawl corpus, News Commentary v13, and CzEng), with a total of 15M pairs of sentences, and test on the newstest2015 en-cs dataset, with 2,656 pairs. We use the encoder-decoder architecture and apply the sparse attention on both encoder and decoder. We use the Transformer model in the Tensor2Tensor framework [ 29] and the same setup as the language modeling task, except for having 6 blocks in the Transformer networks, with head size m= 32 and having autoregressive patterns only in decoders. For this task, we report the cased BLEU score. G.4 GLUE tasks For the GLUE tasks, we use the pre-training and ﬁne-tuning framework [10]. Following Devlin et al. [10] we ﬁrst pre-train a BERTBASE model for 450K steps on the BooksCorpus [36] (800M words) and the English Wikipedia datasets (2,500M words). We later ﬁnetune the model on data from each task separately. For each setting, we use the same sparsity pattern and head conﬁguration in both the pre-training and the ﬁne-tuning stages. The sequence length is n= 128 in both stages. We report the average accuracy of three runs on the dev set for all tasks. For each setting, we pre-train a model and run ﬁne-tuning three times. 29Table 2. Accuracy on the synthetic copying task when using an auto-regressive model. Percentages in parentheses mark the sparsity levels. STRIDED FIXED STAR RANDOM Depth UNION (87%) MULTIHEAD (93%) SEQUENTIAL (93%) UNION (87%) MULTIHEAD (93%) SEQUENTIAL (93%) (87%) (90%) 1-layer 0.79% 0.78% 0.78% 7.02% 7.04% 0.81% 0.77% 33.13% 2-layer 12.40% 8.26% 1.57% 73.43% 13.24% 92.10% 12.32% 67.30% 3-layer 94.50% 65.58% 60.88% 99.87% 70.82% 99.84% 14.03% 89.50% 4-layer 100% 100% 98.40% 99.97% 99.16% 99.97% 31.19% 95.88% (a) WMT en-de  (b) WMT de-en Figure 3. Comparison of sparsity patterns and different head conﬁgurations on the WMT de-en and en-de translation tasks. (a) CoLA  (b) MRPC Figure 4. Comparison of sparsity patterns and different head conﬁgurations on the CoLA and MRPC tasks for the BERTBASE model. H Additional experimental results We report additional experimental results in this section. H.1 Copying task We include the results for the copying task using auto-regressive (unidirectional) models as in LM, where each token can only attend to previous tokens, in Table 2. In this case, the STAR pattern cannot attend to the last replay token. Indeed, the STAR pattern shows better performance when the model is bidirectional (cf. Table 1). H.2 Translation We present experimental results of the translation tasks on the WMT English-German and German- English datasets in Figure 3. We train on WMT18 (Europarl v7, Common Crawl corpus and News Commentary v13) and test on newstest 2015 datasets. The ﬁgures show similar trends to the results on the WMT en-cs dataset in Figure 1b. 30H.3 GLUE tasks Figure 4 presents the results comparing the sparsity patterns and the head conﬁgurations on the CoLA and MRPC tasks using the BERTBASE model. CoLA is a single-sentence classiﬁcation task, asking if a sentence is a grammatical English sentence. MRPC is a sentence-pair classiﬁcation task, where each example is a pair of sentences and the label indicates whether the sentences are semantically equivalent. 31",
      "meta_data": {
        "arxiv_id": "2006.04862v2",
        "authors": [
          "Chulhee Yun",
          "Yin-Wen Chang",
          "Srinadh Bhojanapalli",
          "Ankit Singh Rawat",
          "Sashank J. Reddi",
          "Sanjiv Kumar"
        ],
        "published_date": "2020-06-08T18:30:12Z",
        "pdf_url": "https://arxiv.org/pdf/2006.04862v2.pdf"
      }
    },
    {
      "title": "Robust Graph Representation Learning via Neural Sparsification"
    },
    {
      "title": "Even Sparser Graph Transformers",
      "abstract": "Graph Transformers excel in long-range dependency modeling, but generally\nrequire quadratic memory complexity in the number of nodes in an input graph,\nand hence have trouble scaling to large graphs. Sparse attention variants such\nas Exphormer can help, but may require high-degree augmentations to the input\ngraph for good performance, and do not attempt to sparsify an already-dense\ninput graph. As the learned attention mechanisms tend to use few of these\nedges, such high-degree connections may be unnecessary. We show (empirically\nand with theoretical backing) that attention scores on graphs are usually quite\nconsistent across network widths, and use this observation to propose a\ntwo-stage procedure, which we call Spexphormer: first, train a narrow network\non the full augmented graph. Next, use only the active connections to train a\nwider network on a much sparser graph. We establish theoretical conditions when\na narrow network's attention scores can match those of a wide network, and show\nthat Spexphormer achieves good performance with drastically reduced memory\nrequirements on various graph datasets.",
      "full_text": "Even Sparser Graph Transformers Hamed Shirzad University of British Columbia shirzad@cs.ubc.ca Honghao Lin Carnegie Mellon University honghaol@andrew.cmu.edu Balaji Venkatachalam Meta∗ bave@meta.com Ameya Velingker Independent Researcher∗ ameyav@gmail.com David P. Woodruff CMU & Google Research dwoodruf@cs.cmu.edu Danica J. Sutherland UBC & Amii dsuth@cs.ubc.ca Abstract Graph Transformers excel in long-range dependency modeling, but generally require quadratic memory complexity in the number of nodes in an input graph, and hence have trouble scaling to large graphs. Sparse attention variants such as Exphormer can help, but may require high-degree augmentations to the input graph for good performance, and do not attempt to sparsify an already-dense input graph. As the learned attention mechanisms tend to use few of these edges, such high- degree connections may be unnecessary. We show (empirically and with theoretical backing) that attention scores on graphs are usually quite consistent across network widths, and use this observation to propose a two-stage procedure, which we call Spexphormer: first, train a narrow network on the full augmented graph. Next, use only the active connections to train a wider network on a much sparser graph. We establish theoretical conditions when a narrow network’s attention scores can match those of a wide network, and show that Spexphormer achieves good performance with drastically reduced memory requirements on various graph datasets. Code can be found at https://github.com/hamed1375/Sp_Exphormer. 1 Introduction The predominant story of the last half-decade of machine learning has been the runaway success of Transformer models (Vaswani et al., 2017), across domains from natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Zaheer et al., 2020) to computer vision (Dosovitskiy et al., 2020) and, more recently, geometric deep learning (Dwivedi and Bresson, 2020; Kreuzer et al., 2021; Ying et al., 2021; Rampášek et al., 2022; Shirzad et al., 2023; Müller et al., 2023). Conventional (“full”) Transformers, however, have a time and memory complexity of O(nd2 + n2d), where n is the number of entities (nodes, in the case of graphs), and d is the width of the network. Many attempts have been made to make Transformers more efficient (see Tay et al. (2020) for a survey on efficient variants for sequence modeling). One major line of work involves sparsifying the attention mechanism, constraining attention from all O(n2) pairs to some smaller set of connections. For instance, for sequential data, BigBird (Zaheer et al., 2020) constructs a sparse attention mechanism by combining sliding windows, Erd˝os-Rényi auxiliary graphs, and universal connectors. Similarly, for graph data, Exphormer (Shirzad et al., 2023) constructs a sparse interaction graph consisting of edges from the input graph, an overlay expander graph, and universal connections. We refer to such a network as a sparse attention network. Exphormer reduces each layer’s complexity from O(nd2 + n2d) to O((m + n)d2), where n is the number of nodes, m is the number of interaction edges in the sparse attention mechanism, and d is ∗Work done in part while at Google. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2411.16278v1  [cs.LG]  25 Nov 2024the hidden dimension or width. Even so, training is still very memory-intensive for medium to large scale graphs. Also, for densely-connected input graphs with Θ(n2) edges, there is no asymptotic improvement in complexity, as Exphormer uses all of theΘ(n2) edges of the original input graph. Our goal is to scale efficient graph Transformers, such as Exphormer, to even larger graphs. One general approach for scaling models to larger graphs is based on batching techniques. Prominent approaches include egocentric subgraphs and random node subsets (Wu et al., 2022, 2023, 2024). Egocentric subgraphs choose a node and include all of its k-hop neighbors; the expander graphs used in Exphormer, however, are exactly defined so that the size of these subgraphs grows exponentially in the number of layers – prohibitively expensive for larger graphs. A similar issue arises with universally-connected nodes, whose representation depends on all other nodes. For uniformly- random subset batching, as the number b of batches into which the graph is divided grows, each edge has chance 1 b to appear in a given step. Thus, b cannot be very large without dropping important edges. A similar problem can happen in random neighbor sampling methods such as GraphSAGE (Hamilton et al., 2017). Although this model works well on message-passing neural networks (MPNNs) which only use the graph edges, using it for expander-augmented graphs will select only a small ratio of the expander edges, thereby breaking the universality properties provided by the expander graph. Expander graphs enable global information propagation, and when created with Hamiltonian cycles and self-loops, produce a model that can provably approximate a full Transformer (Shirzad et al., 2023, Theorem E.3). Yet not all of these edges turn out to be important in practice: we expect some neighboring nodes in the updated graph to have more of an effect on a given node than others. Thus, removing low-impact neighbors can improve the scalability of the model. The challenge is to identify low-impact edges without needing to train the (too-expensive) full model. Figure 1 illustrates other advantages of this batching approach; this is also discussed further in Appendix G. One approach is to train a smaller network to identify which edges are significant. It is not obvious a priori that attention scores learned from the smaller network will estimate those in the larger network, but we present an experimental study verifying that attention scores are surprisingly consistent as the network size reduces. We also give theoretical indications that narrow networks are capable of expressing the same attention scores as wider networks of the same architecture. Our approach. We first train a small-width network in order to estimate pairwise attention score patterns, which we then use to sparsify the graph and train a larger network. We first train the graphs without edge attributes. This reduces the complexity of Exphormer to O(md + nd2) and then by training a much smaller width ds ≪ d network, reduces the time and memory complexity by at least a factor of d/ds. We also introduce two additions to the model to improve this consistency. Training this initial network can still be memory-intensive, but as the small width implies the matrix multiplications are small, it is practical to train this initial model on a CPU node with sufficient RAM (typically orders of magnitude larger than available GPU memory), without needing to use distributed computation. Once this initial model is trained, the attention scores can be used in creating a sparse graph, over which we train the second network. These initial attention scores can be used as edge features for the second network. As mentioned previously, we use the attention scores obtained from the trained low-width network to sparsify the graph. By selecting a fixed number c of edges per attention layer for each node, we reduce the complexity of each layer to O(nd2 + ndc). This sparsification alleviates the effect of a large number of edges, and allows for initial training with a larger degree expander graph, since most of the expander edges will be filtered for the final network. This sparsification differs from conventional graph sparsification algorithms (for MPNNs) in three ways. First, we use expander edges, self-loops, and graph edges and sparsify the combination of these patterns together. Second, this sparsification is layer-wise, which means that in a multi-layer network the attention pattern will vary from layer to layer. Finally, our sampling uses a smaller network trained on the same task, identifying important neighbors based on the task, instead of approaches independent of the task such as sampling based on PageRank or a neighbor’s node degree. Another advantage of this approach is that the fixed number of neighbors for each node enables regular matrix calculations instead of the edge-wise calculations used by Kreuzer et al. (2021); Shirzad et al. (2023), greatly improving the speed of the model. After this reduction, batching can be done based on the edges over different layers, enabling Transformers to be effectively batched while still effectively approximating the main Transformer model, enabling modeling long-range dependencies. In batching large graphs, naive implementations of sampling without replacement from 2(a) (b) (d) (e) (f) (c) Figure 1: Figure (a) shows a very simple synthetic graph where each node has a binary classification task of determining whether there exists a node of the opposite color in the same connected component. This task requires learning long-range dependencies. Figure (b) shows a natural clustering of the graph. This clustering would mean no node can do its task if models are trained only on one cluster at a time. Figure (c) shows a neighbor sampling starting from the green node, where random sampling fails to select the single important edge that bridges to the different-colored nodes. Figure (d) shows a random subset sampling strategy, where the task is solvable if and if only the two sides of the bridge between the two colors get selected. If we increase the size of each cluster, while keeping just one edge between two colors, the probability of selecting the bridge in any batch goes to zero, and thus the training will fail in this scenario. (e) shows attention scores between the nodes if trained with an attention-based network. Dashed lines have near zero attention scores, and thicker lines indicate a larger attention score. Knowing these attention scores will mean each node with just one directional edge can do the task perfectly. The attention edges are shown in (f). In case two nodes are equally informative; selecting either of them leads to the correct result. attention edges with varying weights can be very slow. This is especially true if the attention scores are highly concentrated on a small number of neighbors for most of the nodes. We use reservoir sampling (Efraimidis and Spirakis, 2006), enabling parallel sampling with an easy, efficient GPU implementation, improving the sampling process significantly. We only use the Transformer part of the Exphormer model, not the dual MPNN+Transformer architecture used by Shirzad et al. (2023); Rampášek et al. (2022). Unlike the Exphormer approach, we do not assume that the expander graph is of degree O(1); we can see this as interpolating between MPNNs and full Transformers, where smaller degree expander graphs mostly rely on the graph edges and are more similar to MPNNs, while higher degree expander graphs can resemble full attention, in the most extreme case of degree n − 1 exactly recovering a full Transformer. To summarize, the contributions of this paper are as follows: 1) We experimentally and theoretically analyze the similarity of attention scores for networks of different widths, and propose two small architectural changes to improve this similarity. 2) We propose layer-wise sparsification, by sampling according to the learned attention scores, and do theoretical analysis on the sparsification guarantees of the attention pattern. 3) Our two-phase training process allows us to scale Transformers to larger datasets, as it has significantly smaller memory consumption, while maintaining competitive accuracy. 2 Related Work Graph Transformer Architectures. Attention mechanisms were proposed in early (message- passing) Graph Neural Network (GNN) architectures such as Graph Attention Networks (GAT) (Veliˇckovi´c et al., 2018), where they guide node aggregation among neighbors, without using positional encodings. GraphBert (Zhang et al., 2020) finds node encodings based on the underlying graph structure. Subsequent work has proposed full-fledged graph Transformer models that generalize sequence Transformers (Dwivedi and Bresson, 2020) and are not limited to message passing between nodes of the input graph; these include Spectral Attention Networks (SAN) (Kreuzer et al., 2021), Graphormer (Ying et al., 2021), GraphiT (Mialon et al., 2021), etc. GraphGPS (Ram- pášek et al., 2022) combines attention mechanisms with message passing, allowing the best of both worlds. 3Efficient Graph Transformers. Several recent works have proposed various scalable graph trans- former architectures. NAGphormer (Chen et al., 2022a) and Gophormer (Zhao et al., 2021) use a sampling-based approach. On the other hand, Difformer (Wu et al., 2023) proposes a continuous time diffusion-based transformer model. Exphormer (Shirzad et al., 2023) proposes a sparse graph that combines the input graph with edges of an expander graph as well as virtual nodes. They show that their model works better than applying other sparse Transformer methods developed for sequences. Another work, NodeFormer (Wu et al., 2022), which is inspired by Performer (Choromanski et al., 2021), uses the Gumbel-Softmax operator as a kernel to efficiently propagate information among all pairs of nodes. SGFormer (Wu et al., 2024) shows that just using a one layer transformer network can sometimes improve the results of GCN-based networks and the low memory footprint can help scale to large networks. Perhaps most conceptually similar to our work is Skeinformer (Chen et al., 2022b), which uses sketching techniques to accelerate self-attention. Sampling and batching techniques. Some sampling-based methods have been used to alleviate the problem of “neighborhood explosion.” For instance, sampling was used in GraphSAGE (Hamilton et al., 2017), which used a fixed-size sample from a neighborhood in the node aggregation step. GraphSAINT (Zeng et al., 2020) scales GCNs to large graphs by sampling the training graph to create minibatches. Other. Expander graphs were used in convolutional networks by Prabhu et al. (2018). 3 Preliminaries and Notation Exphormer. EXPHORMER is an expander-based sparse attention mechanism for graph transformers that uses O(|V | + |E|) computation, where G = (V, E) is the underlying input graph. Exphormer creates an interaction graph H that consists of three main components: edges from the input graph, an overlaid expander graph, and virtual nodes (which are connected to all the original nodes). For the expander graph component, Exphormer uses a constant-degree random expander graph, with O(n) edges. Expander graphs have several useful theoretical properties related to spectral approximation and random walk mixing, which allow the propagation of information between pairs of nodes that are distant in the input graph G without explicitly connecting all pairs of nodes. The expander edges introduce many alternative short paths between the nodes and avoid the information bottleneck that can be caused by the virtual nodes. Our model. We use H to denote the attention pattern, and NH(i) the neighbors of node i under that pattern. Let X = (x1, x2, . . . ,xn) ∈ Rd×n be the matrix of d-dimensional embeddings for all of the n nodes. Our primary “driver” is then h-head attention: using ⊙ for element-wise multiplication, ATTN H(X):,i = xi + hX j=1 Vj i · σ \u0010\u0000 Ej ⊙ Kj\u0001T Qj i + Bj \u0011 , where Vj i = Wj V XNH(i), K = Wj KXNH(i), and Qj i = Wj Qxi, are linear mappings of the node features for the neighbors XNH(i), and Ej = Wj EENH(i) and Bj = Wj BENH(i) are linear maps of the edge features E, which is a dE × |NH(i)| matrix of features for the edges coming in to node i. Exphormer uses learnable edge features for each type of added edge, and original edge features for the graph’s edges. If the graph does not have any original edge features, it uses a learnable edge feature across all graph edges. Edge features help the model distinguish the type of attention edges. Here, σ is an activation function. In both Exphormer and our work the activation function is ReLU. In the absence of edge features, which is the case for most of the transductive datasets, including the datasets that have been used in this paper, Ee for any attention edge e can have one of three possible representations, and so Ej can be computed more simply by first mapping these three types of edge features with Wj E for head j, and then replacing the mapped values for each edge type. This simple change reduces the complexity of the Exphormer from O(md2 + nd2) to O(md + nd2). Compared to prior work, we introduce Bj as a simpler route for the model to adjust the importance of different edge types. Considering Exphormer as an interpolation between MPNNs and full Transformers, the Bj model has an easier path to allow for attention scores to be close to zero for all non-graph attention edges, without restricting the performance of the attention mechanism on 4graph edges. Consequently, it can function roughly as an MPNN (similar to GAT) by zeroing out the non-local attention paths. We use dE = d, and have each layer output features of the same width as its input, so that each of the Wj · parameter matrices except for Wj B are d × d, and Wj B is d × 1. As a simple illustration that Ej is insufficient to allow near-zero attention scores, thus highlighting the importance of Bj, note that if the columns of K and Q are distributed independently and uniformly on a unit ball (e.g., under a random initialization), there is no vector Ej which is identical for all edges of an expander graph that can make the attention scores for all the expander edges near-zero. Our network compared to Exphormer. We use Exphormer as the base model because it provides us the flexibility to adjust the sparsity of the attention graph and to interpolate between MPNNs and full Transformers. Exphormer can model many long-range dependencies that are not modeled by MPNNs and are very expensive to model in a full Transformer. For example, one cannot train a full Transformer model in the memory of a conventional GPU device for a dataset such as Physics, which has a graph on just 34K nodes. In our instantiation of Exphormer, we add self-loops for every node and use d/2 random Hamiltonian cycles to construct our expander graph as described in (Shirzad et al., 2023, Appendix C.2). We do not add virtual nodes in our networks. (Even so, the resulting network is still a universal approximator; Shirzad et al., 2023, Theorem E.3). Although the best known results for Exphormer combine sparse attention with MPNNs, in this work, we avoid the MPNN component for scalability reasons. We also make two additional changes; see Section 4. 4 Method Our method consists of a two-phase training process. The first phase trains a model we call the Attention Score Estimator Network, whose goal is to estimate the attention scores for a larger network. This model is not particularly accurate; its only goal is for each node to learn which neighbors are most important. The learned attention scores for each layer of the first network are then used to construct sparse interaction graphs for each layer in a second model, which is trained (with hyperparameter tuning for the best results) and serves as the final predictor. Attention Score Estimator Network. For this network, we use a width of 4 or 8, with just one attention head, in our training. We tune the other hyperparameters in order to have a converged training process with reasonably high accuracy, but we do not spend much time optimizing this network as it is sufficient to learn the important neighbors for each node, i.e., edges with high attention scores. This network will be trained with as many layers as the final network we want to train. Because it is so narrow, it has many fewer parameters and hence much less memory and time complexity, making it cheaper to train. Moreover, we only need to do this training once per number of layers we consider, conditioned on the fact that the training converges, even if the final model has a large number of hyperparameters. Compared to Exphormer, we use a much higher-degree expander graph: 30 to 200 instead of the 6 used for most transductive graphs by Shirzad et al. (2023). As most of the considered datasets do not have edge features, we use a learnable embedding for each type of edge (graph edge, expander edge, or self-loop). We also make two small changes to the architecture and the training process of this model, discussed below. Section 5.1 shows experimentally that the low-width network is a good estimator of the attention scores for a large-width network. Normalizing V . Having a smaller attention score, αij < αij′ , does not necessarily mean that j’s contribution to i’s new features is smaller than that of j′: if ∥Vj∥ ≫ ∥Vj′ ∥, the net contribution of j could be larger. Although Transformers typically use layer normalization, they do not typically do so after mapping X to V. We normalize the rows of V to have the same vector sizes for all nodes. In our experiments, normalizing to size one reduced performance significantly; however, adding a learnable global scale s, so that Vi becomes sVi ||Vi||2 , maintained performance while making attention scores more meaningful. Variable Temperature One of the side goals is to have sharper attention scores, guiding the nodes to get their information from as few nodes as possible. Using temperature in the attention mechanism can do this, where logits will be divided by a temperature factor τ before being fed into a softmax. Normal attention corresponds to τ = 1; smaller τ means sharper attention scores. However, setting the temperature to a small value from the beginning will make the random initialization more significant, and increase the randomness in the training process. Instead, we start with τ = 1.0 and gradually anneal it to 0.05 by the end of the training. We set an initial phase for λ epochs 5v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 8 v 1  v 2  v 3  v 4  v 5  v 6  v 7  v 8  KV Q Add & SoftMax Sparse MatMul Feed Forward Network Normalize E B Dot Product Low-width Network Layer (b)  v 1 v 2  v 3  v 4  v 5 v 6  v 7  v 8  v 1 v 2  v 3  v 4  v 5 v 6  v 7  v 8  + v 1 v 2  v 3  v 4  v 5 v 6  v 7  v 8  (a) (c)  v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 8 v 1  v 2  v 3  v 4  v 5  v 6  v 7  v 8  Layer-wise Sampling KV Q Add & SoftMax Sparse MatMul Feed Forward Network E B Dot Product High-width Network Layer (f) (d) (e)  Figure 2: Steps of our method. (a) The attention mechanism for the attention score estimator network combines graph edges with an expander graph and self-loops. The expander graphs are constructed by combining a small number of Hamiltonian cycles – here two, in red and in purple – then confirming the spectral gap is large enough. (b) Self-attention layers in the estimator network use this sparse attention mechanism; its self-attention layers normalize V. (c, d) Attention scores are extracted from this network for each layer, and used to sample, in (e), a sparse directed graph, which becomes the attention graph for the final network (f). This network, with a much larger feature dimension, does not normalize V. where we use τ = 1; this lets the model learn which neighbors are more important for each node slowly. We multiply τ with a factor γ after each epoch, obtaining a temperature in epoch t > λof max(γt−λ, 0.05). We use λ = 5 and γ = 0.99 or 0.95 depending on how fast the learning converges. Sparser Attention Pattern. The memory and time complexity of Exphormer is linearly dependent on the number of edges. Also, with a small number of layers, the expander degree should be high enough to ensure a large enough receptive field for each node in order to learn the long-range dependencies. Not all these edges are equally important, and many of them will have a near-zero effect on the final embedding of each node. Reducing the number of edges can alleviate memory consumption. Additionally, a sparser pattern lets us use batching techniques for the larger graphs. In this work, we analyze how effectively the sparser model can work and up to what factor we can sparsify. For each layer, e.g., ℓ, we select a degℓ as a fixed degree for each node and sample without replacement according to the attention score estimator network’s attention scores in each epoch of training or evaluation. Having the same degree for each node’s attention pattern also means that attention can be calculated using (much-more-optimized) standard matrix multiplications, rather than the propagation techniques used in Exphormer and SAN (Kreuzer et al., 2021). To sparsify the graph, in each epoch, we sample a new set of edges according to the learned attention scores from the smaller network. The reason why we do this rather than a simpler strategy such as selecting top-scored edges is that in many cases, several nodes can have very similar node features. If we assume nodes u1, u2, . . . , up from the neighbors of node v have almost the same features, and if the attention scores for these nodes are α1, α2, . . . , αp, any linear combination of Pp i=1 αi = α will lead to the same representation for node v. If features are exactly the same, α will be divided between these nodes, and even if α is large, each node’s attention score from v can be small. By sampling, we have a total α chance of selecting any of the nodes u1:p. In each epoch, we re-sample a new set of edges for each node from its original neighborhood. Faster Sampling Using Reservoir Sampling. Sampling without replacement using default li- brary calls is very slow, especially if few neighbors dominate the attention scores. We instead use reservoir sampling (Efraimidis and Spirakis, 2006), which is GPU-friendly and parallelizable. For reservoir sampling of k neighbors from the neighborhood of node i, with attention scores a = ( a1, a2, ··· , a|NH(i)|), we first take a uniform random sample u = ( u1, u2, ··· , u|NH(i)|), where the ui are i.i.d. samples from Uniform(0, 1). Then we calculate 1 a ⊙log(u) with element-wise 6multiplication, and select the indices with the top k values from this list. Selecting k-th rank from n values and pivoting has a worst-case O(n) time algorithm, which is much faster than the O(nk) worst case time for trial-and-error. Pseudocode is given in Algorithm 1. The GPU-friendly version of this can be implemented by sampling for nodes in parallel, but requires forming a regular matrix for the attention scores. This can be done by extending each attention score vector to the maximum degree, or selecting a value k′ ≫ k and first sampling k′ and selecting the top k′ attention scores from each node, making sure that the sum of the rest of the neighbor’s attention scores are very near to zero. Then by forming a rectangular attention matrix, uniform sampling and element-wise multiplications are much faster on GPU, and sampling from the entire batch is much more efficient. Algorithm 1 Reservoir Sampling from a Node’s Neighborhood Input: Attention scores a = a(ℓ) i,NH(i), number of neighbors to sample: degℓ Output: List of degℓ neighbors of node i 1: function RESERVOIR SAMPLE (a, degℓ) 2: u ∼ Uniform(0, 1)|NH(i)| 3: return argtopdegℓ(1 a ⊙ log(u)) 4: end function Batching. Each batch starts with a random subset of “target” nodes B. These are the nodes whose last-layer representations we will update in this optimization step. To calculate these representations, we need keys and values based on the previous layer’s representations for the relevant neighbors of each target node (again, sampling neighbors from the graph augmented by an expander graph). To approximate this, we sample degL neighbors for each target node. Then we have a set of at most |B|(degL +1) nodes whose representations we need to calculate in layerL−1; we repeat this process, so that in layer ℓ we need to compute representations for up toQ(ℓ) ≤ min(|B|QL i=ℓ+1(degi +1), n) query nodes, with |Q(ℓ)|degℓ attention edges. Pseudocode is given in Algorithm 2. When the number of layers L and degree degℓ are not too large, this batching can be substantially more efficient than processing the entire graph. Moreover, compared to other batching techniques, our approach selects neighbors according to their task importance. Except for optimization dynamics in the training process corresponding to minibatch versus full-batch training, training with batches is identical to training with the entire sparsified graph; if we choose a large degℓ equal to the maximum degree of the augmented graph, this is exactly equivalent to SGD on the full graph, without introducing any biases in the training procedure. This is in stark contrast to previous approaches, as illustrated in Figure 1. Unlike these prior approaches, which typically use the full graph at inference time, we can run inference with batch size as small as one (trading off memory for computation). Algorithm 2 Neighborhood Sampling for a Batch of Nodes Input: Attention scores in each layer: a = n a(ℓ) i,j | ∀i ∈ V, j∈ NH(i), ,1 ≤ ℓ ≤ L o , number of neighbors to sample in each layer: deg = {deg1, ··· , degL}, and a batch of nodes B ⊆ V Output: Q(ℓ), K(ℓ), V(ℓ), query, key, and value nodes in each layer 1: function SAMPLE NEIGHBORHOOD (B, a, deg) 2: V(L+1) ← B 3: for ℓ ← L to 1 do 4: Q(ℓ) ← V(ℓ+1) 5: for i ← i ∈ Q(ℓ) do 6: K(ℓ) i ← RESERVOIR SAMPLE (ai,NH(i), degℓ) 7: end for 8: K(ℓ) ← S i∈Qℓ K(ℓ) i 9: V(ℓ) ← Q(ℓ) SK(ℓ) 10: end for 11: return \b\u0000 V(ℓ), Q(ℓ), K(ℓ)\u0001 | 1 ≤ ℓ ≤ L \t 12: end function 7Fixed Node Degree Layers. Sparse matrix operations are not yet nearly as efficient as dense operations on GPU devices. Exphormer and SAN use a gather operation, which is memory-efficient but not time-efficient on a GPU (Zaheer et al., 2020). By normalizing the degree, instead of having |Q(ℓ)|degℓ separate dot products between the query and key vectors, we can reshape the key vectors to be of size |Q(ℓ)| ×degℓ ×d and the query is of shape |Q(ℓ)| ×d. Now the dot product of query and key mappings can be done using |Q(ℓ)|, degℓ ×d by d × 1 matrix multiplications. This same size matrix multiplication can be done using highly optimized batch matrix multiplication operations in e.g. PyTorch and Tensorflow (Paszke et al., 2019; Abadi et al., 2015). 4.1 Theoretical Underpinnings We first study the approximability of a network with a smaller hidden dimension or width. Formally, suppose that the width of a wide network is D. Then there exists a network with narrow dimensions for WQ and WK, of dimension O(log n ε2 ) × D instead of D × D, whose attention scores agree with those of the wide network up to O(ε) error (Theorem E.4). This reduction helps with the most intensive part of the calculation; others are linear with respect to the number of nodes n. While this is not the model we use in practice, Shirzad et al. (2024, Section 4) explore some scenarios common in graph Transformers that allow for the existence of “fully” narrow networks with accurate attention scores. They support these claims with experiments that show compressibility for some datasets we use. This is an existence claim; we will justify experimentally that in practice, training a narrow network does approximate attention scores well. We then study the sampling procedure of our sparsification method. Under certain assumptions, we show that sampling roughly O(n log n/ε2) entries of the attention matrix A (corresponding to sampling this many edges in the graph) suffices to form a matrix B with ∥A − B∥2 ≤ ε∥A∥2, if we can access the entries of A (Theorem E.5). We cannot actually access the matrix A, but we do have attention scores A′ from a narrow network. We show that if the entries ofA are not seriously under-estimated by A′, the same bound on the number of samples still holds (Proposition E.7). Table 1: Comparison of our model with other GNNs on six homophilic datasets. The reported metric is accuracy for all datasets. Model Computer Photo CS Physics WikiCS ogbn-arxiv GCN 89.65 ±0.52 92.70 ±0.20 92.92 ±0.12 96.18 ±0.07 77.47 ±0.85 71.74 ±0.29 GRAPHSAGE 91.20 ±0.29 94.59 ±0.14 93.91 ±0.13 96.49 ±0.06 74.77 ±0.95 71.49 ±0.27 GAT 90.78 ±0.13 93.87 ±0.11 93.61 ±0.14 96.17 ±0.08 76.91 ±0.82 72.01 ±0.20 GRAPHSAINT 90.22 ±0.15 91.72 ±0.13 94.41 ±0.09 96.43 ±0.05 - 68.50 ±0.23 NODEFORMER 86.98±0.62 93.46 ±0.35 95.64 ±0.22 96.45 ±0.28 74.73 ±0.94 59.90 ±0.42 GRAPHGPS 91.19 ±0.54 95.06 ±0.13 93.93 ±0.12 97.12 ±0.19 78.66 ±0.49 70.92 ±0.04 GOAT 90.96 ±0.90 92.96 ±1.48 94.21 ±0.38 96.24 ±0.24 77.00 ±0.77 72.41 ±0.40 EXPHORMER+GCN 91.59 ±0.31 95.27 ±0.42 95.77 ±0.15 97.16 ±0.13 78.54 ±0.49 72.44 ±0.28 EXPHORMER* 91.16 ±0.26 95.36 ±0.17 95.19 ±0.26 96.40 ±0.20 78.19 ±0.29 71.27 ±0.27 SPEXPHORMER 91.09±0.08 95.33 ±0.49 95.00 ±0.15 96.70 ±0.05 78.2 ±0.14 70.82 ±0.24 Avg. Edge Percent 7.6% 8.2% 12.8% 11.3% 8.6% 13.7% 5 Experimental Results 5.1 Attention Score Estimation To show how well the smaller network estimates the attention scores for a larger network, we conduct experiments on two smaller datasets, where we can reasonably train the full network at higher width for many runs in order to estimate the distribution of the attention scores. To this end, we use the Actor (Lim et al., 2021) and Photo (Shchur et al., 2018) datasets. We train the network for hidden dimensions h varying from 4 to 64 for both datasets. For each h we train the network 100 times. We consider the distribution of attention scores for each node, and estimate the energy distance (Székely and Rizzo, 2013; an instance of the maximum mean discrepancy, Sejdinovic et al., 2013) for that node’s attention scores across each pair of h sizes. 8uniformrandom 4 8 16 32 64 0.00 0.02 0.04 0.06 0.08 0.10 Actor Dataset without Expanders (a) uniformrandom 4 8 16 32 64 0.00 0.05 0.10 0.15 0.20 Actor Dataset with Expanders (b) uniformrandom 4 8 16 32 64 0.00 0.05 0.10 0.15 0.20 Amazon-Photo Dataset without Expanders (c) uniformrandom 4 8 16 32 64 0.0 0.1 0.2 0.3 Amazon-Photo Dataset with Expanders (d) Figure 3: Energy distance between the attention scores of various networks to a network of width 64. “Uniform” refers to the baseline placing equal scores to each neighbor, while “random” refers to the baseline with uniformly distributed logits. The remaining bars refer to networks trained on the appropriately labeled width. We ran this experiment in two scenarios: first, with just graph edges, and then, by adding expander and self-loop edges. It might be that the model, just by examining the category of the edges, may give a lower score to one type, making distributions seem more similar despite not identifying a small number of important neighbors as we want. However, in the presence of only one type of edge, the model can still consistently estimate which nodes should have a higher attention score. We compare attention scores from our model with the uniform distribution on the neighbors (each neighbor of node i has score 1 di ), and to a distribution with logits uniform over [−8, 8]. The choice of 8 here is because in the network we clip the logits with an absolute value higher than8. Figure 3 shows that even width-4 networks provide far superior estimates of attention scores than these baselines. In Appendix F, we extend our analysis with several experiments: examining pairwise energy distances between all pairs of hidden dimensions as well as uniform and random distributions, providing layer- wise results (Appendix F.2), analyzing the sharpness or smoothness of attention scores across layers (Appendix F.3), assessing their similarity between layers (Appendix F.4), and measuring precision, recall, density, and coverage in estimating the attention scores of the larger network using a smaller one (Appendix F.5). Additionally, we investigate the sum of top-k attention scores (Appendix F.6) and evaluate the role of different edge types in learning representations (Appendix F.7). Our key insights are as follows: Insight 1.Attention scores from a network with a smaller hidden dimension serve as a good estimator for the attention scores in a network with a higher hidden dimension. Insight 2.Attention scores are smoother in the first layer, and become sharper in subsequent layers. Insight 3.The attention scores in the layers after the first are consistently very similar to one another, but distinct from the attention scores in the first layer. Insight 4.The sum of the top-k attention scores is substantially lower than one for many nodes, even for relatively large k values such as 10. Table 2: Comparison of our model with other GNNs on five heterophilic datasets. The reported metric is ROC-AUC (×100) for the Minesweeper, Tolokers, and Questions datasets, and accuracy for all others. Model Actor Minesweeper Tolokers Roman-Empire Amazon-Ratings Questions GLOGNN 36.4 ±1.6 51.08 ±1.23 73.39 ±1.17 59.63 ±0.69 36.89 ±0.14 65.74 ±1.19 GCN 33.23 ±1.16 89.75 ±0.52 83.64 ±0.67 73.69 ±0.74 48.70 ±0.63 76.09 ±1.27 GRAPHGPS 37.1 ±1.5 90.63 ±0.67 83.71 ±0.48 82.00 ±0.61 53.10 ±0.42 71.73 ±1.47 NAGPHORMER - 84.19 ±0.66 78.32 ±0.95 74.34 ±0.77 51.26 ±0.72 68.17 ±1.53 NODEFORMER 36.9±1.0 86.71 ±0.88 78.10 ±1.03 64.49 ±0.73 43.86 ±0.35 74.27 ±1.46 GOAT - 81.09 ±1.02 83.11 ±1.04 71.59 ±1.25 44.61 ±0.50 75.76 ±1.66 EXPHORMER+GAT 38.68 ±0.38 90.74 ±0.53 83.77 ±0.78 89.03 ±0.37 53.51 ±0.46 73.94 ±1.06 EXPHORMER* 39.01 ±0.69 92.26 ±0.56 83.53 ±0.28 84.91 ±0.25 46.80 ±0.53 73.35 ±1.78 SPEXPHORMER 38.59±0.81 90.71 ±0.17 83.34 ±0.31 87.54 ±0.14 50.48 ±0.34 73.25 ±0.41 Avg. Edge Percent 5.8% 17.8% 8.9% 31.1% 15.3% 13.8% 9Actor Photo Minesweeper Tolokers CS ComputerPhysicsogbn-arxiv Dataset 0 5 10 15 20 25 30 35 40GigaBytes Attention Score Estimator Spexphormer Exphormer w Degree 6 Exphormer w Degree 30 Figure 4: Memory usage comparison: Attention Score Estimator network and Spexphormer vs. Exphormer with expander degrees 6 and 30. Exphormer with de- gree 30 for the ogbn-arxiv dataset could not fit into the memory of a 40GB GPU device, and thus the number here is a lower bound. Model ogbn-proteins Amazon2M Pokec * MLP 72.04 ±0.48 63.46±0.10 60.15±0.03GCN 72.51 ±0.35 83.90±0.10 62.31±1.13SGC 70.31 ±0.23 81.21±0.12 52.03±0.84GCN-NSAMPLER 73.51±1.31 83.84±0.42 63.75±0.77GAT-NSAMPLER 74.63±1.24 85.17±0.32 62.32±0.65SIGN 71.24 ±0.46 80.98±0.31 68.01±0.25NODEFORMER 77.45±1.15 87.85±0.24 70.32±0.45SGFORMER 79.53±0.38 89.09±0.10 73.76±0.24SPEXPHORMER 80.65±0.07 90.40±0.03 74.73±0.04 Memory Information for SPEXPHORMER Memory (MB) 2232 3262 2128Batch Size 256 1000 500Hidden Dimension 64 128 64Number of layers 2 2 2Number of Parameters 79,224 300,209 83,781 Table 3: Comparative results on large graph datasets, with ROC-AUC(×100) reported for the ogbn-proteins dataset and accuracy for all others. GPU memory usage, batch sizes, hidden dimensions used to obtain these numbers, and the total number of parameters have been added at the bottom of the table. 5.2 Model Quality We conduct experiments on twelve medium-sized graphs, including six homophilic datasets: CS, Physics, Photo, Computer (Shchur et al., 2018), WikiCS (Mernyei and Cangea, 2020), and ogbn-arxiv (Hu et al., 2021); and six heterophilic datasets: Minesweeper, Tolokers, Roman-empire, Amazon- ratings, Questions (Platonov et al., 2023), and Actor (Lim et al., 2021). For the CS, Physics, Photo, and Computer datasets, we use a random train/validation/test split of 60%/20%/20%. For WikiCS and ogbn-arxiv we follow the standard data split provided by the original source. For the Actor dataset, we use a 50%/25%/25% split following Wu et al. (2022). For the Minesweeper, Tolokers, Roman-empire, Amazon-ratings, and Questions datasets, we use the standard split from Platonov et al. (2023). Results for these experiments are provided in Tables 1 and 2. The EXPHORMER model presented in the tables refers to the attention mechanism of EXPHORMER without incorporating any MPNN components. Interestingly, the results on the Roman-Empire and Amazon-Ratings datasets revealed that removing certain edges led to better performance compared to simply adding an expander layout. In these medium-sized datasets, we are able to train the full Exphormer model. Our goal is to determine the extent of performance reduction when using two memory-efficient networks to estimate the original network. Results show that the two memory-efficient networks can efficiently estimate the original network, enabling us to scale the Exphormer to larger graph datasets. We compare the maximum required memory of the attention score estimator and final networks with that of the corresponding Exphormer model in Figure 4. We then experiment on large graph datasets: ogbn-proteins, Amazon2M (Hu et al., 2021), and Pokec (Takac and Zabovsky, 2012). The results provided in Table 3 demonstrate superior performance of our model despite limited memory constraints. We follow the standard data split for the ogbn-proteins dataset and follow Wu et al. (2024) for the dataset split on the Amazon2M and Pokec datasets, with 10%/10%/80% and 50%/25%/25% train/validation/test ratios. We emphasize that this split differs from the original dataset split used by many other works, making those numbers incomparable. In all our experiments, we train the smaller network once, and then for the second network, we always use the same initial network’s learned attention scores. Attention scores are collected from the network training step with the highest validation accuracy. We use a subset of the following models in each of our tables as baselines, depending on the type of the dataset and scalability level of the models, GCN (Kipf and Welling, 2016), GraphSAGE (Hamilton et al., 2017), GAT (Veliˇckovi´c et al., 2018), GraphSAINT (Zeng et al., 2020), Nodeformer (Wu et al., 2022), Difformer (Wu et al., 2023), SGFormer (Wu et al., 2024), GraphGPS (Rampášek et al., 2022), GOAT (Kong et al., 2023), GloGNN (Li et al., 2022), SGC (Wu et al., 2019), NAGphormer (Chen et al., 2022a), Exphormer (Shirzad et al., 2023), and SIGN (Frasca et al., 2020). We borrow most of the baseline numbers in the tables from Wu et al. (2024); Deng et al. (2024). 10Table 4: Ablation studies on two homophilic and two heterophilic datasets. Metrics: accuracy for Photo and Computer, ROC-AUC (×100) for Tolokers and Minesweeper. For the initial network, we report the result for the network used for training the Spexphormer and thus, there is no confidence interval for them. Model/Dataset Computer Photo Minesweeper Tolokers Initial Network 85.23 91.70 85.67 80.16 Spexphormer-uniform 86.65 ±0.46 94.21 ±0.22 84.15 ±0.22 82.56 ±0.17 Spexphormer-max 89.31 ±0.31 95.07 ±0.20 87.92 ±0.26 80.85 ±0.23 Spexphormer w.o. temp 89.05 ±0.35 95.30 ±0.16 90.02 ±0.02 83.34 ±0.13 Spexphormer w.o. layer norm 89.70±0.25 94.91 ±0.18 89.65 ±0.10 84.06±0.10 Spexphormer 91.09±0.08 95.33 ±0.49 90.71 ±0.17 83.34±0.13 5.3 Ablation Studies We benchmark the effect of different parts of the model in Table 4. Spexphormer-uniform, rather than sampling based on the estimated attention scores, samples uniformly from the augmented graph; this is always worse than attention-based sampling, but the gap is larger for some datasets than others. Spexphormer-max takes the edges with the highest attention scores, rather than sampling; this again performs somewhat worse across datasets. Spexphormer w.o. temp uses a constant temperature of 1 in the initial attention score estimator network; Spexphormer w.o. layer norm removes our added layer normalization. These changes are smaller, and in one case layer normalization makes the results worse. Across the four datasets, however, it seems that both temperature and layer norm help yield more informative and sparser attention scores. 6 Conclusion & Limitations We analyzed the alignment of the attention scores among models trained with different widths. We found that the smaller network’s attention score distributions usually align well with the larger network’s. We also theoretically analyzed the compressibility of the larger Graph Transformer models. Based on these observations, we used a sampling algorithm to sparsify the graph on each layer. As a result of these two steps, the model’s memory consumption reduces significantly, while achieving a competitive accuracy. This strategy also lets us use novel batching techniques that were not feasible with expander graphs of a large degree. Having a regular degree enables using dense matrix multiplication, which is far more efficient with current GPU and TPU devices. While our method successfully scales to datasets with over two million nodes, it relies on large CPU memory for the attention score estimation for these datasets. For extremely large datasets, this is still infeasible without highly distributed computation. Estimated attention scores can be shared and used for training various networks based on attention scores, however, so this only needs to only be computed once per dataset and depth. An area for potential future work is to combine sampling with simultaneous attention score estimation in a dynamic way, scaling this estimation to larger graphs. Acknowledgments and Disclosure of Funding This work was supported in part by the Natural Sciences and Engineering Resource Council of Canada, the Fonds de Recherche du Québec - Nature et technologies (under grant ALLRP-57708- 2022), the Canada CIFAR AI Chairs program, the BC DRI Group, Calcul Québec, Compute Ontario, and the Digital Resource Alliance of Canada. Honghao Lin was supported in part by a Simons Investigator Award, NSF CCF-2335412, and a CMU Paul and James Wang Sercomm Presidential Graduate Fellowship. References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Vasudevan, V ., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y ., and Zheng, X. (2015). 11TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org. Achlioptas, D., Karnin, Z. S., and Liberty, E. (2013). Near-optimal entrywise sampling for data matrices. Advances in Neural Information Processing Systems, 26. Chen, J., Gao, K., Li, G., and He, K. (2022a). Nagphormer: Neighborhood aggregation graph transformer for node classification in large graphs. CoRR, abs/2206.04910. Chen, Y ., Zeng, Q., Hakkani-Tur, D., Jin, D., Ji, H., and Yang, Y . (2022b). Sketching as a tool for understanding and accelerating self-attention for long sequences. In Carpuat, M., de Marneffe, M., and Ruíz, I. V . M., editors,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 5187–5199. Association for Computational Linguistics. Choromanski, K. M., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlós, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. (2021). Rethinking attention with performers. In ICLR. Cramér, H. (1928). On the composition of elementary errors: First paper: Mathematical deductions. Scandinavian Actuarial Journal, 1928(1):13–74. Deng, C., Yue, Z., and Zhang, Z. (2024). Polynormer: Polynomial-expressive graph transformer in linear time. arXiv preprint arXiv:2403.01232. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Di Giovanni, F., Giusti, L., Barbero, F., Luise, G., Lio, P., and Bronstein, M. M. (2023a). On over-squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865–7885. PMLR. Di Giovanni, F., Rusch, T. K., Bronstein, M. M., Deac, A., Lackenby, M., Mishra, S., and Veliˇckovi´c, P. (2023b). How does over-squashing affect the power of gnns? arXiv preprint arXiv:2306.03589. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Dwivedi, V . P. and Bresson, X. (2020). A generalization of transformer networks to graphs.CoRR, abs/2012.09699. Efraimidis, P. S. and Spirakis, P. G. (2006). Weighted random sampling with a reservoir. Information processing letters, 97(5):181–185. Finkelshtein, B., Ceylan, ˙I. ˙I., Bronstein, M., and Levie, R. (2024). Learning on large graphs using intersecting communities. arXiv preprint arXiv:2405.20724. Franks, B. J., Morris, C., Velingker, A., and Geerts, F. (2024). Weisfeiler-leman at the margin: When more expressivity matters. arXiv preprint arXiv:2402.07568. Frasca, F., Rossi, E., Eynard, D., Chamberlain, B., Bronstein, M., and Monti, F. (2020). Sign: Scalable inception graph neural networks. arXiv preprint arXiv:2004.11198. Hamilton, W., Ying, Z., and Leskovec, J. (2017). Inductive representation learning on large graphs. Advances in neural information processing systems, 30. Hu, W., Fey, M., Ren, H., Nakata, M., Dong, Y ., and Leskovec, J. (2021). OGB-LSC: A large-scale challenge for machine learning on graphs. CoRR, abs/2103.09430. Johnson, W. B. (1984). Extensions of lipshitz mapping into hilbert space. In Conference modern analysis and probability, 1984, pages 189–206. 12Kakade, S. and Shakhnarovich, G. (2009). Lecture notes in large scale learning. https://home. ttic.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf. Kipf, T. N. and Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907. Kong, K., Chen, J., Kirchenbauer, J., Ni, R., Bruss, C. B., and Goldstein, T. (2023). Goat: A global transformer on large-scale graphs. In International Conference on Machine Learning , pages 17375–17390. PMLR. Kreuzer, D., Beaini, D., Hamilton, W. L., Létourneau, V ., and Tossou, P. (2021). Rethinking graph transformers with spectral attention. arXiv preprint arXiv:2106.03893. Li, X., Zhu, R., Cheng, Y ., Shan, C., Luo, S., Li, D., and Qian, W. (2022). Finding global homophily in graph neural networks when meeting heterophily. In International Conference on Machine Learning, pages 13242–13256. PMLR. Lim, D., Hohne, F., Li, X., Huang, S. L., Gupta, V ., Bhalerao, O., and Lim, S. N. (2021). Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in Neural Information Processing Systems, 34:20887–20902. Liu, X., Yan, M., Deng, L., Li, G., Ye, X., and Fan, D. (2021). Sampling methods for efficient training of graph convolutional networks: A survey. IEEE/CAA Journal of Automatica Sinica, 9(2):205–234. Mernyei, P. and Cangea, C. (2020). Wiki-cs: A wikipedia-based benchmark for graph neural networks. arXiv preprint arXiv:2007.02901. Mialon, G., Chen, D., Selosse, M., and Mairal, J. (2021). Graphit: Encoding graph structure in transformers. CoRR, abs/2106.05667. Müller, L., Galkin, M., Morris, C., and Rampášek, L. (2023). Attending to graph transformers. arXiv preprint arXiv:2302.04181. Naeem, M. F., Oh, S. J., Uh, Y ., Choi, Y ., and Yoo, J. (2020). Reliable fidelity and diversity metrics for generative models. In International Conference on Machine Learning, pages 7176–7185. PMLR. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. (2020). Geom-gcn: Geometric graph convolutional networks. arXiv preprint arXiv:2002.05287. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. (2023). A critical look at the evaluation of GNNs under heterophily: Are we really making progress? arXiv preprint arXiv:2302.11640. Prabhu, A., Varma, G., and Namboodiri, A. M. (2018). Deep expander networks: Efficient deep networks from graph theory. In Ferrari, V ., Hebert, M., Sminchisescu, C., and Weiss, Y ., editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, volume 11217 of Lecture Notes in Computer Science, pages 20–36. Springer. Rampášek, L., Galkin, M., Dwivedi, V . P., Luu, A. T., Wolf, G., and Beaini, D. (2022). Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:14501–14515. Rizzo, M. L. and Székely, G. J. (2016). Energy distance. wiley interdisciplinary reviews: Computa- tional statistics, 8(1):27–38. Rusch, T. K., Bronstein, M. M., and Mishra, S. (2023). A survey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993. 13Sajjadi, M. S., Bachem, O., Lucic, M., Bousquet, O., and Gelly, S. (2018). Assessing generative models via precision and recall. Advances in neural information processing systems, 31. Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fukumizu, K. (2013). Equivalence of distance- based and RKHS-based statistics in hypothesis testing. The Annals of Statistics , 41(5):2263 – 2291. Shchur, O., Mumme, M., Bojchevski, A., and Günnemann, S. (2018). Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868. Shirzad, H., Lin, H., Venkatachalam, B., Velingker, A., Woodruff, D. P., and Sutherland, D. J. (2024). A theory for compressibility of graph transformers for transductive learning. In Machine Learning and Compression Workshop at NeurIPS. arXiv preprint arXiv:2411.13028. Shirzad, H., Velingker, A., Venkatachalam, B., Sutherland, D. J., and Sinop, A. K. (2023). Exphormer: Sparse transformers for graphs. In ICML. Székely, G. J. and Rizzo, M. L. (2013). Energy statistics: A class of statistics based on distances. Journal of Statistical Planning and Inference, 143(8):1249–1272. Takac, L. and Zabovsky, M. (2012). Data analysis in public social networks. InInternational scientific conference and international workshop present day trends of innovations, volume 1. Tay, Y ., Dehghani, M., Bahri, D., and Metzler, D. (2020). Efficient transformers: A survey.arXiv preprint arXiv:2009.06732. Topping, J., Di Giovanni, F., Chamberlain, B. P., Dong, X., and Bronstein, M. M. (2021). Understand- ing over-squashing and bottlenecks on graphs via curvature. arXiv preprint arXiv:2111.14522. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In NeurIPS, pages 5998–6008. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . (2018). Graph attention networks. In ICLR. Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K. (2019). Simplifying graph convolutional networks. In International conference on machine learning , pages 6861–6871. PMLR. Wu, Q., Yang, C., Zhao, W., He, Y ., Wipf, D., and Yan, J. (2023). Difformer: Scalable (graph) transformers induced by energy constrained diffusion. arXiv preprint arXiv:2301.09474. Wu, Q., Zhao, W., Li, Z., Wipf, D. P., and Yan, J. (2022). Nodeformer: A scalable graph structure learning transformer for node classification. NeurIPS, 35:27387–27401. Wu, Q., Zhao, W., Yang, C., Zhang, H., Nie, F., Jiang, H., Bian, Y ., and Yan, J. (2024). Simplifying and empowering transformers for large-graph representations. Advances in Neural Information Processing Systems, 36. Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y ., and Liu, T.-Y . (2021). Do transformers really perform bad for graph representation? ArXiv, abs/2106.05234. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. (2020). Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283–17297. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V . K. (2020). Graphsaint: Graph sampling based inductive learning method. In 8th International Conference on Learning Represen- tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Zhang, J., Zhang, H., Xia, C., and Sun, L. (2020). Graph-Bert: Only attention is needed for learning graph representations. arXiv preprint arXiv:2001.05140. Zhao, J., Li, C., Wen, Q., Wang, Y ., Liu, Y ., Sun, H., Xie, X., and Ye, Y . (2021). Gophormer: Ego-graph transformer for node classification. CoRR, abs/2110.13094. 14A Notation Table Table 5: A summary of the notation used in this paper. The hat notation always refers to a compressed network equivalent of a vector or matrix from the reference network. Notation Definition n The number of nodes in the graph m The number of attention edges in total, including graph and expander edges d Hidden dimension of a narrow network D Hidden dimension of the original large graph L The total number of layers in the network ℓ Arbitrary layer index V Value mapping of the vectors in the attention mechanism Q Query mapping of the vectors in the attention mechanism K Key mapping of the vectors in the attention mechanism W(ℓ) · Weight matrix of mapping such as key, query, value, edge features, or bias in layerℓ cW(ℓ) · Low dimensional network’s weight matrix for a mapping in layerℓ M· A linear mapping matrix (usually from the higher dimension to the smaller) ReLU Rectified Linear Unit H(ℓ) Output of layerℓ −1 from the reference network ¯H(ℓ) A low-rank estimation ofH(ℓ) bH(ℓ) Output of layerℓ −1 from a compressed network h(ℓ) i columni of matrixH(ℓ) a(ℓ) ij The Attention score between nodesi andj in layerℓ ˆa(ℓ) ij The attention score between nodesi andj in layerℓ from a smaller network B Dataset Descriptions Below, we provide descriptions of the datasets on which we conduct experiments. A summarized statistics of these datasets have been provided in Table 6. Amazon datasets Amazon Computers and Amazon Photo are Amazon co-purchase graphs. Nodes represent products purchased. An edge connects a pairs of products purchased together. Node features are bag-of-words encoded reviews of the products. Class labels are the product category. Amazon-Ratings The Amazon-ratings is an Amazon co-purchasing dataset. Each node represents a product and the edges are between the nodes purchased together frequently. Node features are the average of word embeddings from the product description. The task is to predict the average rating of the product. Amazon2M Amazon2M dataset is a graph from the co-purchasing network. Each node represents an item. Edges between items represents products purchased together. The node features are generated from the product description. The node labels are from the top-level categories the product belongs to. WikiCS WikiCS contains pages from Wikipedia. Each node represents an article from Wikipedia related to the Computer Science field. Edges represent the hyperlinks between the articles. The node features are the average of the word embeddings from the articles. The task is to classify the nodes into ten different branches of the field. Actor dataset The actor dataset is created by the actor-only subgraph of a larger graph of actor, director, writer, and film co-occuring on a Wikipedia page, limited to English-language films. Each node corresponds to an actor. Edges denote co-occurence on a Wikipedia page. Node features are based on the terms in the actor’s page. The prediction task is categorizing into one of five categories (Pei et al., 2020). 15Roman-Empire This dataset is a graph constructed from the “Roman Empire” article from Wikipedia. Each node is a word from this text. Two words are connected to each other if they follow each other in the text, or they are connected in the dependency tree of the sentence. The task is to predict the syntactic role of the word in the sentence. Graph is highly sparse and heterophilic. Coauthor datasets The datasets, CS and Physics are co-authorship graphs from Microsoft Aca- demic Graph. The nodes represent the authors and an edge connects two authors who share a paper. The node features are the keywords in the papers. The class represents the active area of study for the author. ogbn-arxiv (Hu et al., 2021) The ogbn-arxiv dataset is from OGBN datasets. The nodes represents the papers and edges represent the citations between the papers. Nodes are 128-dimensional feature vector that is an average of the embeddings of words in the title and abstract. The prediction task is to identify the category of the 40 subject areas. ogbn-proteins dataset The ogbn-proteins dataset is an undirected graph with edge weights and types based on species. The nodes represent proteins from eight different species. Edges indicate various biologically meaningful associations between the proteins (e.g., co-expression, homology etc.). The edges are eight-dimensional, with each dimension having a value from [0,1] indicates the confidence score. The prediction task is a multi-label binary classification among 112 labels — to predict the presence of protein functions. The performance measurement is the average of ROC-AUC scores across the 112 tasks. Minesweeper The dataset is a graph representation of the 100x100 grid from the Minesweeper game. A node represents a cell and the edges connect a node to its eight neighboring cells. 20% of the nodes are marked as mines. The features of the nodes are the one-hot encoding of the mines among the neighbors. For 50% of the nodes the features are unknown and indicated by a separate binary feature. Tolokers Tolokers is a graph representation of the workers in a crowd-sourcing platform, called Toloka. Each node represents a worker. Two nodes are connected if the workers have worked on the same task. Node features are based on the worker’s task performance statistics and other profile information. The task is to predict which nodes have been banned for a project. Questions This dataset is derived from the Yandex Q question-answering platform, focusing on interactions among users interested in the topic of medicine from September 2021 to August 2022. Nodes represent users, and edges denote answers given to another user’s questions. Node features include fastText-based embeddings of user descriptions, supplemented by a binary indicator for missing descriptions. The task is to predict user activity status at the end of the period. Pokec Pokec is a large-scale social network dataset. Nodes represents users of the network. Nodes features include profile data like geographical region, age etc. The task is to predict the gender of users based on the graph. C More Experiments C.1 Time-Memory Trade-off One advantage of our method is its ability to trade time for memory without sacrificing accuracy. Figure 5 illustrates this trade-off on two datasets: ogbn-proteins and arxiv. In these experiments, all hyperparameters are kept constant, with the only variation being the batch size. The results demonstrate that memory usage and runtime can be adjusted without introducing bias into the training process. It is important to note that in random subset batching, the average degree of nodes and the number of edges included in the training process are closely related to the batch size. A very small batch size relative to the graph size can randomly exclude a significant portion of the graph’s edges during training, potentially ignoring critical edges without considering their importance. 16Table 6: Dataset statistics. The reported number of edges is the number of directed edges, which will be twice the number of actual edges for the undirected graphs. Dataset Nodes Edges Average Degree Node Features Classes Metric Amazon Photo 7,487 238,162 31.13 745 8 Accuracy Coauthor Physics 34,493 495,924 14.38 8,415 5 Accuracy Amazon Computer 13,381 491,722 35.76 767 10 Accuracy Coauthor CS 18,333 163,788 8.93 6,805 15 Accuracy WikiCS 11,701 431,726 36.90 300 10 Accuracy ogbn-arxiv 169,343 2,332,486 13.77 128 40 Accuracy Actor 7,600 33,391 4.39 932 5 Accuracy Minesweeper 10,000 78,804 7.88 7 2 AUC Tolokers 11,758 1,038,000 88.28 10 10 AUC Roman-Empire 22,662 65,854 2.91 300 18 Accuracy Amazon-Ratings 24,492 186,100 7.60 300 5 Accuracy Questions 48,921 307,080 6.28 301 2 AUC Pokec 1,632,803 30,622,564 18.75 65 2 AUC ogbn-proteins 132,534 79,122,504 597.00 8 112 AUC Amazon2M 2,449,029 123,718,280 50.52 100 47 AUC 256 512 1000 2048 4096 Batch Size 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Memory (GB) 60 70 80 90 100 110 Time per Epoch (s) ogbn-proteins Dataset (a) 128 256 512 1000 2048 4096 8192 Batch Size 1.5 2.0 2.5 3.0 3.5 4.0Memory (GB) 15 20 25 30 35 Time per Epoch (s) ogbn-arxiv Dataset (b) Figure 5: The memory and runtime trade-off for the ogbn-proteins and ogbn-arxiv datasets. The plot demon- strates that memory and time can be effectively exchanged in our approach. The reported runtime includes the whole process of preprocessing the batches, train, and validation on validation and test sets. All experiments were conducted on a V100 GPU with 32GB of memory. C.2 Neighborhood Expansion The level of neighborhood expansion significantly impacts the efficiency of our model. As described in Algorithm 2, neighborhood expansion begins from the final nodes for which we require representations in the final layer and proceeds backward through the layers, sampling neighbors based on attention scores at each layer. We analyze the number of nodes across four datasets: Amazon-Photo, Coauthor-CS, Minesweeper, and Tolokers, to observe how the number of nodes increases as we trace back through the layers. This experiment is conducted with varying sampling degrees per layer, and the results are summarized in Figure 6. In all experiments, we assume that representations are needed for 10 final nodes. We sample 100 times of these 10 random seed nodes and plot average and standard deviations of the neighborhood node counts. The process has an upper bound, which is the total number of nodes in the graph. As the number of sampled nodes approaches this limit, the likelihood of encountering new nodes decreases. We compare these results with full-neighborhood sampling methods, as employed in k-hop neighborhood-induced subgraph techniques, and demonstrate that in the presence of expander graphs, this neighborhood can rapidly encompass the entire graph. The impact of limited neighborhood sampling becomes even more pronounced on extremely large graphs. 174 3 2 1 0 Layer Number 0 2000 4000 6000 8000Number of Nodes Dataset: Amazon-Photo 4 3 2 1 0 Layer Number 0 5000 10000 15000 20000 25000 30000 35000Number of Nodes Dataset: Coauthor-Physics 4 3 2 1 0 Layer Number 0 2000 4000 6000 8000 10000Number of Nodes Dataset: Minesweeper 4 3 2 1 0 Layer Number 0 2000 4000 6000 8000 10000 12000Number of Nodes Dataset: Tolokers degree = 1 degree = 3 degree = 5 degree = 10 Full Neighborhood Figure 6: The neighborhood expansion of the graph is analyzed to determine the number of nodes required in each layer to obtain representations for 10 nodes in the final layer. This is compared between Spexphormer’s degree-based sampling and full-neighborhood selection. The shadowed regions in the plot represent the 95% confidence intervals, calculated from 100 iterations of sampling the ten final nodes. C.3 Memory and Runtime with Graph Size Inspired by Finkelshtein et al. (2024), we compare the memory and runtime of our method to a Graph Convolutional Network (GCN) during a forward pass. In many real-world scenarios, the average degree of nodes is not constant and tends to scale with the graph size. One advantage of our method is its ability to subsample neighborhoods for each node, identifying a small yet representative set of neighbors. While GCN is a more computationally efficient network, we demonstrate that, even with a small but superlinear growth in neighborhood size, the memory and runtime requirements of GCN can surpass those of our method, which employs a sparse but regular self-attention layer with a fixed neighborhood size. In these experiments, we evaluate different growth factors for the GCN and varying neighborhood sampling sizes for our sparse self-attention method. For these comparisons, no batching is used; the entire process operates on the whole graph. Both models consist of a single layer of the corresponding network followed by a linear layer that maps the values to dimension 1. We use a hidden dimension of 128. In this setup, the GCN has approximately 16K parameters, while the self-attention layer in our method has about 65K parameters. We vary the number of nodes from10K to 50K, using an Erd˝os-Rényi distribution with specified probabilities p, denoted as ER(n, p). Here, n represents the number of nodes, and p is the probability that any pair of nodes is independently connected. In the GCN model input, p varies with n and can also be viewed as a function of n. The average degree of a node in this model is pn. For the Spexphormer model, we sample d-regular graphs as input. Node features are drawn from N(0, I128), where I128 is the 128-dimensional identity matrix. 18The results, shown in Figure 7, indicate that except for a constant average degree in the GCN model, the memory and runtime growth rates are higher for the GCN under all other configurations. For sufficiently large and dense graphs, our method proves to be significantly more efficient in both memory and runtime. 10 15 20 25 30 35 40 45 50 Number of Nodes (×1000) 0 1 2 3 4 5Memory Usage (GB) Spexphormer, d=5 Spexphormer, d=10 Spexphormer, d=20 GCN, p = 20 n GCN, p = 4log n n GCN, p = 1 4 n GCN, p = 0.002 (a) 10 15 20 25 30 35 40 45 50 Number of Nodes (×1000) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Runtime (ms) Spexphormer, d=5 Spexphormer, d=10 Spexphormer, d=20 GCN, p = 20 n GCN, p = 4log n n GCN, p = 1 4 n GCN, p = 0.002 (b) Figure 7: The memory and runtime comparison between our model and the GCN demonstrates that our model, with sparsification, significantly outperforms even a very simple GCN model on a forward pass. C.4 Accuracy, Memory, and Runtime with Sampling Degree For four datasets—Tolokers, Minesweeper, Amazon-Photo, and Coauthor-CS—we analyze how accuracy, memory usage, and runtime change as the sampling degree is varied. In this experiment, all hyperparameters are fixed except for the sampling degree degℓ, which is kept consistent across all layers to simplify the analysis. The results are shown in Figure 8, where we plot both the Accuracy/AUC results and the memory/runtime metrics. For more heterophilic datasets, larger neighborhood sampling generally improves performance; however, the improvement becomes marginal beyond a certain point, while memory usage and runtime continue to increase linearly. For homophilic datasets, a very small neighborhood size is sufficient, and increasing the neighborhood size further does not provide noticeable benefits. D Experiment Details D.1 Hyperparameters In our networks, we use a higher expander degree than what was used in the EXPHORMER paper. Since many of these edges will get a small attention score, a higher attention score increases the receptive field of the nodes, letting the final network be able to sample from wider options and have better access to long-range dependencies. We also noticed, the attention scores in the first layer are usually more flat than the other layers and so we usually sample more edges from the first attention layer. For the comparisons both on the results and the memory we have given the same expander degree to the Exphormer and the ogbn-arxiv dataset could barely fit into a 40GB GPU memory device with higher expander degree. For the attention score estimator network, we do not use dropout, and we only use one attention head in these networks. The number of layers is always equal between both networks. We use AdamW optimization algorithm in all our networks and use a cosine learning rate scheduler with it. We use weight decay of 1e −3 in all networks. We use layer norm in attention score estimator networks to keep attention scores more meaningful, but use a batch norm for better results in the final SPEXPHORMER model. Other key hyperparameters can be found in Tables 7 to 9. 191 3 5 7 10 15 Sampled Graph Node Degree 76 77 78 79 80 81 82 83 84AUC (a) Tolokers AUC 1 3 5 7 10 15 Sampled Graph Node Degree 0 100 200 300 400 500 600 700Memory Usage (MB) (b) Tolokers memory 1 3 5 7 10 15 Sampled Graph Node Degree 0.00 0.05 0.10 0.15 0.20Runtime(s) (c) Tolokers runtime 1 3 5 7 10 15 Sampled Graph Node Degree 80 82 84 86 88 90 92AUC (d) Minesweeper AUC 1 3 5 7 10 15 Sampled Graph Node Degree 0 100 200 300 400 500 600Memory Usage (MB) (e) Minesweeper memory 1 3 5 7 10 15 Sampled Graph Node Degree 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200Runtime(s) (f) Minesweeper runtime 1 3 5 7 10 15 Sampled Graph Node Degree 93.5 94.0 94.5 95.0 95.5 96.0 96.5Accuracy (g) Photo accuracy 1 3 5 7 10 15 Sampled Graph Node Degree 0 100 200 300 400 500 600 700 800Memory Usage (MB) (h) Photo memory 1 3 5 7 10 15 Sampled Graph Node Degree 0.00 0.05 0.10 0.15 0.20 0.25 0.30Runtime(s) (i) Photo runtime 1 3 5 7 10 15 Sampled Graph Node Degree 94.2 94.4 94.6 94.8 95.0 95.2Accuracy (j) CS accuracy 1 3 5 7 10 15 Sampled Graph Node Degree 0 500 1000 1500 2000Memory Usage (MB) (k) CS memory 1 3 5 7 10 15 Sampled Graph Node Degree 0.0 0.2 0.4 0.6 0.8Runtime(s) (l) CS runtime Figure 8: AUC and accuracy results, along with memory and runtime analysis, are presented for four datasets: two homophilic datasets (Amazon-Photo and Coauthor-CS) and two heterophilic datasets (Tolokers and Minesweeper). Larger sampling degrees generally lead to better results; however, for the homophilic datasets, even a very small neighborhood size can yield substantial performance. Increasing the sampling degree increases memory and runtime requirements accordingly. D.2 Hardware For all trainings of the medium-sized graph datasets and the final network training of the large-sized graphs, we used GPUs of type A100 with 40GB memory, and V100, both 32GB and 16GB versions. While these are powerful GPUs, we have always monitored the GPU memory usage for computational efficiency, ensuring that no more than 8GB is used for whole graph training and no more than 4GB of GPU memory is used with batching. Training with even less memory is feasible with smaller batch sizes. 20Table 7: Hyperparameters used for training the networks for homophilous datasets. Hyperparameter OGBN-Arxiv Computer Photo CS Physics WikiCS Attention Score Estimator L 3 4 4 4 4 4 ds 8 4 4 4 4 4 Num Epochs 200 200 200 200 200 100 Learning Rate 0.01 0.1 0.001 0.002 0.001 0.1 Final Spexphormer Network dl 96 80 56 64 64 64 degℓ [6, 6, 6] [5, 5, 5, 5] [5, 5, 5, 5] [5, 5, 5, 5] [5, 5, 5, 5] [8, 5, 5, 5] Number of Heads 2 2 2 2 2 2 Learning Rate 0.01 0.001 0.01 0.002 0.001 0.001 Num Epochs 600 150 100 120 80 100 Dropout 0.3 0.5 0.5 0.4 0.4 0.5 Table 8: Hyperparameters used for training the networks for heterophilic datasets. HyperparameterActor Minesweeper Tolokers Roman-Empire Amazon-ratings Questions Attention Score Estimator L 3 4 4 4 4 4 ds 4 4 4 4 4 4 Num Epochs 100 100 200 200 100 100 Learning rate 0.01 0.01 0.01 0.01 0.01 0.01 Final Spexphormer Network dl 32 32 32 40 64 32 degℓ [2, 2, 2] [12,5,5,5] [12, 10, 10, 10] [12, 10, 10, 10] [8, 5, 5, 5] [5, 5, 5, 5] Number of Heads 4 4 4 2 2 2 Learning Rate 0.01 0.01 0.01 0.03 0.01 0.01 Num Epochs 100 80 200 200 200 80 Dropout 0.5 0.2 0.25 0.1 0.1 0.5 For calculating the attention scores on the large graph datasets, we have used CPU devices Intel Xeon E5-2680 v4, with 500GB of memory. Except for the Amazon2M dataset, for the other datasets 200GB of memory would be sufficient. E Theory In this section, we theoretically analyze the compressibility of the Graph Transformer architecture and also sparsification guarantees using the attention score estimator network. For simplification, we use the following formulation of a single head Transformer network: h(ℓ+1/2) i = degiX j=1 a(l) ij V(ℓ) j , h(ℓ+1) i = W(ℓ) 2 \u0010 σ \u0010 W(ℓ) 1 \u0010 h(ℓ+1/2) i \u0011\u0011\u0011 , a(l) ij = exp \u0010 K(ℓ) j · Q(ℓ) i \u0011 P u∈NH(i) exp \u0010 K(ℓ) u · Q(ℓ) i \u0011, where, V(ℓ) = W(ℓ) V h(ℓ), Q(ℓ) = W(ℓ) Q h(ℓ), K(ℓ) = W(ℓ) K h(ℓ), and σ can be any 1-Lipchitz activation function, such as ReLU, which has been used in practice in our networks. We re- move the normalization parts from the architecture but assume that in all steps for all vectors, ∥Xi∥2, ∥h(ℓ+1/2) i ∥2, ∥h(ℓ) i ∥2 ≤ √α, and all linear mapping W· matrices’ operator norm is bounded by a constant β. The first assumption is realistic because of the layer-norm applied between the layers in real-world architectures. The second assumption is also justified as the operator norms are near 2 21Table 9: Hyperparameters used for training the networks for the large graphs datasets. Hyperparameter ogbn-proteins Amazon2M Pokec Attention Score Estimator L 2 2 2 ds 8 8 8 expander degree 200 30 30 Num Epochs 150 150 150 Learning rate 0.01 0.01 0.01 Final Spexphormer Network dl 64 128 64 degℓ [50, 30] [10,10] [20, 20] Number of Heads 1 1 1 Learning Rate 0.005 0.001 0.01 Num Epochs 200 200 300 Dropout 0.1 0.2 0.2 Batch size 256 1000 500 GPU Memory 2232MB 3262MB 2128MB in the initialization of the network by the default PyTorch initialization and during the optimization we expect the operator norm to not increase drastically from the initialization. Also, we assume h(0) = X, which is the input features. For a simpler notation, we will use D for a hypothetical large network hidden dimension in this analysis, and d is the hidden dimension of the narrow network. For simplicity, in our analysis, we assume X ∈ Rn×D. In case each node has less than D features, we can concatenate them with zeros. E.1 On the Compressibility of the Graph Transformer Our approach uses a narrow network to estimate the attention scores. We want to show if we have a large network with good accuracy for a task on a graph, we can have a less complex network that can work on the same input graph and the error of this network is bounded by O(ε) from the large network. The most memory/time-intensive part of a Transformer architecture is its attention score calculation part. The rest of the sections are node/token-wise and linear with respect to the number of nodes. The attention score estimation part of a full-Transformer layer requires O(n2d) operations and O(md) operators are required for a sparse Transformer with m attention edges. In the main Exphormer network, this would also be more intensive as the edge features mappings requireO(md2) operations, but since we replace edge feature mappings with edge embeddings by their type, this part in case we do not have other edge features is O(md), but m still can be ω(n), and it will be the most computationally-intensive part. Assume we have a large network with L layers, where L is O(1), and hidden dimension D, we will show that there is a similar network with L layers where the attention score calculation matrices WQ, WK ∈ RD×d, and all other matrices are of the same size and d is O(CL log n ϵ2 ), where C is a constant based on α and β. For this proof we use the distributional Johnson-Lindenstrauss transform lemma (Johnson, 1984): Lemma E.1 (Johnson-Lindenstrauss Transform Lemma ( JLT)). Assume 0 < ϵ, δ <1 2 and any positive integer D, if d = O(log(1/δ) ϵ2 ), there exist a distribution over matrices M ∈ Rd×D that for any x ∈ RD and ∥x∥ = 1: Pr(∥Mx∥ −1 > ϵ) < δ The following corollary is an immediate conclusion from the JLT. Corollary E.2. Assume 0 < ϵ, δ <1 2 and any positive integer D, if d = O(log(1/δ) ε2 ), there exist a distribution over matrices M ∈ Rd×D that for any x, y∈ RD: 22Pr((1 − ε)∥x − y∥ < ∥Mx − My∥ < (1 + ε)∥x − y∥) < δ This can derived by replacing x from JLT with x−y ∥x−y∥. From this, we can derive another corollary about the dot product of the vectors in low-dimensional space. Corollary E.3 (JLT-dot product). Assume 0 < ϵ, δ <1 2 and any positive integer D, if d = O(log(1/δ) ε2 ), there exist a distribution over matrices M ∈ Rd×D that for any x, y∈ RD, and ∥x∥, ∥y∥ ≤√α: Pr((1 − εα)xTy < xTMTMy <(1 + εα)xTy) < δ For the proof see (Kakade and Shakhnarovich, 2009, Corollary 2.1). As a result of this corollary, if we have m pairs of vectors (xi, yi), and for each i ∥xi∥2, ∥yi∥2 ≤ √α of √α, and d = O(log(m) ε2 ), there exists an M such that for all these pairs |xT i MTMyi − xT i yi| < εα. The proof can be done using a union bound over the error from Corollary E.3. Also, in our case where m is the number of edges, we know that m ≤ n2, thus we can also say d = O(log(n) ε2 ). Theorem E.4. Assume we have a Transformer network T with arbitrary large hidden dimension D, L = O(1) layers, and in this network, in all layers, we have ∥h·∥2 ≤ √α, and ∥W·∥op ≤ β. There exists a Transformer bT , that for any layer WQ and WK are in Rd×D for a d = O(log n ε2 ), with a sufficiently small ε, and for all i ∈ [n], ∥T (X)i − bT (X)i∥2 = O(ε). And furthermore, for any attention score a(ℓ) ij ba(ℓ) ij = 1 + O(ε). Proof. In the proof we use hat notation, b□, for the vectors and matrices from bT , for example, ˆh(ℓ) are the outputs of layer ℓ, and cW· are the weight matrices for this network. In all layers for both networks WV , W1, and W2, are of the same size, so we set cWV = WV , cW1 = W1, and cW2 = W2. For the proof, we want to findε(0), ··· , ε(L) in a way that for anyv in layer ℓ, |h(ℓ) v −ˆh(ℓ) v | < ε(ℓ). We will find these bounds inductively, starting from the first layer. We haveε(0) = 0, as both networks have the same input, and we want to bound ε(ℓ+1) based on ε(ℓ). We have Q(ℓ) = W(ℓ) Q H(ℓ), K(ℓ) = W(ℓ) K H(ℓ) and assume ¯Q(ℓ) = W(ℓ) Q bH(ℓ), ¯K(ℓ) = W(ℓ) K bH(ℓ). Because of the operator norm of matricesWQ and WK, for each i we have ∥q(ℓ) i −¯q(ℓ) i ∥ ≤ε(ℓ)β and ∥k(ℓ) i − ¯k(ℓ) i ∥ ≤ε(ℓ)β. Also, we have ∥q(ℓ) i ∥, ∥k(ℓ) i ∥ ≤β√α, thus ∥¯q(ℓ) i ∥, ∥¯k(ℓ) i ∥ ≤β(ε(ℓ) + √α). Now, for each pair of i and j, we have: |q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · ¯k(ℓ) j | = |q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · k(ℓ) j + ¯q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · ¯k(ℓ) j | ≤ |q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · k(ℓ) j | + |¯q(ℓ) i · k(ℓ) j − ¯q(ℓ) i · ¯k(ℓ) j | = |(q(ℓ) i − ¯q(ℓ) i ) · k(ℓ) j | + |¯q(ℓ) i · (k(ℓ) j − ¯k(ℓ) j )| ≤ ∥q(ℓ) i − ¯q(ℓ) i ∥∥k(ℓ) j ∥ + ∥¯q(ℓ) i ∥∥k(ℓ) j − ¯k(ℓ) j ∥ ≤ √αβε(ℓ) + (√α + βε(ℓ))βε(ℓ) = 2√αβε(ℓ) + (βε(ℓ))2 On the other hand, according to the E.3, for a 0 < ε < 1/2 and d = O(log(n) ε2 ) there exists a matrix MQK ∈ Rd×D, such that if we define bQ(ℓ) = MQK ¯Q(ℓ) and bK(ℓ) = MQK ¯K(ℓ), |¯q(ℓ) i · ¯k(ℓ) j − ˆq(ℓ) i · ˆk(ℓ) j | < β2(α + (ε(ℓ))2 + 2√αε(ℓ))ε for all (i, j) pairs in the attention pattern. Note that we can define cW(ℓ) Q = M(ℓ) QKW(ℓ) Q , and cW(ℓ) K = M(ℓ) QKW(ℓ) K , both in Rd×D, as weights 23for the narrow attention score estimator network. With a triangle inequality we have |q(ℓ) i · k(ℓ) i − ˆq(ℓ) i · ˆk(ℓ) i | < β2(α + (ε(ℓ))2 + 2√αε(ℓ))ε + 2√αβε(ℓ) + (βε(ℓ))2. By setting ε(ℓ) ≤ 1, we have |q(ℓ) i · k(ℓ) i − ˆq(ℓ) i · ˆk(ℓ) i | < β2(α + 1 + 2√α)ε + β(2√α + β)ε(ℓ). Let us define εa = β2(α + 1 + 2√α)ε + β(2√α + β)ε(ℓ), we have: ba(ℓ) ij = exp(ˆq(ℓ) i · ˆk(ℓ) j ) P u∈NH(i) exp(ˆq(ℓ) i · ˆk(ℓ) u ) ≤ exp(q(ℓ) i · k(ℓ) j + εa) P u∈NH(i) exp(q(ℓ) i · k(ℓ) j − εa) ≤ a(ℓ) ij exp(2εa) ba(ℓ) ij = exp(ˆq(ℓ) i · ˆk(ℓ) j ) P u∈NH(i) exp(ˆq(ℓ) i · ˆk(ℓ) u ) ≥ exp(q(ℓ) i · k(ℓ) j − εa) P u∈NH(i) exp(q(ℓ) i · k(ℓ) u + εa) ≥ a(ℓ) ij exp(−2εa) Now we bound ∥h(ℓ+1/2) i − ˆh(ℓ+1/2) i ∥: ∥h(ℓ+1/2) i − ˆh(ℓ+1/2) i ∥ = ∥ X j∈Nei(i) a(ℓ) ij v(ℓ) j − baij ˆv(ℓ+1/2) j ∥ = ∥ X j∈Nei(i) a(ℓ) ij v(ℓ) j − ba(ℓ) ij v(ℓ) j + ba(ℓ) ij v(ℓ) j − baij ˆv(ℓ) j ∥ = ∥ X j∈Nei(i) (a(ℓ) ij − ba(ℓ) ij )v(ℓ) j + ba(ℓ) ij (v(ℓ) j − ˆv(ℓ) j )∥ = ∥(v(ℓ) j − ˆv(ℓ) j ) + v(ℓ) j X j∈Nei(i) (a(ℓ) ij − ba(ℓ) ij )∥ ≤ ∥v(ℓ) j − ˆv(ℓ) j ∥ + ∥v(ℓ) j ∥ X |a(ℓ) ij − ba(ℓ) ij | ≤ ε(ℓ)β + √α X max(1 − exp(−2εa), exp(2εa) − 1)a(ℓ) ij ≤ ε(ℓ)β + √α(exp(2εa) − 1), and since 1 + x <exp(x) < 1 + 2x for 0 < x <1, if we have εa < 1, we have ∥h(ℓ+1/2) i − ˆh(ℓ+1/2) i ∥ ≤βε(ℓ) + 4√αεa (1) For the feed-forward network part, we know that this network is β2-Lipschitz because W(ℓ) 1 and W(ℓ) 2 have maximum operator norm β and σ is a 1-Lipschitz activation function. Thus we have ∥h(ℓ+1) i − ˆh(ℓ+1) i ∥ ≤β2(βε(ℓ) + 4√αεa) = (β3 + 8βα + 4β2√α)ε(ℓ) + 4β2(α√α + 2α + √α)ε. Both β3 + 8βα + 4β2√α and 4β2(α√α + 2α + √α) are constants, and if we define them as c1 and c2, we have ε(ℓ+1) ≤ c1ε(ℓ) + c2ε Given ε(0) = 0, as both networks get the same input, we have ε(L) ≤ c1ε(L−1) + c2ε ≤ c1(c1ε(L−2) + c2ε) + c2ε ··· ≤ c2ε(cL−1 1 + ··· + c1) = c1(cL 2 − 1) c2 − 1 ε 24While the error increases exponentially with the number of layers, when we have L = O(1), then the error is bounded by a constant factor of chosen ε. Now, we know that ∥T (X)i − bT (X)i∥2 ≤ ε(L) = O(ε). While from the theorem it seems that the error is increasing exponentially by the layers, in practice the maximum number of layers used in this work is four with most large graph experiments using just two layers. Thus the constant factor will not be as large as it might look. Also, in real-world graphs usually, the columns of X are not quite n distinct vectors and many vectors would be equal or very similar to each other if we have κ unique vectors in the first layer the complexity for the d can be reduced to O(log κ ε2 ). In the homophily graphs the representations h(ℓ) tend to converge to each other and thus again the number of unique vectors will be reduced letting us have smaller d, but these assumptions are not considered in the proof as we keep it general. Although we have proved the existence of the bT , this does not mean that training with a gradient- based algorithm will necessarily lead to the introduced weights, but this gives at least the guarantee that such a network exists. However, on the other hand, it is also possible that the training process finds a set of weights that work better than the weights constructed in this proof. Theorem E.4, by narrowing the attention score calculation part, reduced the complexity fromO(mD+ nD2) to O(md + nD2), and for dense graphs or in scenarios we add denser expander graphs, where m ≫ n, already the introduced network has a much lower complexity. However, our narrow network uses narrow hidden dimensions in all steps and has complexity O(md + nd2). Proving the same guarantee along the whole network is not easy, if not impossible, without any further assumptions on X and the large network. Shirzad et al. (2024) explores these settings further, in the presence of various additional assumptions. E.2 Analysis of the Sampling Process After training a network with a smaller width d, we sample the edges from the original graph and use them in the second-phase training with a large hidden width D. In this section, we shall analyze our sampling process. Formally, we model our process as follows. Suppose that A is the attention score matrix with hidden width D, then we sample and rescale s entries of A to form a sparse matrix B where the goal is the matrix B can approximate A well, i.e., ∥A − B∥2 ≤ ε∥A∥2. However, recall that we can not access the entries of A precisely. Instead, we consider another attention score matrix A′, which corresponds to hidden width d. The first question is how many samples we indeed need to form the matrix B that approximates A well? To answer this, we have the following lemma for the attention score matrix A. Theorem E.5. Suppose that an n × n matrix A satisfies the following conditions: 1. For each i, we have ∥A(i)∥1 = 1. 2. maxj∥A(j)∥1 = K 3. Each column A(j) is ℓ-sparse. Then, consider the sampling procedure that samples s ≥ s0 = O(nK log n/(ε2∥A∥2 2)) = O(nℓ log n/(ε2K)) entries of A with replacement: 1. For each sampleBt, the probability thatBt samples entry Aij is pij = 1 n · |Aij| ∥A(i)∥1 = 1 n |Aij| (with a rescale factor 1/pij, i.e., Bt[i, j] = Aij/pij), and each Bt only samples one entry of A. 2. Form the matrix B = (B1 + B2 + ··· + Bs)/s. Then, we have that with probability at least 9/10, ∥A − B∥2 ≤ ε∥A∥2. To prove this lemma, we need the following matrix Bernstein inequality. 25Lemma E.6 (Matrix Bernstein inequality). Consider a finite sequence Xi of i.i.d. random m × n matrices, with E[Xi] = 0 and Pr(∥Xi∥2 ≤ R) = 1. Let σ2 = max{∥E[XiXT i ]∥2, ∥E[XT i Xi]∥2}. For some fixed s ≥ 1, let X = (X1 + X2 + ··· + Xs)/s, then we have that Pr[∥X∥2 ≥ ε] ≤ (m + n) · exp \u0012 sε2 −σ2 + Rε/3 \u0013 . Proof. We follow a similar proof strategy to that of Achlioptas et al. (2013). At a high level, the work of Achlioptas et al. (2013) considers the matrix Bernstein inequality, whose tail bound is dependent on the following two quantities: σ2 = max{∥E[(A − B1)(A − B1)T ]∥, ∥E[(A − B1)T (A − B1)]∥} and R = max∥A − B1∥ over all possible realizations of B1. Here B1 is the matrix that only samples one entry, and the final output isB = (B1 +B2 +··· +Bs)/s. Instead, we consider the following quantities, ˜σ2 = max   max i X j A2 ij/pij, max j X i A2 ij/pij    ˜R = max ij |Aij|/pij. It is shown in Lemma A.2 of Achlioptas et al. (2013) that |σ/˜σ − 1| ≤ ∥A∥2 2P i ∥A(i)∥2 1 and |R/ ˜R − 1| ≤ ∥A∥2 ∥A∥1 . From our condition on the matrix A, both of the upper bounds are at most 1. Hence, we only need to consider ˜σ and ˜R. Back to our case, we have that pij = 1 n · |Aij| ∥A(1)∥1 = 1 n · |Aij|, from this and the assumption of A we have ˜σ2 = n · max   max i X j |Aij|, max j X i |Aij|    ≤ n · K ˜R = max ij |Aij|/pij = n. Hence, to make δ ≤ 0.1, we only need to set ε′ = ε∥A∥2 in the Matrix Bernstein inequality and then we have s ≥ O(nK log n/(ε2∥A∥2 2)). Finally, note that if ∥A(j)∥1 = K, then we have ∥A∥2 ≥ ∥Aej∥2 = ∥A(j)∥2 ≥ K/ √ ℓ, which means that nK log n/(ε2∥A∥2 2) ≤ nℓ log n/(ε2K). However, as mentioned, we can not access the value of the entries ofA but the entries of A′ (which corresponds to the trained network with a small hidden width d). We next show that even in the case where we sample the entries of A from A′, we can still get the same order of the bound if the entries of A are not under-estimated seriously in A′. Proposition E.7. Suppose that the matrices A and A′ satisfy the condition in Theorem E.5 and for every i, jwe have |A′ ij| ≥1 α|Aij| for some sufficiently large constant α. Then consider the same sampling procedure in Theorem E.5 but sampling the entries of A from the value of A′. Then, the guarantee in Theorem E.5 still holds. Proof. We only need to note that from the assumption, the actual sampling probability p′ ij ≥ 1 α · pij in Theorem E.5, hence it will increase the ˜σ2 and ˜R by at most α times, which means that we can increase s by an α factor to make the error probability at most 0.1. 26F Attention Score Analysis In Figure 3, we observed that the attention scores are relatively close to the reference attention scores. In this section, we provide further details on these experiments and offer additional analysis of the attention scores. For our experiments, we used an implementation of the Exphormer model with normalization on V mappings and temperature adjustment for the attention scores. For each random seed, we selected the model with the best result on the validation set. We used an expander degree of 10 for the Actor dataset and 30 for Amazon-Photos. The difference in expander degrees is due to the significant variation in the average degree of nodes across the datasets. We aimed to balance the number of expander edges and graph edges since it has an impact on some of the experiments. In addition to the expander edges, we also included self-loops, which are necessary for the universal approximation theorem outlined by Shirzad et al. (2023). All networks in these experiments were trained with four layers. For each hidden dimension, we ran 100 experiments with different initializations. The learning rate was adjusted for each hidden dimension to ensure more stable convergence. However, for smaller hidden dimensions, some experiments led to drastically lower accuracy results, which we did not exclude from the analysis. All results, including those with lower accuracy, were considered in our analysis. F.1 Preliminaries Before presenting further experimental results, we provide a brief introduction to the metrics used. For two random variables X ∼ Pand Y ∼ Q, both defined in Rd (or equivalently, defined by their cumulative distribution functions (CDFs) F and G), we can define the following metrics: Energy Distance Energy distance is a metric used to measure the distance between two distributions (Székely and Rizzo, 2013; Sejdinovic et al., 2013; Rizzo and Székely, 2016), and is defined as: D2(F, G) = 2E[X − Y ] − E[X − X′] − E[Y − Y ′], where X, X′ ∼ Pand Y, Y′ ∼ Q, with all variables being independent of each other. This value is shown to be twice the Harald Cramer’s distance (Cramér, 1928), which is defined as: Z (F(x) − G(x))2 dx. This metric is non-negative; however, an unbiased estimator based on samples may yield negative results. Although the energy distance is a useful metric for identifying the distance between two probability distributions, it may not fully capture the variations between them. This issue becomes particularly relevant when measuring the performance of generative models, as it helps assess whether the generative model correctly approximates the real distribution. The following pairs of metrics provide finer-grained understanding of two different types of approximation. Precision & Recall (Sajjadi et al., 2018) These metrics assess generative models by constructing a manifold for both real and generated data. This is done by forming a hypersphere around each data point, extending its radius to the k-th nearest neighbor, and then aggregating these hyperspheres. Precision measures the proportion of generated samples that fall within the real data manifold, while recall quantifies the fraction of real samples covered by the generated data manifold. These metrics correlate well with human judgments in the visual domain and are effective in detecting issues like mode collapse and mode dropping. Density & Coverage (Naeem et al., 2020) These metrics, similar to Precision and Recall, evaluate generative models by considering individual hyperspheres rather than aggregating them into a manifold. Density measures the average number of real hyperspheres that each generated sample falls into, while Coverage quantifies the fraction of real samples that fall into at least one generated hypersphere. These metrics have been shown to be more robust than the Precision and Recall metrics in certain scenarios. 274 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.003 0.017 0.013 0.020 0.017 0.163 0.003 0.000 0.009 0.006 0.012 0.029 0.145 0.017 0.009 0.000 0.002 0.003 0.054 0.108 0.013 0.006 0.002 0.000 0.003 0.050 0.121 0.020 0.012 0.003 0.003 0.000 0.059 0.109 0.017 0.029 0.054 0.050 0.059 0.000 0.219 0.163 0.145 0.108 0.121 0.109 0.219 0.000 Actor Dataset without Expanders 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.000 0.004 0.008 0.018 0.187 0.115 0.000 0.000 0.000 0.007 0.018 0.200 0.102 0.004 0.000 0.000 0.001 0.009 0.214 0.120 0.008 0.007 0.001 0.000 0.017 0.208 0.125 0.018 0.018 0.009 0.017 0.000 0.228 0.154 0.187 0.200 0.214 0.208 0.228 0.000 0.360 0.115 0.102 0.120 0.125 0.154 0.360 0.000 Actor Dataset with Expanders 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.028 0.004 0.067 0.037 0.027 0.197 0.028 0.000 0.029 0.035 0.043 0.095 0.108 0.004 0.029 0.000 0.057 0.025 0.039 0.203 0.067 0.035 0.057 0.000 0.024 0.144 0.137 0.037 0.043 0.025 0.024 0.000 0.091 0.198 0.027 0.095 0.039 0.144 0.091 0.000 0.306 0.197 0.108 0.203 0.137 0.198 0.306 0.000 Amazon-Photo Dataset without Expanders 0.00 0.05 0.10 0.15 0.20 0.25 0.30 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.009 0.008 0.040 0.068 0.225 0.095 0.009 0.000 0.022 0.080 0.116 0.217 0.065 0.008 0.022 0.000 0.024 0.053 0.276 0.124 0.040 0.080 0.024 0.000 0.011 0.340 0.225 0.068 0.116 0.053 0.011 0.000 0.362 0.271 0.225 0.217 0.276 0.340 0.362 0.000 0.208 0.095 0.065 0.124 0.225 0.271 0.208 0.000 Amazon-Photo Dataset with Expanders 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35  (d) Figure 9: Pairwise energy distance across networks with different hidden dimensions, uniform distribution, and randomly generated attention scores. For all these metrics, we first consider the distribution of attention scores for each individual node’s neighborhood in a single layer, trained with a specific hidden dimension, represented as a vector. We then compare these distributions across different hidden dimensions or among different layers. Finally, we average the results over all nodes. F.2 Pairwise Distances While we demonstrated the energy distances from the reference hidden dimension of 64 in Figure 3, it is also valuable to examine all pairwise distances. We present these pairwise distances in Figure 9. Additionally, these distances may vary layer by layer, so it is insightful to explore how these distances change across different layers of the network. To this end, we provide the results in Figures 10 to 13. These experiments consistently show that attention scores obtained from different hidden dimension sizes are close to each other. In contrast to uniform sampling, or randomly generated attention scores, this distribution provides a much better reference for drawing neighborhood samples when the goal is to select nodes based on their attention score importance. ♂lightbulbInsight 1 Attention scores from a network with a smaller hidden dimension serve as a good estimator for the attention scores in a network with a higher hidden dimension. 284 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.000 0.000 0.000 0.000 0.000 0.000 0.218 0.000 0.000 0.000 0.000 0.000 0.000 0.219 0.219 0.219 0.219 0.219 0.218 0.219 0.000 Actor Dataset without Expanders, Layer 0 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.004 0.020 0.020 0.029 0.022 0.148 0.004 0.000 0.009 0.009 0.016 0.039 0.121 0.020 0.009 0.000 0.002 0.004 0.064 0.078 0.020 0.009 0.002 0.000 0.003 0.069 0.086 0.029 0.016 0.004 0.003 0.000 0.078 0.069 0.022 0.039 0.064 0.069 0.078 0.000 0.219 0.148 0.121 0.078 0.086 0.069 0.219 0.000 Actor Dataset without Expanders, Layer 1 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.004 0.026 0.016 0.028 0.021 0.148 0.004 0.000 0.011 0.005 0.014 0.040 0.120 0.026 0.011 0.000 0.003 0.003 0.073 0.071 0.016 0.005 0.003 0.000 0.004 0.060 0.092 0.028 0.014 0.003 0.004 0.000 0.077 0.071 0.021 0.040 0.073 0.060 0.077 0.000 0.219 0.148 0.120 0.071 0.092 0.071 0.219 0.000 Actor Dataset without Expanders, Layer 2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.002 0.023 0.015 0.024 0.026 0.136 0.002 0.000 0.014 0.008 0.017 0.039 0.121 0.023 0.014 0.000 0.003 0.006 0.080 0.066 0.015 0.008 0.003 0.000 0.005 0.070 0.086 0.024 0.017 0.006 0.005 0.000 0.080 0.075 0.026 0.039 0.080 0.070 0.080 0.000 0.219 0.136 0.121 0.066 0.086 0.075 0.219 0.000 Actor Dataset without Expanders, Layer 3 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200  (d) Figure 10: Pairwise energy distance between networks with different hidden dimensions, uniform distribution, and random attention scores for the Actor dataset, without the expander graph, on individual layers. This dataset has a very low average degree and it appears that almost always the first layer’s attention scores are very similar to the uniform distribution. F.3 Entropy of Attention Scores In this experiment, we analyze the entropy of the attention scores to examine how they change across layers. When the scores approach a one-hot vector, we refer to them as sharp attentions, while more smooth scores resemble a uniform distribution over the neighbors. The goal is to assess how sharp or smooth the attention scores are, on average, across the nodes. To achieve this, we use the entropy metric. Higher entropy indicates more smooth attention scores, while entropy is zero for one-hot vectors. We calculate the entropy for each node’s neighborhood and then average the entropies across all nodes and all random seeds in the layer. The results are presented in Figure 14. An insightful observation from this experiment is that the first layer, across all four datasets, con- sistently exhibits smoother attention scores, while the scores become sharper in subsequent layers. Generally, however, the attention scores are not very sharp in experiments without expander graphs, suggesting that all neighbors are likely similarly informative. This does not necessarily imply that all these nodes are equally important. If identical nodes with the same neighborhoods surround a node, all of them will receive equal attention scores, which indicates no selection in this case. Thus, this does not contradict the idea that a sparse matrix can estimate the same results. Sharpness varies across different hidden dimensions, which may be due to factors such as training dy- namics, learning rate, and the varying temperature setup for different hidden dimensions. Regardless, in all datasets and across all hidden dimensions, the first layer consistently has higher entropy. This 294 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.000 0.009 0.027 0.035 0.168 0.153 0.000 0.000 0.001 0.015 0.029 0.147 0.159 0.009 0.001 0.000 0.003 0.013 0.139 0.185 0.027 0.015 0.003 0.000 0.014 0.111 0.216 0.035 0.029 0.013 0.014 0.000 0.187 0.228 0.168 0.147 0.139 0.111 0.187 0.000 0.360 0.153 0.159 0.185 0.216 0.228 0.360 0.000 Actor Dataset with Expanders, Layer 0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.000 0.005 0.004 0.010 0.175 0.104 0.000 0.000 0.000 0.000 0.014 0.209 0.082 0.005 0.000 0.000 0.000 0.009 0.229 0.099 0.004 0.000 0.000 0.000 0.013 0.218 0.090 0.010 0.014 0.009 0.013 0.000 0.245 0.125 0.175 0.209 0.229 0.218 0.245 0.000 0.360 0.104 0.082 0.099 0.090 0.125 0.360 0.000 Actor Dataset with Expanders, Layer 1 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.001 0.001 0.004 0.017 0.214 0.100 0.001 0.000 0.002 0.018 0.013 0.205 0.087 0.001 0.002 0.000 0.007 0.009 0.244 0.102 0.004 0.018 0.007 0.000 0.035 0.269 0.104 0.017 0.013 0.009 0.035 0.000 0.238 0.134 0.214 0.205 0.244 0.269 0.238 0.000 0.360 0.100 0.087 0.102 0.104 0.134 0.360 0.000 Actor Dataset with Expanders, Layer 2 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.001 0.000 0.000 0.009 0.192 0.101 0.001 0.000 0.000 0.000 0.015 0.239 0.079 0.000 0.000 0.000 0.000 0.003 0.242 0.093 0.000 0.000 0.000 0.000 0.007 0.232 0.089 0.009 0.015 0.003 0.007 0.000 0.243 0.128 0.192 0.239 0.242 0.232 0.243 0.000 0.360 0.101 0.079 0.093 0.089 0.128 0.360 0.000 Actor Dataset with Expanders, Layer 3 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35  (d) Figure 11: Pairwise energy distance between networks with different hidden dimensions, uniform distribution, and random attention scores for the Actor dataset, with the expander graph, on individual layers. suggests that for sampling, larger neighborhood sizes may be needed in the first layer, while smaller neighborhood sizes could suffice in the subsequent layers. ♂lightbulbInsight 2 Attention scores are smoother in the first layer, and become sharper in subsequent layers. F.4 Inter-layer Attention Scores Similarity After observing that the entropy is higher in the first layer and similar across the subsequent layers, it is worth examining the distance between the attention scores of each pair of layers. The experimental results are presented in Figure 15. All values are relatively small compared to the previous ones, so they are multiplied by 100 for better presentation. The results show that, consistently, the first layer has some distance from all other layers, but the layers following it exhibit very similar attention scores. This suggests that the initial network may be trained using fewer layers, and further layer sampling could be achieved by repeating the attention scores from the final layer to train a deeper Spexphormer model. 304 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.004 0.002 0.038 0.016 0.012 0.239 0.004 0.000 0.012 0.022 0.013 0.029 0.207 0.002 0.012 0.000 0.049 0.021 0.006 0.270 0.038 0.022 0.049 0.000 0.010 0.079 0.197 0.016 0.013 0.021 0.010 0.000 0.040 0.213 0.012 0.029 0.006 0.079 0.040 0.000 0.306 0.239 0.207 0.270 0.197 0.213 0.306 0.000 Amazon-Photo Dataset without Expanders, Layer 0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.031 0.004 0.075 0.041 0.034 0.176 0.031 0.000 0.030 0.033 0.047 0.114 0.074 0.004 0.030 0.000 0.061 0.026 0.050 0.175 0.075 0.033 0.061 0.000 0.031 0.170 0.094 0.041 0.047 0.026 0.031 0.000 0.108 0.181 0.034 0.114 0.050 0.170 0.108 0.000 0.306 0.176 0.074 0.175 0.094 0.181 0.306 0.000 Amazon-Photo Dataset without Expanders, Layer 1 0.00 0.05 0.10 0.15 0.20 0.25 0.30  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.037 0.005 0.082 0.044 0.031 0.189 0.037 0.000 0.034 0.040 0.054 0.114 0.075 0.005 0.034 0.000 0.062 0.026 0.052 0.183 0.082 0.040 0.062 0.000 0.033 0.171 0.116 0.044 0.054 0.026 0.033 0.000 0.108 0.196 0.031 0.114 0.052 0.171 0.108 0.000 0.306 0.189 0.075 0.183 0.116 0.196 0.306 0.000 Amazon-Photo Dataset without Expanders, Layer 2 0.00 0.05 0.10 0.15 0.20 0.25 0.30 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.041 0.005 0.072 0.045 0.031 0.183 0.041 0.000 0.040 0.043 0.057 0.125 0.075 0.005 0.040 0.000 0.055 0.028 0.049 0.187 0.072 0.043 0.055 0.000 0.023 0.156 0.142 0.045 0.057 0.028 0.023 0.000 0.108 0.204 0.031 0.125 0.049 0.156 0.108 0.000 0.306 0.183 0.075 0.187 0.142 0.204 0.306 0.000 Amazon-Photo Dataset without Expanders, Layer 3 0.00 0.05 0.10 0.15 0.20 0.25 0.30  (d) Figure 12: Pairwise energy distance between networks with different hidden dimensions, uniform distribution, and random attention scores for the Amazon-Photo dataset, without the expander graph, on individual layers. ♂lightbulbInsight 3 The attention scores in the layers after the first are consistently very similar to one another, but distinct from the attention scores in the first layer. F.5 Precision, Recall, Density, & Coverage (PRDC) Since the energy distance may not fully capture how distributions match in some cases, alternative metrics have been proposed, primarily for assessing the performance of generative models (Sajjadi et al., 2018; Naeem et al., 2020). In this work, we apply these metrics by considering the attention scores from the network with a hidden dimension of 64 as the reference distribution, assuming that all other dimensions aim to generate the same distribution. We use violin plots to illustrate the distribution of PRDC values across the nodes in each layer. The results are presented in Figures 16 to 19. The plots show the kernel density estimate of the corresponding metrics across all nodes, layers, and random initializations. Precision & Recall and Density & Coverage are pairs of metrics that together describe how well the distribution has been learned. Excelling in just one of these metrics does not necessarily imply that the samples are close to each other. As shown in the results, attention scores from other hidden dimensions consistently achieve high values across all metrics, while uniform distribution and random attention scores fall short in at least one of the metrics from each pair. 314 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.009 0.014 0.042 0.064 0.241 0.146 0.009 0.000 0.041 0.079 0.104 0.226 0.122 0.014 0.041 0.000 0.008 0.020 0.327 0.232 0.042 0.079 0.008 0.000 0.004 0.396 0.307 0.064 0.104 0.020 0.004 0.000 0.437 0.348 0.241 0.226 0.327 0.396 0.437 0.000 0.208 0.146 0.122 0.232 0.307 0.348 0.208 0.000 Amazon-Photo Dataset with Expanders, Layer 0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 (a) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.012 0.004 0.036 0.060 0.229 0.088 0.012 0.000 0.015 0.087 0.117 0.222 0.042 0.004 0.015 0.000 0.031 0.058 0.260 0.087 0.036 0.087 0.031 0.000 0.012 0.327 0.205 0.060 0.117 0.058 0.012 0.000 0.324 0.230 0.229 0.222 0.260 0.327 0.324 0.000 0.208 0.088 0.042 0.087 0.205 0.230 0.208 0.000 Amazon-Photo Dataset with Expanders, Layer 1 0.00 0.05 0.10 0.15 0.20 0.25 0.30  (b) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.003 0.006 0.037 0.071 0.212 0.074 0.003 0.000 0.007 0.054 0.098 0.220 0.058 0.006 0.007 0.000 0.024 0.063 0.258 0.087 0.037 0.054 0.024 0.000 0.015 0.314 0.187 0.071 0.098 0.063 0.015 0.000 0.338 0.246 0.212 0.220 0.258 0.314 0.338 0.000 0.208 0.074 0.058 0.087 0.187 0.246 0.208 0.000 Amazon-Photo Dataset with Expanders, Layer 2 0.00 0.05 0.10 0.15 0.20 0.25 0.30 (c) 4 8 16 32 64 uniformrandom 4 8 16 32 64 uniform random 0.000 0.012 0.008 0.045 0.078 0.216 0.072 0.012 0.000 0.026 0.100 0.145 0.200 0.037 0.008 0.026 0.000 0.031 0.070 0.261 0.090 0.045 0.100 0.031 0.000 0.012 0.323 0.202 0.078 0.145 0.070 0.012 0.000 0.348 0.258 0.216 0.200 0.261 0.323 0.348 0.000 0.208 0.072 0.037 0.090 0.202 0.258 0.208 0.000 Amazon-Photo Dataset with Expanders, Layer 3 0.00 0.05 0.10 0.15 0.20 0.25 0.30  (d) Figure 13: Pairwise energy distance between networks with different hidden dimensions, uniform distribution, and random attention scores for the Amazon-Photo dataset, with the expander graph, on individual layers. F.6 Top-k Attention Sum Another way to assess the sharpness of the attention scores is by examining the sum of the top- k attention scores. If the top-k attention scores for a small k almost sum to one for all nodes, then using the top-k scores can closely approximate the representations of the larger network. However, this is not always the case. In this experiment, we analyze the sum of the top- k attention scores for k ranging from one to ten, across all nodes for hidden dimensions of 64 and 4. While the top-k attention score distributions are similar, the assumption that the sum will be close to one is unrealistic and does not occur frequently. The results, shown in Figure 20, include mean, median, and interquartile range, which indicate the spread of the middle 50% of the results. These results suggest that top-k attention selection may not be fully representative in transductive learning on graphs. This could be due to the presence of many similar nodes, causing the attention to be distributed across these nodes rather than being concentrated on a small subset, which affects the ability to approximate the larger network effectively using just the top-k scores. ♂lightbulbInsight 4 The attention scores in the layers after the first are consistently very similar to one another, but distinct from the attention scores in the first layer. 324 8 16 32 64 uniformrandom 0123 0.799 0.799 0.799 0.799 0.799 0.799 0.281 0.767 0.740 0.653 0.682 0.609 0.799 0.280 0.768 0.738 0.629 0.692 0.619 0.799 0.281 0.754 0.738 0.619 0.681 0.624 0.799 0.280 Actor Dataset without Expanders (a) 4 8 16 32 64 uniformrandom 0123 2.100 2.217 2.311 2.464 2.314 2.644 1.060 1.830 1.752 1.749 1.738 1.655 2.644 1.061 1.714 1.786 1.709 1.615 1.675 2.644 1.060 1.768 1.658 1.658 1.665 1.615 2.644 1.060 Actor Dataset with Expanders (b) 4 8 16 32 64 uniformrandom 0123 2.817 2.788 2.855 2.743 2.755 2.879 1.379 2.668 2.224 2.647 2.069 2.572 2.879 1.379 2.695 2.215 2.677 2.158 2.602 2.879 1.379 2.677 2.188 2.679 2.276 2.616 2.879 1.379 Amazon-Photo Dataset without Expanders (c) 4 8 16 32 64 uniformrandom 0123 2.896 2.767 2.882 2.908 2.902 4.015 2.263 2.566 2.263 2.303 2.668 2.856 4.015 2.264 2.596 2.387 2.316 2.635 2.879 4.015 2.263 2.549 2.361 2.323 2.680 2.885 4.015 2.264 Amazon-Photo Dataset with Expanders (d) Figure 14: Average entropy of attention scores for nodes across different layers. F.7 Attention Scores by Edge Type An interesting question is to examine the ratio of attention scores coming from graph edges versus expander edges and self-loops. Figure 21 illustrates how much of the attention score, on average, is attributed to each edge type across different hidden dimensions. We average the values over all nodes in all layers and random initializations. As expected, for a homophilic dataset like Amazon-Photo, the graph edges are more important. As the model’s hidden dimension increases, the model learns to place more attention on these edges. However, for a heterophilic dataset like Actor, the story is different, with graph edges playing a lower role. In Figure 22, we present a normalized version showing the average attention score by edge type. G Discussion Graph datasets arise from various domains, meaning that they might have differing inductive biases. More expressive methods may not necessarily yield better results on all datasets (Franks et al., 2024). Depending on the architecture and the task, more complex models can even lead to poorer results. Here, we discuss possible scenarios in which our model can be a good fit as well as the shortcomings of other classes of models that are overcome by our model. Graph Structure The relevance of the structure of the graph to the task can vary. For the simple synthetic task introduced in 1, the structure of the graph does not matter. So Transformers without inductive biases of the graphs are expressive enough to solve this problem; however message-passing networks will be restricted to the graph edges and rely on enough number of layers and may be challenged by oversquashing and oversmoothing problems. On the other hand, if the structure of the graph matters, such as counting the number of neighbor nodes with the same color for each node, the structure and the edges will be an important part. Transformers without expressive enough encodings to identify the graph edges will fail in this task. On the other hand, MPNNs even with one layer can easily solve this problem. Our approach enables solving problems in either case, by having both expander graphs for universal information propagation and the actual graph edges for inductive bias, 330 1 2 1 2 3 2.146 2.104 0.000 2.641 0.017 0.039 hidden dim = 4 0 1 2 1 2 3 3.910 3.960 0.010 3.865 -0.007 -0.022 hidden dim = 8 0 1 2 1 2 3 6.401 7.322 0.016 7.950 0.059 -0.002 hidden dim = 16 0 1 2 1 2 3 6.916 6.002 0.035 6.940 -0.027 0.025 hidden dim = 32 0 1 2 1 2 3 7.719 7.605 -0.060 7.931 0.067 0.077 hidden dim = 64 (a) Actor Dataset without Expander 0 1 2 1 2 3 1.003 0.497 0.387 0.498 0.085 -0.388 hidden dim = 4 0 1 2 1 2 3 1.760 1.609 -0.304 2.618 -0.048 0.210 hidden dim = 8 0 1 2 1 2 3 2.865 3.632 -0.002 3.758 -0.287 -0.231 hidden dim = 16 0 1 2 1 2 3 5.256 8.981 0.761 6.172 -0.379 0.273 hidden dim = 32 0 1 2 1 2 3 4.602 3.182 -0.058 3.917 -0.061 -0.250 hidden dim = 64 (b) Actor Dataset with Expander 0 1 2 1 2 3 0.679 0.515 0.011 0.554 -0.037 -0.032 hidden dim = 4 0 1 2 1 2 3 4.414 4.533 -0.077 5.172 0.043 -0.045 hidden dim = 8 0 1 2 1 2 3 2.562 2.683 -0.016 2.527 0.034 -0.035 hidden dim = 16 0 1 2 1 2 3 5.920 5.883 0.146 5.077 0.746 0.299 hidden dim = 32 0 1 2 1 2 3 2.933 3.048 0.025 3.112 0.257 0.024 hidden dim = 64 (c) Amazon-Photo Dataset without Expander 0 1 2 1 2 3 0.943 1.295 -0.069 1.563 0.005 -0.190 hidden dim = 4 0 1 2 1 2 3 2.930 1.836 0.047 2.976 -0.067 0.371 hidden dim = 8 0 1 2 1 2 3 5.029 5.319 -0.156 5.619 -0.051 -0.142 hidden dim = 16 0 1 2 1 2 3 3.754 4.692 0.024 4.066 -0.024 -0.004 hidden dim = 32 0 1 2 1 2 3 7.902 6.771 0.075 5.896 0.427 0.025 hidden dim = 64 (d) Amazon-Photo Dataset with Expander Figure 15: Inter-layer energy distances (×100) for different hidden dimensions. allowing the model to decide the subset of edges that suit the task better — only graph edges, only expander edges or a combination of both. Short-range Vs. Long-range Dependencies If the neighboring nodes tend to be from the same class, i.e., high homophily, MPNNs and methods such as NAGphormer (Chen et al., 2022a), which summarize the neighborhood have good inductive biases; whereas Transformers without proper identification for the neighborhoods may not be as fit for this task. Heterophily may not necessarily mean long-range dependencies, label of each node may just depend on the neighbor nodes, but still label of the neighbor nodes may be different most of the time. For example, for finding the grammatical function of the words in a sentence from a very long text, neighboring words are usually enough for this identification, and nearby words would be from different classes. On the other hand, some tasks may require long-range dependencies — identifying if there are other people in a social network with similar interests or the synthetic task introduced in 1 are some examples. Local models such as MPNNs would require deeper networks for modeling long-range dependencies that makes them prone to common problems such as oversquashing and oversmoothing (Topping et al., 2021; Di Giovanni et al., 2023b,a; Rusch et al., 2023). Our approach can be reduced to MPNN by giving lower attention scores to the expander edges, for learning on the tasks with short-range dependencies only. And also lets the long-range dependency modeling using expander edges. While models 344 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Precision 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Recall 4 8 16 32 uniform random 0.0 2.5 5.0 7.5 10.0Density 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Coverage Actor Dataset without Expanders Figure 16: Violin plots of Precision, Recall, Density, and Coverage metrics for the Actor dataset without expander graphs. 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Precision 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Recall 4 8 16 32 uniform random 0 2 4 6 8Density 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Coverage Actor Dataset with Expanders Figure 17: Violin plots of Precision, Recall, Density, and Coverage metrics for the Actor dataset with expander graphs. 354 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Precision 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Recall 4 8 16 32 uniform random 0 2 4 6 8 10Density 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Coverage Amazon-Photo Dataset without Expanders Figure 18: Violin plots of Precision, Recall, Density, and Coverage metrics for the Amazon-Photo dataset without expander graphs. 4 8 16 32 uniform random 0.00 0.25 0.50 0.75 1.00Precision 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Recall 4 8 16 32 uniform random 0 1 2 3 4Density 4 8 16 32 uniform random 0.0 0.2 0.4 0.6 0.8 1.0Coverage Amazon-Photo Dataset with Expanders Figure 19: Violin plots of Precision, Recall, Density, and Coverage metrics for the Amazon-Photo dataset with expander graphs. 361 2 3 4 5 6 7 8 9 10 k 0.2 0.4 0.6 0.8 1.0Top-k Attention Scores Sum Amazon-Photo without Expander Dataset  1 2 3 4 5 6 7 8 9 10 k 0.0 0.2 0.4 0.6 0.8Top-k Attention Scores Sum Amazon-Photo with Expander Dataset  1 2 3 4 5 6 7 8 9 10 k 0.4 0.6 0.8 1.0Top-k Attention Scores Sum Actor without Expander Dataset  1 2 3 4 5 6 7 8 9 10 k 0.2 0.3 0.4 0.5 0.6 0.7Top-k Attention Scores Sum Actor with Expander Dataset  Top-k Attention Scores Sum, Across Datasets dim=4 Mean dim=4 Median dim=4 Interquartile Range dim=64 Mean dim=64 Median dim=64 Interquartile Range Figure 20: Top-k attention scores sum for k values between 1 to 10. 4 8 16 32 64 Hidden Dimension 0.0 0.2 0.4 0.6 0.8 1.0Ratio Amazon-Photo Dataset 4 8 16 32 64 Hidden Dimension Actor Dataset Ratio of Attention Scores by Edge Type Graph Edges Expander Edges Self-loops Figure 21: Average sum of attention scores for different edge types—graph edges, expander edges, and self- loops—per node neighborhood. The total sum of attention scores per node is one. 374 8 16 32 64 Hidden Dimension 0.00 0.02 0.04 0.06 0.08Average Attention Score Amazon-Photo 4 8 16 32 64 Hidden Dimension 0.0 0.1 0.2 0.3 0.4 0.5Average Attention Score Actor Average Attention Scores by Edge Type Graph Edges Expander Edges Self-loops Figure 22: Average attention scores for different edge types across two datasets and for different hidden dimensions. designed specifically for some of these tasks may have the advantage of reduced complexity. But our approach lets learning without concern about the nature of the problem or having domain knowledge for the task or graph. Subsampling Graphs Many approaches break the graph into sections or subsample nodes or neighbors for training. This approach has shown promising results in many works such as (Zeng et al., 2020; Hamilton et al., 2017; Liu et al., 2021). However, there are many cases in which these approaches are not expressive enough. Clustering the nodes or batching and subsampling based on the neighborhood will not have the required inductive biases to solve the tasks with long-range dependencies. Approaches such as neighbor sampling or connected-subgraph sampling not only inherit the limits of the MPNN networks, but may even miss short-range dependencies. For example, Example (c) in 1 by merely random selection of the neighbors or subgraphs without considering the task. Random subset of node selection that has been used in several promising papers such as Wu et al. (2022, 2023, 2024) gives a chance for nodes from the same label to appear in the same batch, but the batch-size should increase with the graph size accordingly. Very small ratio of batch size to graph size would mean many edges or possible pair of nodes will never be appear in any batch and depending on the task this can limit the power of these models. Also, these models are usually not memory efficient, as graph size grows, they can not keep the batches small, and the required memory grows accordingly. On the other hand, our approach (1) makes smarter selection of neighbors based on the small network’s attention scores; (2) our sampling allows making k-hop neighborhood subgraphs from the extended graph connectivity, and (3) allows the training by trading off memory and time, without critical harm to the model’s expressive power. Unline the GraphSAGE and SGFormer, which use the full graph for the inference time our model uses the same sampling and batching techniques, letting efficient inference beside the efficient training. 38",
      "meta_data": {
        "arxiv_id": "2411.16278v1",
        "authors": [
          "Hamed Shirzad",
          "Honghao Lin",
          "Balaji Venkatachalam",
          "Ameya Velingker",
          "David Woodruff",
          "Danica Sutherland"
        ],
        "published_date": "2024-11-25T10:59:25Z",
        "pdf_url": "https://arxiv.org/pdf/2411.16278v1.pdf"
      }
    },
    {
      "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations",
      "abstract": "The training of graph neural networks (GNNs) is extremely time consuming\nbecause sparse graph-based operations are hard to be accelerated by hardware.\nPrior art explores trading off the computational precision to reduce the time\ncomplexity via sampling-based approximation. Based on the idea, previous works\nsuccessfully accelerate the dense matrix based operations (e.g., convolution\nand linear) with negligible accuracy drop. However, unlike dense matrices,\nsparse matrices are stored in the irregular data format such that each\nrow/column may have different number of non-zero entries. Thus, compared to the\ndense counterpart, approximating sparse operations has two unique challenges\n(1) we cannot directly control the efficiency of approximated sparse operation\nsince the computation is only executed on non-zero entries; (2) sub-sampling\nsparse matrices is much more inefficient due to the irregular data format. To\naddress the issues, our key idea is to control the accuracy-efficiency trade\noff by optimizing computation resource allocation layer-wisely and\nepoch-wisely. Specifically, for the first challenge, we customize the\ncomputation resource to different sparse operations, while limit the total used\nresource below a certain budget. For the second challenge, we cache previous\nsampled sparse matrices to reduce the epoch-wise sampling overhead. Finally, we\npropose a switching mechanisms to improve the generalization of GNNs trained\nwith approximated operations. To this end, we propose Randomized Sparse\nComputation, which for the first time demonstrate the potential of training\nGNNs with approximated operations. In practice, rsc can achieve up to\n$11.6\\times$ speedup for a single sparse operation and a $1.6\\times$ end-to-end\nwall-clock time speedup with negligible accuracy drop.",
      "full_text": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Zirui Liu 1 Shengyuan Chen 2 Kaixiong Zhou 1 Daochen Zha 1 Xiao Huang 2 Xia Hu 1 Abstract Training graph neural networks (GNNs) is ex- tremely time-consuming because sparse graph- based operations are hard to be accelerated by community hardware. Prior art successfully reduces the computation cost of dense matrix based operations (e.g., convolution and linear) via sampling-based approximation. However, unlike dense matrices, sparse matrices are stored in an irregular data format such that each row/column may have a different number of non-zero entries. Thus, compared to the dense counterpart, approx- imating sparse operations has two unique chal- lenges (1) we cannot directly control the effi- ciency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sampling sparse matrices is much more ineffi- cient due to the irregular data format. To address the issues, our key idea is to control the accuracy- efficiency trade-off by optimizing computation re- source allocation layer-wisely and epoch-wisely. For the first challenge, we customize the com- putation resource to different sparse operations, while limiting the total used resource below a cer- tain budget. For the second challenge, we cache previously sampled sparse matrices to reduce the epoch-wise sampling overhead. To this end, we propose Randomized Sparse Computation. In practice, RSC can achieve up to 11.6× speedup for a single sparse operation and 1.6× end-to- end wall-clock time speedup with almost no ac- curacy drop. Codes are available at https:// github.com/warai-0toko/RSC-ICML. 1Department of Computer Science, Rice University, Houston, TX, USA 2Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong SAR. Correspondence to: Xia Hu <xia.hu@rice.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 1. Introductions Graph Neural Networks (GNNs) have achieved great suc- cess across different graph-related tasks (Hamilton et al., 2017; Hu et al., 2020; Ying et al., 2018; Jiang et al., 2022; Zhou et al., 2022; 2023). However, despite its effective- ness, the training of GNNs is very time-consuming. Specifi- cally, GNNs are characterized by an interleaved execution that switches between the aggregation and update phases. Namely, in the aggregation phase, every node aggregates messages from its neighborhoods at each layer, which is implemented based on sparse matrix-based operations (Fey & Lenssen, 2019; Wang et al., 2019). In the update phase, each node will update its embedding based on the aggre- gated messages, where the update function is implemented with dense matrix-based operations (Fey & Lenssen, 2019; Wang et al., 2019). In Figure 1, SpMM and MatMul are the sparse and dense operations in the aggregation and update phases, respectively. Through profiling, we found that the aggregation phase may take more than 90% running time for GNN training. This is because the sparse matrix opera- tions in the aggregation phase have many random memory accesses and limited data reuse, which is hard to be acceler- ated by community hardwares (e.g., CPUs and GPUs) (Duan et al., 2022b; Han et al., 2016; Duan et al., 2022a). Thus, training GNNs with large graphs is often time-inefficient. ognb-proteins Reddit ogbn-product0 20 40 60 80 100Percentage of Time Consumption Other MatMul(forward) MatMul(backward) SpMM(forward) SpMM(backward) Figure 1: The time profiling of a two-layer GCNs on dif- ferent datasets. SpMM may take 70% ∼ 90% of the total time. We measure the time on a single NVIDIA RTX3090 (24GB). The detailed software and hardware information can be found in Appendix D. Existing works towards this problem can be roughly divided into three categories. First, some works propose distributed GNNs training systems, which focus on minimizing the 1 arXiv:2210.10737v2  [cs.LG]  2 Jul 2023RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations communication cost among hardware (Zheng et al., 2020; Ramezani et al., 2022; Wan et al., 2022b; Md et al., 2021; Wan et al., 2022a). Second, another research line optimizes the memory access pattern of sparse operations via coalesc- ing the memory access and fusing consecutive operations (Zhang et al., 2022; Huang et al., 2020a; Rahman et al., 2021; Wang et al., 2021). Third, some other works try to accelerate the training process from the optimization aspect, i.e., using fewer iterations to converge (Narayanan et al., 2022; Cong et al., 2020; Xu et al., 2021; Cai et al., 2021). In parallel, an orthogonal direction is to replace the ex- pensive operations with their faster-approximated versions (Adelman et al., 2021; Drineas et al., 2006b). The key idea is to sub-sample tensors onto low dimensional spaces and perform the original operations here. For example, for the linear operation between two matrices A ∈ Rn×m and B ∈ Rm×q, we first obtain A′ ∈ Rn×k and B′ ∈ Rk×q (k < m) by picking k representative columns of A and the corresponding rows of B (Drineas et al., 2006b). Then we approximate AB ≈ A′B′. With this procedure, the number of floating-point operations (FLOPs) and memory access are both reduced. Based on the idea, previous work success- fully accelerates the dense matrix based operations, such as convolution and linear operations (Adelman et al., 2021). The approximated operation can plug-and-play replace the exact operation to improve per-operation efficiency, and thus is compatible with most of the efficient training methods. Despite the potential, this perspective however has not been explored for the sparse operations in GNNs. The approximation method reduces the computational com- plexity at the cost of giving noisy outputs. Thus, there naturally exists an accuracy-efficiency trade-off. Com- pared to approximating dense matrix operations, there are two unique challenges to optimizing the trade-off for ap- proximated sparse operations. First, unlike the previous example of approximating linear operation, k cannot di- rectly control the efficiency (FLOPs) for sparse operations. This is because, for dense matrices, each row/column has the same amount of parameters. Thus the reduction of FLOPs in approximated dense operations is determined by the dimensions of the sub-sampled matrices (i.e., k). How- ever, in sparse operations, each row/column in the sparse adjacency matrix has different numbers of non-zero en- tries, and the computation is only executed on non-zero entries (i.e., irregular data format). Thus, the reduction of FLOPs in the sparse operations is decided by the selection of representative rows/columns. It lacks a mechanism to directly control the efficiency-accuracy trade-off for each sparse operation. Second, compared to the dense counter- part, sub-sampling (i.e., slicing) the sparse matrix is much more time-consuming due to its irregular data format (Han et al., 2016; Fey & Lenssen, 2019), which counteracts the acceleration from the FLOPs reduction. To this end, we propose Randomized Sparse Computation, dubbed RSC , the first approximation framework tailored for efficient GNN training. Our core idea is to control the trade-off by optimizing the computation resource alloca- tion at the “global” level. Specifically, to tackle the first challenge, at the layer-wise level, we propose to customize the FLOPs of each sparse operation while limiting the total FLOPs under a certain budget. The rationale behind this strategy is that each operation may have a different contribu- tion to the model accuracy. Thus, we could to assign more computational resources to “important” operations under a certain budget. More concretely, we frame it as a constraint optimization problem. Then we propose a greedy algorithm to solve it efficiently. To tackle the second challenge, at the epoch-wise level, we found that the selection of represen- tative row/columns tends to remain similar across nearby iterations. Based on this finding, we develop a caching mechanism to reuse the previously sampled sparse matrix across nearby iterations to reduce per-epoch sampling time. Finally, inspired by the recent finding that the final stage of training usually needs smaller noise to help convergence (Li et al., 2019; Dao et al., 2022), we propose to use approxi- mated sparse operation during most of the training process, while switching back to the original sparse operation at the final stage. This switching mechanism significantly reduces the accuracy drop, at the cost of slightly less speedup. We summarize our contributions as follows: • We accelerate the training of GNNs from a new perspec- tive, namely, replacing the expensive sparse operations with their faster-approximated versions. • Instead of focusing on balancing the efficiency-accuracy trade-off at the operation level, we control the trade-off through optimizing resource allocation at the layer-wise and epoch-wise levels. • We propose a caching mechanism to reduce the cost of sampling sparse matrices by reusing previous results. • Extensive experiments have demonstrated the effective- ness of the proposed method. Particularly, RSC can achieve up to 11.6× speedup for a single sparse opera- tion and a 1.6× end-to-end wall-clock time speedup with negligible (≈ 0.3%) accuracy drop. 2. Background and Preliminary 2.1. Graph Neural Networks Let G = ( V, E) be an undirected graph with V = (v1, ··· , v|V|) and E = (e1, ··· , e|E|) being the set of nodes and edges, respectively. Let X ∈ R|V|×d be the node feature matrix. A ∈ R|V|×|V| is the graph adjacency matrix, where Ai,j = 1 if (vi, vj) ∈ Eelse Ai,j = 0. 2RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations ˜A = ˜D−1 2 (A + I) ˜D−1 2 is the normalized adjacency ma- trix, where ˜D is the degree matrix of A + I. GNNs re- cursively update the embedding of a node by aggregating embeddings of its neighbors. For example, the forward pass of the lth Graph Convolutional Network (GCN) layer (Kipf & Welling, 2017) can be defined as: H(l+1) = ReLU( ˜AH(l)Θ(l)), (1) where H(l) is the node embedding matrix at the lth layer and H(0) = X. Θ(l) is the weight matrix of the lth layer. In practice, ˜A is often stored in the sparse matrix format, e.g., compressed sparse row (CSR) (Fey & Lenssen, 2019). From the implementation aspect, the computation of Equa- tion (1) can be described as: H(l+1) = ReLU   SpMM \u0012 ˜A, MatMul(H(l), Θ(l)) \u0013! , where SpMM(·, ·) is the Sparse-Dense Matrix Multiplica- tion and MatMul(·, ·) is the Dense Matrix Multiplication. Sparse operations, such as SpMM , have many random mem- ory accesses and limited data reuse. Thus they are much slower than the dense counterpart (Han et al., 2016; Duan et al., 2022b). To get a sense of the scale, we show in Figure 1 that for GCNs, SpMM may take roughly 70% ∼ 90% of the total training time. 2.2. Fast Approximated MatMul with Sampling Let X ∈ Rn×m, Y ∈ Rm×q. The goal is to efficiently esti- mate the matrix production XY . Truncated Singular Value Decomposition (SVD) outputs provably optimal low-rank estimation of XY (Adelman et al., 2021). However, SVD is almost as expensive as matrix production itself. Instead, the sampling algorithm is proposed to approximate the matrix product XY by sampling k columns of X and correspond- ing rows of Y to form smaller matrices, which are then multiplied as usual (Drineas et al., 2006b). This algorithm reduces the computational complexity from O(mnq) to O(knq). Specifically, XY = mX i=1 X:,iYi,: ≈ kX t=1 1 st X:,itYit,: = approx(XY ), (2) where X:,i ∈ Rn×1 and Yi,: ∈ R1×q are the ith column and row of X and Y , respectively. In this paper, we call (X:,i, Yi,:) the ith column-row pair. k is the number of sam- ples (1 ≤ k ≤ m). {pi}m i=1 is a probability distribution over the column-row pairs. it ∈ {1, ··· m} is the index of the sampled column-row pair at the tth trial. st is the scale fac- tor. Theoretically, (Drineas et al., 2006b) shows that if we set st = 1 kpit , then we have E[approx(XY )] =XY . Fur- ther, the approximation errorE[||XY −approx(XY )||F ] is minimized when the sampling probabilities {pi}m i=1 are proportional to the product of the column-row Euclidean norms (Drineas et al., 2006b): pi = ||X:,i||2 ||Yi,:||2Pm j=1 ||X:,j||2 ||Yj,:||2 . (3) 2.2.1. T OP-k SAMPLING The above sampling-based method is originally developed for accelerating the general application ofMatMul (Drineas et al., 2006b). Directly applying it to neural networks may be sub-optimal since it does not consider the characteristic of neural network weights. Based on the empirical observation that the distribution of weights remains centered around zero during training (Glorot & Bengio, 2010; Han et al., 2015), (Adelman et al., 2021) proposes a top-k sampling algorithm: Picking k column-row pairs with the largest ||X:,i||2 ||Yi,:||2Pm j=1 ||X:,j||2 ||Yj,:||2 deterministically without scaling. Equivalently, it means pi of column-row pairs with the k- largest value in Equation (3) equals 1, otherwise it equals 0. And sit is a constant 1. Albeit without the scaling while sampling column-row pairs deterministically, under on the assumption of zero-centered weight distribution, (Adelman et al., 2021) theoretically show that top-k sampling still yields an unbiased estimation of XY with minimal approx- imation error. Consequently, the top-k sampling algorithm empirically shows a significantly lower accuracy drop when approximating the convolution and linear operations in the neural networks (Adelman et al., 2021). In the next section, we explore how to approximate the expensive sparse operation via the top-k sampling. 3. The Proposed Framework The overview of RSC is shown in Figure 2, where we use the computation graph of GCN as an example. We first explore which SpMM in the computation graph can be re- placed with its approximated version (Section 3.1). Then since GNNs have multiple SpMM and each of them may have different importance to the model performance, we then automatically allocate computation resources to dif- ferent SpMM (Section 3.2). Finally, we explore two simple and effective tricks for improvingRSC , including a caching mechanism to reduce the overhead of sampling sparse ma- trices (Section 3.3.1) and a switching mechanism to reduce the accuracy drop (Section 3.3.2). 3.1. Where to Apply the Approximation 3.1.1. E XPERIMENTAL ANALYSIS Each sparse operation is executed twice at each train- ing step, i.e., one in the forward pass and the other one 3RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations SPMM MatMul Approx SpMM MatMul Forward Pass Backward Pass Down Sampl ing Cache Caching (Sec 3.3.1) Down Sampl ing Constraint  Optimization Eq. 5 𝑘!  Resource Allocation (Sec 3.2) Θ(!) 𝑯(!$%) 𝛁𝑯(!$%) 𝛁Θ(!) 𝛁𝑯(!)𝑯(!) 𝑱(!) 𝛁𝑱(!) 𝑨' Figure 2: Overview of RSC . For convenience, ReLU is ignored. RSC only replace the SpMM in the backward pass with its approximated version using top-k sampling (Section 3.1). kl is the number of samples for top-k sampling at the lth layer, which is automatically allocated (Section 3.2). To reduce the overhead of sampling, we also cache the sampled graph and reuse it across nearby iterations (Section 3.3). in the backward pass. As shown in Figure 2, here we take SpMM in the lth GCN layer as an example, the for- ward one is H(l+1) = ReLU(SpMM( ˜A, J(l))), where J(l) = MatMul(H(l), Θ(l)) is the intermediate node representations. And the backward one is ∇J(l) = SpMM( ˜A⊤, ∇H(l+1)). ∇J(l) and ∇H(l) are the gradient with respect to J(l) and H(l), respectively. Even though the approximation method itself is statisti- cally unbiased, replacing the exact sparse operation with their faster-approximated versions still injects noise to the computation graph. As we analyzed above, each SpMM is executed twice in the training step. Below we first exper- imentally analyze the impact of the injected noise in the forward pass and the backward pass. As shown in Table 1, we apply top-k sampling to approximate the SpMM in the forward pass, backward pass, or both, respectively. Table 1: Preliminary results on approximatingSpMM via top- k sampling. The model is a two-layer GCN, and the dataset is Reddit. Here we set thek as 0.1|V| across different layers. Method Reddit without approximation 95.39±0.04 only forward 16.45±0.39 only backward 95.25±0.03 forward and backward 80.74±1.00 From Table 1, the accuracy drop is negligible if we only replace SpMM in the backward pass. Notably, if we apply ap- proximation in both the forward and backward pass, the re- sult is significantly better than only applying top-k sampling in the forward pass. The reason is that when only apply- ing approximation in the forward pass, some row/columns are not included in the computation graph, so intuitively these row/columns should be excluded in the backward pass. “forward and backward” result in Table 1 is built based on this intuition such that in the backward pass, we use the column-row pairs sampled in the forward pass to compute the gradient (Adelman et al., 2021). However, it is still not comparable to the result of applying approximation only in the backward pass. Below we mathematically analyze the reason behind the results in Table 1. 3.1.2. T HEORETICAL ANALYSIS We first analyze the case of approximating the sparse opera- tions in the forward pass. Namely, replacingSpMM( ˜A, J(l)) with approx( ˜AJ(l)). We note that we have E[f(x)] ̸= f(E[x]) for any non-linear function f(·), e.g., E[x2] ̸= E2[x]. Thus, even when the approximation method gives an unbiased estimation, i.e., E[approx( ˜AJ(l))] = ˜AJ(l), the node embeddings H(l+1) are still biased since the acti- vation function is non-linear. To see this, E[H(l+1)] =E[ReLU(approx( ˜AJ(l))]) ̸= ReLU(E[approx( ˜AJ(l))]) =H(l+1). Thus, if we apply the approximation for the SpMM in the forward pass, the bias will be propagated layer-by-layer and cause significantly worse results. For the case of only approximating the sparse operation in the backward pass, we have the following proposition: Proposition 3.1 (Proof in Appendix A). If the approxima- tion method is itself unbiased, and we only replace theSpMM in the backward pass with its approximated version, while leaving the forward one unchanged, then the calculated gradient is provably unbiased. The high-level idea is that the gradient of the activation function in the backward pass is only related to the pre- activations in the forward pass, and thus is independent of the approximation error introduced in the backward pass. Due to the page limit, we also discuss why sampling-based approximation is suitable for accelerating GNNs in Ap- pendix A. As suggested by our theoretical and empirical analysis, as shown in Figure 2, we only approximate the sparse operations in the backward pass, while leaving all other operations unchanged. 3.2. How to Apply the Approximation As we mentioned, for sparse operations, the acceleration is decided by the selection of sampled column-row pairs. To see this, as shown in Figure 3, suppose we use top- k sampling to approximate SpMM( ˜A⊤, ∇H). Since the computations are only executed on the non-zero entries, so selecting the orange pairs (i.e., pair 1 and 3) will result in 3 7 × less computational cost (FLOPs) compared to selecting 4RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 1 11 111111132100123 3210∇𝐻!!∇𝐻!\"∇𝐻!#∇𝐻\"! ∇𝐻#!∇𝐻$! ∇𝐻\"\" ∇𝐻#\"∇𝐻$\" ∇𝐻\"# ∇𝐻##∇𝐻$# Nodeembeddinggradients∇𝐻∈ℝ!×#,with𝑑=3Sparseadjacencymatrix𝐴$∈ℝ!×!,with𝑁=4 × Figure 3: For approximated sparse operations, the accelera- tion is decided by the selection of column-row pairs. the blue pair (i.e., pair 0 and 2). For both the orange and blue cases, we have k = 2. Thus, the number of samples k cannot directly constrain the FLOPs for each individual operation. Moreover, a GNN has multiple operations (or layers), and the model accuracy has a different sensitivity to the approximation error at different layers. To optimize the accuracy-efficiency trade-off, our key idea is to customize the computation resources (i.e., FLOPs) for each layer by adjusting the number of samples kl in the l-th layer. In this way, we minimize the impact of approximation, while limiting the overall FLOPs under a certain budget. Based on the idea, we frame the resource allocation problem as the following constrained optimization problem: min {kl} − LX l=1 X i∈Topkl ∥ ˜A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥ ˜A∥F ∥∇H(l+1)∥F , (4a) s.t. LX l=1 X i∈Topkl #nnzi ∗ dl ≤ C LX i=1 |E|dl, (4b) where C is the budget (0 < C <1) that controls the overall reduced FLOPs. kl is the number of samples for the top-k sampling at the l-th layer. dl is the hidden dimensions ofl-th layer, and #nnzi is the number of non-zero entries at the i-th column of ˜A⊤. Topkl is the set of indices associated with the kl largest ∥ ˜A⊤ :,i∥2∥∇H(l+1) i,: ∥2. Equation (4a) is equivalent to minimizing the relative ap- proximation error E[|| ˜A⊤∇H(l+1)−approx( ˜A⊤∇H(l+1))||F ∥ ˜A∥F ∥∇H(l+1)||F ] summarized over all layers (Adelman et al., 2021). Also, different sparse operations are weighted summation by the magnitude of gradient ∥∇H(l+1)∥2, which implicitly en- codes the importance of different operations. Equation (4b) is the constraint that controls the overall FLOPs. Specifically, the FLOPs of SpMM between ˜A and the gradient ∇H ∈ RN×d is O(|E|d) and P j∈V #nnzj = |E|. We note that Equation (4b) also bounds the number of memory access of SpMM . 3.2.1. GREEDY SOLUTION The above combination optimization objective is NP-hard, albeit it can be solved by dynamic programming. However, dynamic programming is very slow, which somehow con- tradicts our purpose of being efficient. Thus, we propose to use a greedy algorithm to solve it. Specifically, it starts with the highest kl = |V| for all layers. In each move, it chooses a kl among {kl}L l=1 to reduce by a step size (e.g., 0.02|V|), such that the increment of errors in Equation (4a) is mini- mal. The greedy algorithm will stop when the current total FLOPs fits in the budget in Equation (4b). This algorithm runs super fast, and we found that it has minimal impact on efficiency. We provide the pseudo-code of our greedy algorithm in Algorithm 1 of Appendix B. 3.3. When to Apply the Approximation 3.3.1. CACHE THE SAMPLED SPARSE MATRICES We first give the details about the Compressed Sparse Row (CSR) format for representing the sparse matrix here. CSR stores nonzero values in a matrix and their position in three arrays: index array Rowptr, column array Col, and value array Val. The elements in Rowptr act as the starting indices of the elements in Col and Val that correspond to each row. Specifically, the elements of row i are stored in indices Rowptr[i] to Rowptr[i+ 1] − 1 of Col and Val . The elements in Col and Val are the column index and value in that column, respectively. Figure 5 shows the CSR format of the matrix shown in Figure 3. We ignore the Val array here for illustration convenience. Executing the top- k sampling contains two steps: First, it decides the indices corresponding to the top- k largest column row norms in Equation (3). Second, slicing the matrices according to the indices. In practice, the overhead of the first step can be ignored. However, unlike dense matrices, slicing the adjacency matrix is much slower due to its irregular data format. To see this, suppose the top-k indices of the sparse matrix in Figure 3 correspond to the orange column-row pairs. Figure 5 shows the process of slicing the adjacency matrix in CSR format by reserving only the orange columns. Slicing sparse matrices requires to re-process the graph to build the new Rowptr and Col (Fey & Lenssen, 2019), which introduces significant time overhead, especially for large graphs. For the full graph training, we use the same adjacency matrix across different epochs1. We made a crucial observation that the top-k indices in the adjacency matrix tend to be the same across iterations. In Figure 4, we plot the AUC score of top- k indices between every iteration t and iteration t + 10for 1For sub-graph based training, we can first sample all of the sub-graphs offline. Then during the training, we apply the caching mechanism to each sampled graph. 5RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score Reddit GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score Yelp GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score ogbn-proteins GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer Figure 4: For each layer, the selected column-row pairs tend to be very similar across iterations. Models here are two-layer GCN and GraphSAGE. Here we show the matching scores (AUC) of top-k indices between every 10 steps. Figure 5: The process of slicing the sparse matrix in Figure 3 by only reserving orange columns (in CSR format). each layer throughout the whole training process. Here we note that AUC score is a commonly used ranking measure and a 1.0 AUC score means the ranking of column-row pairs is identical across iterations. The results in Figure 4 indicate that the top-k indices won’t change significantly within a few iterations. Thus, as shown in Figure 2, we propose to reuse the sampled adjacency matrix for each layer across nearby iterations. Discussion. The rationale behind the success of caching is the slow rate of change in the learned embeddings within GNNs (Fey et al., 2021; Wan et al., 2022a). Prior research has leveraged this “staleness” of embeddings to enhance the efficiency of GNN training [1, 2]. The success of caching can also be explained by the staleness: if embeddings (and their gradients) across consecutive steps remain nearly iden- tical, the sampled sparse matrix will also exhibit minimal variation. Later we experimentally show that the caching mechanism does not impact the model performance a lot, but leads to a significant speedup. 3.3.2. SWITCH BACK AT THE END When training neural networks, the common practice is to use a large learning rate for exploration and anneal to a small one for final convergence (Li et al., 2019). The ratio- nale behind this strategy is that, at the end of the training process, we need to fine-tune our model with small noise for convergence. Since our approximation sparse operations will bring extra noise to the gradient, intuitively, we can switch back to the original sparse operations to help con- vergence. More formally, we propose to use approximated sparse operation during most of the training process, while switching back to the original sparse operation at the final stage. We experimentally show that this switching mecha- nism significantly reduces the accuracy drop at the cost of slightly less acceleration effect. We note that the switching mechanism is not proposed in this paper. The switching mechanism takes inspiration from previous work Dao et al. (2022), and both our work and Dao et al. (2022) utilize the switching mechanism to minimize the impact of approximation. 4. Related work and Discussion Due to the page limit, we first discuss the related work on approximated matrix multiplication. Other related topics, i.e., subgraph-based training, randomized GNN training, and non-approximated GNN acceleration, can be found in Appendix C. Approximated Matrix Multiplication.The approximated matrix production can be roughly divided into three cat- egories. However, only a few of them can be used for accelerating GNN training. Specifically, (1) Random walk- based methods (Cohen & Lewis, 1999) performs random walks on a graph representation of the dense matrices, but is only applicable to non-negative matrices; (2) Butterfly- based methods (Chen et al., 2021; Dao et al., 2022) replace dense matrices with butterfly matrices. It is not applicable to SpMM in GNNs because the adjacency matrix often cannot be reduced to a butterfly matrix. (3) Column-row sampling methods(Drineas et al., 2006a; Drineas & Kannan, 2001) sample the input matrices with important rows and columns, then perform the production on the sampled matrix as usual. 5. Limitations First, to guarantee the model accuracy, we only replace the sparse operation in the backward pass. Thus the upper bound of RSC ’s speedup is limited. However, we note that the backward pass usually is more time-consuming than the forward pass, which is also empirically shown in Table 2. Second, some GNNs rely on the scatter-and-gather instead of SpMM (and its variant) to perform the aggregation, such as GAT (Veliˇckovi´c et al., 2017). They are not covered in this paper. However, scatter-and-gather based GNNs can also 6RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations be accelerated by RSC because the column-row sampling is also applicable to scatter and gather operation. Similarly, the caching and switching mechanisms are also applicable to them. However, for the resource allocation Algorithm 1, the scatter and gather operations require tailored error bound and the computation cost modeling in Equation (4). We leave it as future work. 6. Experiments We verify the effectiveness of our proposed framework via answering the following research questions: Q1: How ef- fective is RSC in terms of accuracy with reduced training time? Q2: How effective is our proposed allocation strategy compared to the uniform allocation strategy? Q3: What is the layer-wise ratio assigned by RSC ? Q4: How effec- tive is the caching and switching mechanism in terms of the trade-off between efficiency and accuracy? If without explicitly mentioned, all reported results are averaged over ten random trials 6.1. Experimental Settings Datasets and Baselines. To evaluateRSC , we adopt four common large-scale graph benchmarks from different do- mains, i.e., Reddit (Hamilton et al., 2017), Yelp (Zeng et al., 2020), ogbn-proteins (Hu et al., 2020), and ogbn- products (Hu et al., 2020). We evaluate RSC under both the mini-batch training and full-batch training settings. For the mini-batch training setting, we integrate RSC with one of the state-of-the-art sampling methods, GraphSAINT (Zeng et al., 2020). For the full-batch training setting, we inte- grate RSC with three popular models: two commonly used shallow models, namely, GCN (Kipf & Welling, 2017) and GraphSAGE (Hamilton et al., 2017), and one deep model GCNII (Chen et al., 2020). To avoid creating confusion, GCN, GraphSAGE, and GCNII are all trained with the whole graph at each step. For a fair comparison, we use the MEAN aggregator for GraphSAGE and GraphSAINT throughout the paper. Details about the hyperparameters and datasets are in Appendix D. Hyperparameter settings. RSC contains three parts. First, the allocation strategy. We choose the overall budget C in Equation (4b) from {0.1, 0.3, 0.5}. We run the resource allocation strategy every ten steps. The step size α in Algo- rithm 1 is set as 0.02|V|. Second, the caching mechanism. According to Figure 4, we sample the adjacency matrix every ten steps and reuse the sampled matrices for nearby steps. Third, the switching mechanism, where we apply RSC for 80% of the total epochs, while switching back to the original operations for the rest of the 20% epochs. Due to the page limit, We present a detailed hyperparameter study in Appendix E Figure 11 and Figure 12. Evaluation metrics. To evaluate the practical usage of RSC , we report the wall clock time speedup measured on GPUs. Specifically, the speedup equalsTbaseline/Trsc, where Tbaseline and Trsc are the wall clock training time of baseline and RSC , respectively. We note that the Trsc includes the running time of the greedy algorithm, and the effects of caching and switching. 6.2. Performance Analysis 6.2.1. A CCURACY -EFFICIENCY TRADE -OFF To answer Q1, we summarize the speedup and the test accuracy/F1-micro/AUC of different methods in Table 3. Since RSC accelerates the sparse operation in the backward pass, we also provide the detailed efficiency analysis in Table 2. In summary, we observe: ❶ At the operation level, RSC can accelerate the sparse operation in the backward pass by up to 11.6×. For end- to-end training, the accuracy drop of applying RSC over baselines is negligible (0.3%) across different models and datasets, while achieving up to 1.6× end-to-end wall clock time speedup. The gap between the operation speedup and the end-to-end speedup is due to the following two reasons. First, we focus on accelerating the sparse computations in GNNs, which is the unique bottleneck to GNNs. The other dense computations can certainly be accelerated by approximation methods, but this is beyond the scope of this paper. Second, we only accelerate the sparse computation in the backward pass instead of the forward one to guaran- tee performance. We note that for approximation methods that accelerate the training process at operation level, a 1.2 ≈ 1.3× wall-clock speedup with negligible accuracy drop can be regarded as non-trivial (for details, please see Table 1 in (Adelman et al., 2021)), especially considering that these approximation methods are orthogonal to most of the existing efficient training methods. For GraphSAINT, the speedup of RSC is around 1.1×, which is smaller than the full graph training. This is because for subgraph-based training, the equivalent “batch size” is much smaller than the full graph counterparts. As a result, the GPU utility is low since it does not assign each processor a sufficient amount of work and the bottleneck is the mini-batch transfer time (Kaler et al., 2022). We note that the mini-batch sampling and transfer time can be optimized from the system perspec- tive (Kaler et al., 2022), which is orthogonal to our work. The speedup is expected to be larger when the mini-batch sampling time is optimized. 6.2.2. A BLATION ON RESOURCE ALLOCATION . Due to the page limit, we first show the running time of the greedy algorithm in Appendix E Table 11. We conclude that the overhead of the greedy algorithm is negligible com- pared to the acceleration effect of RSC . To answer Q2, we 7RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 2: Comparison on the efficiency at the operation level. fwd/bwd is the wall-clock time for a single forward/backward pass (ms). SpMM MEAN corresponds to the MEAN aggregator used in GraphSAGE (Appendix A.3). Reddit Yelp ogbn- proteins ogbn- products fwd bwd fwd bwd fwd bwd fwd bwd SpMM Baseline 36.28 44.23 26.88 34.38 31.72 42.99 261.03 316.80 +RSC - 3.81 (11.6 ×) - 9.86 (3.49 ×) - 14.87 (2.89 ×) - 35.28 (8.98 ×) SpMMMEAN (Appendix A.3) Baseline 36.21 44.27 26.78 34.38 31.80 43.11 261.03 316.84 +RSC - 7.47 (5.92 ×) - 19.62 (1.75 ×) - 5.22 (8.26 ×) - 71.59 (4.43 ×) Table 3: Comparison on the test accuracy/F1-micro/AUC and speedup on four datasets. Bold faces indicate the accuracy drop is negligible (≈ 0.3%) or the result is better compared to the baseline.The hardware here is a RTX3090 (24GB). # nodes # edges 230K 11.6M 717K 7.9M 132K 39.5M 2.4M 61.9M Model Methods Reddit Yelp ogbn- proteins ogbn- products Acc. Budget C Speedup F1-microBudget C Speedup AUC Budget C Speedup Acc. Budget C Speedup Graph- SAINT Baseline 96.40±0.03 1 1 × 63.30±0.14 1 1 × — — — 79.01±0.21 1 1 × +RSC 96.24±0.030.1 1.11 × 63.34±0.180.1 1.09 × — — — 78.99±0.32 0.3 1.04 × GCN Baseline 95.33±0.03 1 1 × 44.28±1.04 1 1 × 71.99±0.66 1 1 × 75.74±0.11 1 1 × +RSC 95.13±0.050.1 1.47 × 46.09±0.540.1 1.17 × 71.60±0.450.3 1.51 × 75.44±0.21 0.3 1.35 × GraphSAGE (full batch) Baseline 96.61±0.05 1 1 × 63.06±0.18 1 1 × 76.09±0.77 1 1 × 78.73 ± 0.12 1 1 × +RSC 96.52±0.040.1 1.32 × 62.89±0.190.1 1.13 × 76.30±0.420.3 1.60 × 78.50± 0.090.1 1.53 × GCNII Baseline 96.71±0.07 1 1 × 63.45±0.17 1 1 × 73.79±1.32 1 1 × — — — +RSC 96.50±0.120.3 1.45 × 63.57±0.210.1 1.19 × 75.20±0.540.5 1.41 × — — — /uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013 /uni0000001c/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000026/uni00000031/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 /uni00000014/uni00000011/uni00000014/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000015/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000015/uni00000018/uni00000014/uni00000011/uni00000015/uni00000018/uni00000013/uni00000014/uni00000011/uni00000015/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000016/uni00000013/uni00000013/uni00000014/uni00000011/uni00000016/uni00000015/uni00000018 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001c/uni00000019/uni00000011/uni00000018/uni0000001b /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000013 /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000015 /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000017/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 /uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001c/uni00000018/uni00000011/uni00000018 /uni0000001c/uni00000019/uni00000011/uni00000013 /uni0000001c/uni00000019/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 Figure 6: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. Here we disabled the caching and switch mechanism for a fair comparison. More results can be found in Appendix E Table 4: Ablation on the caching and switching mechanism. Experiments are conducted on ogbn-proteins. All results are averaged over five random trials. Ablation on Caching Switching AUC Speedup GCN ✗ ✗ 71.60 ± 0.66 1.19 × ✗ ✓ 72.19 ± 0.79 1.14 × ✓ ✗ 69.80 ± 0.60 1.60 × ✓ ✓ 71.60 ± 0.45 1.51 × GraphSAGE ✗ ✗ 75.23 ± 0.79 1.37 × ✗ ✓ 76.39 ± 0.39 1.32 × ✓ ✗ 75.53 ± 0.60 1.78 × ✓ ✓ 76.30 ± 0.42 1.60 × GCNII ✗ ✗ 74.07 ± 0.83 1.10 × ✗ ✓ 74.50 ± 0.52 1.04 × ✓ ✗ 72.47 ± 0.75 1.46 × ✓ ✓ 75.20 ± 0.54 1.41 × compare RSC with the uniform allocation strategy, i.e., set- ting kl = C|V| for all sparse operations in the backward pass. As shown in Figure 6, we plot the Pareto frontier of the accuracy-efficiency trade-off on the Reddit dataset for RSC and the uniform strategy with different C. For a fair comparison, we disabled the caching and switching mechanism. Due to page limit, more results are shown in Appendix E. We observe that: ❷ RSC exhibits a supe- rior trade-off between accuracy and efficiency compared to the uniform allocation, especially under high speedup regime. Namely, compared to the uniform allocation, RSC can achieve higher model accuracy under the same speedup. This can be explained by the fact that each operation has a different importance to the model performance. RSC can au- tomatically allocate more resources to important operations under a given total budget. To answer Q3, due to the page limit, we visualize the al- located kl for each layer across iterations in Appendix E Figure 7, and the degree of picked nodes in Appendix E Figure 8. We observe: ❸ The kl assigned by RSC evolves along with the training. 6.2.3. A BLATION ON CACHING AND SWITCHING . In section 6.2.2, we have shown the superior results of the proposed resource allocation strategy. As we mentioned in Section 3.3, we also introduce two simple tricks to for improving RSC , i.e., the caching and switching mechanism. To verify the effect of each of them (Q4), we conduct incre- mental evaluations on GCN, GraphSAGE and GCNII with 8RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations ogbn-proteins, which are summarized in Table 4. The row without caching and switching in Table 4 corresponds to the results with the proposed resource allocation strategy. We observe: ❹ Switching mechanism significantly improves the model performance, at the cost of slightly less acceleration effect. As we analyzed in Section 3.3.2, the improvement can be explained by the fact that the final training stage requires smaller gradient noise to help convergence. ❺ Caching mechanism significantly improves the wall-clock time speedup, at the cost of worse model performance. Al- though caching mechanism can reduce the overhead of sam- pling, the performance drop is too large (> 1%). Intuitively, the accuracy drop of caching also implies that we could not use a “static” down-sampled graph throughout the training process. ❻ Surprisingly, jointly applying the caching and switching, the performance drop can be minimized. 7. Acknowledgements The authors thank the anonymous reviewers for their helpful comments. The work is in part supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. 8. Conclusions and Future work We propose RSC , which replaces the sparse computations in GNNs with their fast approximated versions. RSC can be plugged into most of the existing training frameworks to improve their efficiency. Future work includes exploring RSC for GNNs that rely on scatter-and-gather operations. References Adelman, M., Levy, K., Hakimi, I., and Silberstein, M. Faster neural network training with approximate tensor operations. Advances in Neural Information Processing Systems, 34:27877–27889, 2021. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-y., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, pp. 1204–1215. PMLR, 2021. Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., and Re, C. Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint arXiv:2112.00029, 2021. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduction. In Inter- national conference on machine learning. PMLR, 2017. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Sim- ple and deep graph convolutional networks. In Interna- tional Conference on Machine Learning, pp. 1725–1735. PMLR, 2020. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257–266, 2019. Cohen, E. and Lewis, D. D. Approximating matrix multi- plication for pattern recognition tasks. Journal of Algo- rithms, 30(2):211–252, 1999. Cong, W., Forsati, R., Kandemir, M., and Mahdavi, M. Minimal variance sampling with provable guarantees for fast training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 1393–1403, 2020. Dao, T., Chen, B., Sohoni, N. S., Desai, A., Poli, M., Gro- gan, J., Liu, A., Rao, A., Rudra, A., and R´e, C. Monarch: Expressive structured matrices for efficient and accurate training. In International Conference on Machine Learn- ing, pp. 4690–4721. PMLR, 2022. Drineas, P. and Kannan, R. Fast monte-carlo algorithms for approximate matrix multiplication. In Proceedings 42nd IEEE Symposium on Foundations of Computer Science, pp. 452–459. IEEE, 2001. Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132– 157, 2006a. Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132– 157, 2006b. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethink- ing. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022a. URL https://openreview.net/forum? id=2QrFr_U782Z. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethinking. 2022b. Feng, W., Zhang, J., Dong, Y ., Han, Y ., Luan, H., Xu, Q., Yang, Q., Kharlamov, E., and Tang, J. Graph random neural networks for semi-supervised learning on graphs. 9RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Advances in neural information processing systems, 33: 22092–22103, 2020. Feng, W., Dong, Y ., Huang, T., Yin, Z., Cheng, X., Khar- lamov, E., and Tang, J. Grand+: Scalable graph random neural networks. In Proceedings of the ACM Web Confer- ence 2022, pp. 3248–3258, 2022. Fey, M. and Lenssen, J. E. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Repre- sentation Learning on Graphs and Manifolds, 2019. Fey, M., Lenssen, J. E., Weichert, F., and Leskovec, J. Gn- nautoscale: Scalable and expressive graph neural net- works via historical embeddings. In International confer- ence on machine learning, 2021. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. InProceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News, 44(3):243–254, 2016. Han, X., Jiang, Z., Liu, N., and Hu, X. G-mixup: Graph data augmentation for graph classification. arXiv preprint arXiv:2202.07179, 2022. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. Huang, G., Dai, G., Wang, Y ., and Yang, H. Ge-spmm: General-purpose sparse matrix-matrix multiplication on gpus for graph neural networks. In SC20: International Conference for High Performance Computing, Network- ing, Storage and Analysis, pp. 1–12. IEEE, 2020a. Huang, Q., He, H., Singh, A., Lim, S.-N., and Benson, A. R. Combining label propagation and simple models out-performs graph neural networks. In International Conference on Learning Representations, 2020b. Huang, W., Zhang, T., Rong, Y ., and Huang, J. Adap- tive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, 2018. Jiang, Z., Han, X., Fan, C., Liu, Z., Zou, N., Mostafavi, A., and Hu, X. Fmp: Toward fair graph message passing against topology bias. arXiv preprint arXiv:2202.04187, 2022. Jin, W., Ma, Y ., Liu, X., Tang, X., Wang, S., and Tang, J. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 66–74, 2020. Kaler, T., Stathas, N., Ouyang, A., Iliopoulos, A.-S., Schardl, T., Leiserson, C. E., and Chen, J. Accelerating training and inference of graph neural networks with fast sampling and pipelining. Proceedings of Machine Learning and Systems, 4:172–189, 2022. Kipf, T. N. and Welling, M. Semi-supervised classi- fication with graph convolutional networks. In In- ternational Conference on Learning Representations , 2017. URL https://openreview.net/forum? id=SJU4ayYgl. Klicpera, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2018. Li, Y ., Wei, C., and Ma, T. Towards explaining the regu- larization effect of initial large learning rate in training neural networks. Advances in Neural Information Pro- cessing Systems, 32, 2019. Liu, Z., Jin, H., Wang, T.-H., Zhou, K., and Hu, X. Di- vaug: Plug-in automated data augmentation with explicit diversity maximization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4762– 4770, 2021. Martinsson, P.-G. and Tropp, J. Randomized numerical linear algebra: foundations & algorithms (2020). arXiv preprint arXiv:2002.01387, 2020. Md, V ., Misra, S., Ma, G., Mohanty, R., Georganas, E., Heinecke, A., Kalamkar, D., Ahmed, N. K., and Avancha, S. Distgnn: Scalable distributed training for large-scale graph neural networks. In Proceedings of the Interna- tional Conference for High Performance Computing, Net- working, Storage and Analysis, pp. 1–14, 2021. Narayanan, S. D., Sinha, A., Jain, P., Kar, P., and SEL- LAMANICKAM, S. Iglu: Efficient GCN training via lazy updates. In International Conference on Learning 10RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Representations, 2022. URL https://openreview. net/forum?id=5kq11Tl1z4. Qiu, J., Dhulipala, L., Tang, J., Peng, R., and Wang, C. Lightne: A lightweight graph processing system for net- work embedding. In Proceedings of the 2021 interna- tional conference on management of data, pp. 2281–2289, 2021. Rahman, M. K., Sujon, M. H., and Azad, A. Fusedmm: A unified sddmm-spmm kernel for graph embedding and graph neural networks. In 2021 IEEE International Par- allel and Distributed Processing Symposium (IPDPS), pp. 256–266. IEEE, 2021. Ramezani, M., Cong, W., Mahdavi, M., Kandemir, M., and Sivasubramaniam, A. Learn locally, correct globally: A distributed algorithm for training graph neural networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=FndDxSz3LxQ. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node clas- sification. arXiv preprint arXiv:1907.10903, 2019. Savas, B. and Dhillon, I. S. Clustered low rank approxi- mation of graphs in information science applications. In Proceedings of the 2011 SIAM International Conference on Data Mining, pp. 164–175. SIAM, 2011. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2017. Wan, C., Li, Y ., Kim, N. S., and Lin, Y . {BDS}- {gcn}: Efficient full-graph training of graph convolu- tional nets with partition-parallelism and boundary sam- pling, 2021. URL https://openreview.net/ forum?id=uFA24r7v4wL. Wan, C., Li, Y ., Li, A., Kim, N. S., and Lin, Y . Bns-gcn: Efficient full-graph training of graph convolutional net- works with partition-parallelism and random boundary node sampling. Proceedings of Machine Learning and Systems, 4:673–693, 2022a. Wan, C., Li, Y ., Wolfe, C. R., Kyrillidis, A., Kim, N. S., and Lin, Y . Pipegcn: Efficient full-graph training of graph convolutional networks with pipelined feature communi- cation. arXiv preprint arXiv:2203.10428, 2022b. Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X., Zhou, J., Ma, C., Yu, L., Gai, Y ., Xiao, T., He, T., Karypis, G., Li, J., and Zhang, Z. Deep graph library: A graph- centric, highly-performant package for graph neural net- works. arXiv preprint arXiv:1909.01315, 2019. Wang, Y ., Feng, B., and Ding, Y . Tc-gnn: Accelerating sparse graph neural network computation via dense tensor core on gpus. arXiv preprint arXiv:2112.02052, 2021. Wang, Z., Wu, X. C., Xu, Z., and Ng, T. E. Cupcake: Acom- pression optimizer for scalable communication-efficient distributed training. Wang, Z., Xu, Z., Wu, X., Shrivastava, A., and Ng, T. E. Dragonn: Distributed randomized approximate gradients of neural networks. In International Conference on Ma- chine Learning, pp. 23274–23291. PMLR, 2022. Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Wein- berger, K. Simplifying graph convolutional networks. In International conference on machine learning, pp. 6861– 6871. PMLR, 2019. Xu, K., Zhang, M., Jegelka, S., and Kawaguchi, K. Op- timization of graph neural networks: Implicit acceler- ation by skip connections and more depth. In Inter- national Conference on Machine Learning, pp. 11592– 11602. PMLR, 2021. Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., and Leskovec, J. Graph convolutional neural net- works for web-scale recommender systems. In Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974–983, 2018. Yu, L., Shen, J., Li, J., and Lerer, A. Scalable graph neu- ral networks for heterogeneous graphs. arXiv preprint arXiv:2011.09679, 2020. Yuan, B., Wolfe, C. R., Dun, C., Tang, Y ., Kyril- lidis, A., and Jermaine, C. Distributed learning of fully connected neural networks using independent sub- net training. Proc. VLDB Endow. , 15(8):1581–1590, 2022. URL https://www.vldb.org/pvldb/ vol15/p1581-wolfe.pdf. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V . Graphsaint: Graph sampling based in- ductive learning method. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=BJe8pkHFwS. Zha, D., Feng, L., Tan, Q., Liu, Z., Lai, K.-H., Bhushanam, B., Tian, Y ., Kejariwal, A., and Hu, X. Dreamshard: Gen- eralizable embedding table placement for recommender systems. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Pro- cessing Systems, 2022. URL https://openreview. net/forum?id=_atSgd9Np52. 11RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Zha, D., Feng, L., Luo, L., Bhushanam, B., Liu, Z., Hu, Y ., Nie, J., Huang, Y ., Tian, Y ., Kejariwal, A., and Hu, X. Pre- train and search: Efficient embedding table sharding with pre-trained neural cost models. CoRR, abs/2305.01868, 2023. doi: 10.48550/arXiv.2305.01868. URL https: //doi.org/10.48550/arXiv.2305.01868. Zhang, H., Yu, Z., Dai, G., Huang, G., Ding, Y ., Xie, Y ., and Wang, Y . Understanding gnn computational graph: A coordinated computation, io, and memory perspective. Proceedings of Machine Learning and Systems, 4:467– 484, 2022. Zheng, D., Ma, C., Wang, M., Zhou, J., Su, Q., Song, X., Gan, Q., Zhang, Z., and Karypis, G. Distdgl: distributed graph neural network training for billion-scale graphs. In 2020 IEEE/ACM 10th Workshop on Irregular Appli- cations: Architectures and Algorithms (IA3), pp. 36–44. IEEE, 2020. Zhong, S., Zhang, G., Huang, N., and Xu, S. Revisit kernel pruning with lottery regulated grouped convolutions. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=LdEhiMG9WLO. Zhou, K., Liu, Z., Chen, R., Li, L., Choi, S., and Hu, X. Table2graph: Transforming tabular data to unified weighted graph. In Raedt, L. D. (ed.), Proceedings of the Thirty-First International Joint Conference on Ar- tificial Intelligence, IJCAI 2022, Vienna, Austria, 23- 29 July 2022 , pp. 2420–2426. ijcai.org, 2022. doi: 10.24963/ijcai.2022/336. URL https://doi.org/ 10.24963/ijcai.2022/336. Zhou, K., Choi, S.-H., Liu, Z., Liu, N., Yang, F., Chen, R., Li, L., and Hu, X. Adaptive label smoothing to regularize large-scale graph training. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), pp. 55–63. SIAM, 2023. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. arXiv preprint arXiv:1911.07323, 2019. 12RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations A. Mathematical Analysis A.1. Why Sampling-based Approximation for GNN? In the main text, we mentioned SpMM is the main speed bottleneck for GNNs. Below we illustrate why the column- row sampling is suitable for accelerating SpMM in GNNs, from the approximation error perspective. Here we analyze ˜AJ(l) = SpMM( ˜A, J(l)) for illustration convenience. For the backward pass of SpMM , the analysis is similar, except that we are approximating ∇J(l) = SpMM( ˜A⊤, ∇H(l+1)). Column-row sampling approximates the matrix production by excluding some “unimportant” columns and rows in the original matrix. So intuitively, the approximation error E[|| ˜AJ(l) − approx( ˜AJ(l))||F ] is low if the “unimportant” columns/rows are correlated in the selected one. Namely, ˜A and J(l) are low-rank. Formally, we have the following theorem: Theorem A.1 ((Martinsson & Tropp, 2020)) . Suppose we approximate ˜AJ(l) using column-row sampling, and pi is obtained by Equation (3). Then for any positive number ϵ, if the number of samples k satisfies k ≥ ϵ−2(srank( ˜A) + srank(J(l))) log(|V| + d), we have E[|| ˜AJ(l) − approx( ˜AJ(l))||F ] ≤ 2ϵ, where srank in Theorem A.1 is called the stable rank, which is the continuous surrogate measure for the rank that is largely unaffected by tiny singular values. Formally for any matrix Y , srank(Y ) =||Y ||2 F ||Y ||2 ≤ rank(Y ). Fortunately, most real-world graphs are cluster-structured, which means the adjacency matrix ˜A is low-rank (Qiu et al., 2021; Savas & Dhillon, 2011). The low-rank property of real-world graphs is also wildly reported in previous work (Jin et al., 2020; Qiu et al., 2021). Moreover, the intermediate activations J(l) and the activation gradients are also low-rank, due to the aggregation. Namely, low-rank means “correlation” in the row/column space. The embedding (i.e., rows in the activation matrix) of connected nodes tend to close due to the graph propagation, which resulting in the low-rank property of the activation matrix. Thus for GNNs, the approximation error is low with a relatively small number of sample k. This perspective is also experimentally verified in the experiment section. A.2. Proof of Proposition 1 Proposition A.2 (Proof in Appendix A). If the approximation method is itself unbiased, and we only replace the SpMM in the backward pass with its approximated version, while leaving the forward one unchanged, then the calculated gradient is provably unbiased. Here we note that in the main text, for the notation convenience, we ignore the backward pass of ReLU. However, the proof here will consider the non-linear activation function to prove the unbiasedness. Let H(l+1) pre = SpMM( ˜A, J(l)) be the pre-activation. The backward pass of ReLU is: E[∇H(l+1) pre ] =E[1 H(l+1) pre >0 ⊙ ∇H(l+1)] = 1 H(l+1) pre >0 ⊙ E[∇H(l+1)], (5) where ⊙ is the element-wise product and 1 is the indicator function. The element-wise product is linear operation and 1 H(l+1) pre >0 is only related to the pre-activation in the forward pass, we only apply the approximation during the backward pass so 1 H(l+1) pre >0 can be extracted from the expectation. We know that for the last layer, we have E[∇H(L)] = H(L) since we do not apply ReLU at the output layer. We then can prove by induction that E[∇H(l+1)] = H(l+1) and E[∇J(l)] =E[approx( ˜A⊤∇H(l+1) pre )] =∇J(l) for any layer l. A.3. Analysis of MEAN aggregator For GraphSAGE, one commonly used aggregator is the MEAN aggregator, which can be expressed as follows: H(l+1) = W1H(l) + W2SpMM MEAN(A, H(l)), (6) 13RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations where SpMM MEAN is one variant of the vanilla SpMM , which replace the reducer function from sum(·) to mean(·). We note that in popular GNN packages, the MEAN aggregator usually is implemented based on SpMM MEAN (Fey & Lenssen, 2019; Wang et al., 2019) to reduce the memory usage. Here we give an example of SpMM MEAN to illustrate how it works: SpMM MEAN(   1 0 0 4 5 6  , \u00147 8 9 10 \u0015 ) = \"1 2 (1 × 7 + 0× 9) 1 2 (1 × 8 + 0× 10) 1 2 (0 × 7 + 4× 9) 1 2 (0 × 8 + 4× 10) 1 2 (5 × 7 + 6× 9) 1 2 (5 × 8 + 6× 10) # , Equivalently, the SpMM MEAN can also be expressed as: SpMM MEAN(A, H(l)) =D−1AH(l), where D is the degree matrix ofA. Thus, although we did not normalize the adjacency matrix in GraphSAGE, when applying the top-k sampling to approximate SpMM MEAN, the column norm of A:,ji is actually 1√ Degji due to the normalization. Also, for GraphSAGE, the inputs to the first SpMM MEAN operation are A and X. They do not require gradient since they are not trainable. Thus, the first SAGE layer is not presented in Figure 8 and Figure 7. B. Pseudo code of the greedy algorithm Algorithm 1 The greedy algorithm Inputs: Gradients of node embeddings{∇H(1), ···∇ H(L)}, adjacency matrix A, graph G = (V, E), hidden dimensions {d1, ··· dL}. Parameters: The step size α, the overall budget C. Outputs: The layer-wise {k1, ··· kL} associated with the top-k sampling. B ← PL l=1 |E|dl. ∀i, kl ← |V|, Topkl ← {1, ···|V|} . while B ≥ C PL l=1 |E|dl do m ← arg minl∈{1,···L}(P i∈Topkl ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥A∥F ∥∇H(l+1)∥F − P i∈Topkl−α|V| ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2) ∥A∥F ∥∇H(l+1)∥F /* Choose the layer m to reduce by a step size α|V|, such that the increment of errors is minimal. */ B ← B − dm P i∈Topkm∩i/∈Topkm−α|V| #nnzi /*Since we exclude some column-row pairs for layer m, here we reduce the budget B accordingly. */ km ← km − α|V| /* Update km accordingly. */ Topkm ← the set of indices i associated with km largest ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥A∥F ∥∇H(l+1)∥F /* Update Topkm accordingly. */ end while Return {k1, ··· , kL} In algorithm 1, here we provide the pseudo code of our greedy algorithm for solving the constrained optimization problem. In Table 11, we show the run time of the greedy algorithm, which is negligible compared to the acceleration effect. C. Extended Related works Connections to Graph Data Augmentation Data augmentation (Liu et al., 2021; Han et al., 2022) is wildly adopted in the graph learning for improving model generalization, including dropping nodes (Feng et al., 2020), dropping edges (Rong et al., 2019), and graph mixup (Han et al., 2022). As shown in Figure 5, the top- k sampling drops the entire columns in the adjacency matrix, while keeping the number of rows unchanged. That means RSC drops all of the out edges for a set of nodes. This can be viewed as the “structural dropedge” for improving the efficiency. Since we only apply the top-k sampling in the backward pass and top- k indices are different for each operation, RSC essentially forward pass with the whole graph, backward pass with different subgraphs at each layer. This structural dropedge and heterogeneous backward propagation introduce the regularization effect. Thus as shown in the experiment section, RSC may also improve the model accuracy over the baseline. 14RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Subgraph-based GNN training. The key idea of this line of work is to improve the scalability of GNNs by separating the graph into overlapped small batches, then training models with sampled subgraphs (Hamilton et al., 2017; Huang et al., 2018; Zou et al., 2019; Chiang et al., 2019; Zeng et al., 2020). Based on this idea, various sampling techniques have been proposed, including the node-wise sampling (Hamilton et al., 2017; Chen et al., 2017), layer-wise sampling (Huang et al., 2018; Zou et al., 2019), and subgraph sampling (Chiang et al., 2019; Zeng et al., 2020). However, this approach reduces the memory footprint but results in extra time cost to compute the overlapping nodes between batches. Generally, methods in this category are orthogonal to RSC , and they can be combined. Graph precomputation. The graph precomputation methods decouple the message passing from the model training, either as a preprocessing step (Wu et al., 2019; Klicpera et al., 2018; Yu et al., 2020) or post-processing step (Huang et al., 2020b), where the model is simplified as the Multi-Layer Perceptron (MLP). We did consider this line of work in this paper since the backbone model is not GNN anymore. Distributed GNN training. The distributed training leverages extra hardwares to increase the memory capacity and training efficiency (Zha et al., 2023; 2022; Yuan et al., 2022; Wang et al., 2022; Wang et al.). However, the graph data cannot be trivially divided into independent partitions due to the node connectivity. Thus, the graph distributed training frameworks propose to split graph into related partitions and minimize the communication overhead (Wan et al., 2021; 2022b; Ramezani et al., 2022). Our methods are orthogonal to this line of work. Other randomized GNN training. Dropedge (Rong et al., 2019) randomly drops edges to avoid the over-smoothing problem. Graph Random Neural Networks (Grand) (Feng et al., 2020) randomly drop nodes to generate data augmentation for improving model generalization. Grand+ improves the scalability over Grand by pre-computing a general propagation matrix and employ it to perform data augmentation (Feng et al., 2022). As shown in Section C, the key difference between GRAND(+) and RSC is that RSC does not drop any node. Instead RSC drops all of the out edges for a set of nodes only during backward pass. Moreover, the drop pattern are evolving during the training process. This can be viewed as the “structural dropedge”. However, unlike Dropedge (Rong et al., 2019), RSC drop the column-row pairs according to the euclidean norm instead of uniformly dropping. D. Experimental Settings D.1. Software and Hardware Descriptions All experiments are conducted on a server with four NVIDIA 3090 GPUs, four AMD EPYC 7282 CPUs, and 252GB host memory. We implement all models based on Pytorch and Pytorch Geometric. During our experiments, we found that the version of Pytorch, Pytorch Sparse, and Pytorch Scatter can significantly impact the running speed of the baseline. Here we list the details of our used packages in all experiments in Table 5. Table 5: Package configurations of our experiments. Package Version CUDA 11.1 pytorch sparse 0.6.12 pytorch scatter 2.0.8 pytorch geometric 1.7.2 pytorch 1.9.0 OGB 1.3.2 D.2. Statistics of benchmark datasets The statistics for all used datasets are shown in Table 6. We follow the standard data splits and all datasets are directly downloaded from Pytorch Geometric or the protocol of OGB (Hu et al., 2020). D.3. Hyperparameter Settings Regarding Reddit and Yelp dataset, we follow the hyperparameter reported in the respective papers as closely as possible. Regarding ogbn-proteins and ogbn-products dataset, we follow the hyperparameter configurations and codebases provided 15RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 6: Dataset Statistics. Dataset Task Nodes Edges Classes Label Rates Reddit multi-class 232,965 11,606,919 41 65.86% Yelp multi-label 716,847 6,977,409 100 75.00% ogbn-proteins binary-Class 132,534 39,561,252 2 65.00% ogbn-products multi-class 2,449,029 61,859,076 47 8.03% on the OGB (Hu et al., 2020) leader-board. Please refer to the OGB website for more details. The optimizer is Adam for all these models. All methods terminate after a fixed number of epochs. We report the test accuracy associated with the highest validation score. Table 10 summarize the hyperparameter configuration of GraphSAINT. Table 7, Table 8, and Table 9 summarize the hyperparameter configuration of full-Batch GCN, GraphSAGE, and GCNII, respectively. Table 7: Configuration of Full-Batch GCN. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 3 256 Yelp 0.01 500 0.1 Yes 3 512 ogbn- proteins 0.01 1000 0.5 No 3 256 ogbn- products 0.001 500 0.5 No 3 256 Table 8: Configuration of Full-Batch GraphSAGE. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 3 256 Yelp 0.01 500 0.1 Yes 3 512 ogbn- proteins 0.01 1000 0.5 No 3 256 ogbn- products 0.001 500 0.5 No 3 256 Table 9: Configuration of Full-Batch GCNII. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 4 256 Yelp 0.01 500 0.1 Yes 4 256 ogbn- proteins 0.01 1000 0.5 No 4 256 E. More experiment results The running time of the greedy algorithm is shown in 11. We also visualize the allocated kl for each layer across iterations in Figure 7, and the degree of picked nodes in Figure 8. Here we use Reddit dataset for the case study. We observe that the kl assigned by RSC evolves along with the training. 16RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 10: Training configuration of GraphSAINT. Dataset RandomWalk Sampler Training Archtecture Walk length Roots Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 4 8000 0.01 40 0.1 Yes 3 128 Yelp 2 8000 0.01 75 0.1 Yes 3 512 ogbn- products 3 60000 0.01 20 0.5 No 3 256 Table 11: The running time (second) of the greedy algorithm. Reddit Yelp ogbn- proteins ogbn- products GCN 0.03 0.03 0.03 0.03 GraphSAGE 0.02 0.02 0.03 0.03 GCNII 0.05 0.05 0.06 - /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000026/uni00000031/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000014/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000014/uni00000013 /uni00000013/uni00000011/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000015/uni00000013 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000014/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000017/uni00000057/uni0000004b/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 Figure 7: The allocated layer-wise kl for GCN, GraphSAGE and GCNII on Reddit, where budget C is set as 0.1. The input of the SpMM in the first GraphSAGE layer does not require gradient and thus absent in the Figure (Appendix A.3). 0 50 100 150 200 250 300 350 400 Epochs 50 100 150Node degrees GCN 1st layer GCN 2nd layer GCN 3rd layer 0 50 100 150 200 250 300 350 400 Epochs 20 40 60 80 100 120Node degrees GraphSAGE 2nd layer GraphSAGE 3rd layer 0 50 100 150 200 250 300 350 400 Epochs 50 100 150Node degrees GCNII 1st layer GCNII 2nd layer GCNII 3rd layer GCNII 4th layer Figure 8: The averaged degrees of nodes picked by top-k sampling along the whole training process, where the applied dataset is Reddit and overall budget C is set as 0.1. E.1. Additional Ablation Results to the Resource Allocation Algorithm (Figure 6) Due to the page limit, we present more ablation study on the resource allocation algorithm here. Specifically, in Figure 9, we compare RSC to the uniform allocation on ogbn-proteins dataset with GCN, GraphSAGE, and GCNII, respectively. In Figure 10, we compare RSC to the uniform allocation on Yelp dataset with GCN, GraphSAGE, and GCNII, respectively. We conclude that RSC generally outperforms the uniform allocation strategy. E.2. Hyperparameter Sensitivity Analysis Here we analyze the impacts of the main hyperparameters of RSC : (1) the budget C, which controls the efficiency-accuracy trade-off; (2) the step size α in the greedy Algorithm 1; (3) when switching back to the original sparse operations. In Figure 12, we vary only one of them with the others fixed. We conclude (1) larger budget C leads to better accuracy with smaller speedup, since we are using more computational resources to approximate the full operation. (2) larger step size α leads 17RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 1.01.21.41.61.8 Speedup 67 68 69 70 71 72Accuracy (%) GCN (ogbn-proteins) Uniform Allocation RSC 1.21.41.61.82.02.2 Speedup 70 71 72 73 74 75 76Accuracy (%) GraphSAGE (ogbn-proteins) Uniform Allocation RSC 1.01.21.41.61.8 Speedup 64 66 68 70 72 74Accuracy (%) GCNII (ogbn-proteins) Uniform Allocation RSC Figure 9: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. The dataset is ogbn-proteins. Here we disabled the caching and switch mechanism for a fair comparison. 0.960.981.001.021.041.061.081.10 Speedup 42 44 46 48Accuracy (%) GCN (Yelp) Uniform Allocation RSC 1.0001.0251.0501.0751.1001.1251.1501.175 Speedup 63.0 63.2 63.4 63.6 63.8 64.0Accuracy (%) GraphSAGE (Yelp) Uniform Allocation RSC 1.101.121.141.161.181.201.221.241.26 Speedup 64.0 64.1 64.2 64.3 64.4Accuracy (%) GCNII (Yelp) Uniform Allocation RSC Figure 10: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. The dataset is Yelp. Here we disabled the caching and switch mechanism for a fair comparison. to marginally larger speedup since the greedy algorithm will terminate earlier. Also the step size α does not affect the model accuracy a lot. In practice, we set α = 0.02|V|. (3) The later we switch back to the original operation, the larger the accuracy drop and the smaller the speedup, it is equivalent to using less resources to approximate the full operation epoch-wisely. Thus, we apply RSC for 80% of the total epochs to balance the trade-off. 0 100 200 300 400 Epochs 20 40 60 80Validation Accuracy GCN (Reddit) Baseline C=0.1 C=0.2 C=0.3 0 100 200 300 400 Epochs 20 40 60 80 100Validation Accuracy GCNII (Reddit) Baseline C=0.1 C=0.2 C=0.3 Figure 11: Learning curves for validation accuracy under different overall budget C on Reddit dataset. Here we disabled the caching and switching mechanism for ablating the effect of C. 0.1 0.2 0.3 0.4 0.5 (a) Budget C 73 74 75 76Test AUC Baseline AUC RSC AUC RSC Speedup 1.5 1.6 1.7 1.8 Speedup GraphSAGE (ogbn-proteins) 0.01| |  0.02| |  0.05| |  0.1| |  0.2| | (b) step size  75.9 76.0 76.1 76.2 76.3Test AUC Baseline AUC RSC AUC RSC Speedup 1.58 1.60 1.62 1.64 Speedup GraphSAGE (ogbn-proteins) At 60%  total epochs At 70%  total epochs At 80%  total epochs At 90%  total epochs At 95%  total epochs (c) When switching back to the original 75.50 75.75 76.00 76.25 76.50Test AUC Baseline AUC RSC AUC RSC Speedup 1.45 1.50 1.55 1.60 1.65 1.70 Speedup GraphSAGE (ogbn-proteins) Figure 12: Hyperparameter analysis w.r.t. the budget C, the step size α in Algorithm 1, and when switching back to the original operations. The model is GraphSAGE and the applied dataset is ogbn-proteins. 18",
      "meta_data": {
        "arxiv_id": "2210.10737v2",
        "authors": [
          "Zirui Liu",
          "Shengyuan Chen",
          "Kaixiong Zhou",
          "Daochen Zha",
          "Xiao Huang",
          "Xia Hu"
        ],
        "published_date": "2022-10-19T17:25:33Z",
        "pdf_url": "https://arxiv.org/pdf/2210.10737v2.pdf"
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      }
    },
    {
      "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method",
      "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning\nrepresentations of attributed graphs. To scale GCNs to large graphs,\nstate-of-the-art methods use various layer sampling techniques to alleviate the\n\"neighbor explosion\" problem during minibatch training. We propose GraphSAINT,\na graph sampling based inductive learning method that improves training\nefficiency and accuracy in a fundamentally different way. By changing\nperspective, GraphSAINT constructs minibatches by sampling the training graph,\nrather than the nodes or edges across GCN layers. Each iteration, a complete\nGCN is built from the properly sampled subgraph. Thus, we ensure fixed number\nof well-connected nodes in all layers. We further propose normalization\ntechnique to eliminate bias, and sampling algorithms for variance reduction.\nImportantly, we can decouple the sampling from the forward and backward\npropagation, and extend GraphSAINT with many architecture variants (e.g., graph\nattention, jumping connection). GraphSAINT demonstrates superior performance in\nboth accuracy and training time on five large graphs, and achieves new\nstate-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).",
      "full_text": "Published as a conference paper at ICLR 2020 GraphSAINT: G RAPH SAMPLING BASED INDUCTIVE LEARNING METHOD Hanqing Zeng∗ University of Southern California zengh@usc.edu Hongkuan Zhou∗ University of Southern California hongkuaz@usc.edu Ajitesh Srivastava University of Southern California ajiteshs@usc.edu Rajgopal Kannan US Army Research Lab rajgopal.kannan.civ@mail.mil Viktor Prasanna University of Southern California prasanna@usc.edu ABSTRACT Graph Convolutional Networks (GCNs) are powerful models for learning repre- sentations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use variouslayer samplingtechniques to alleviate the “neighbor explosion” problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efﬁciency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure ﬁxed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the for- ward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on ﬁve large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). 1 I NTRODUCTION Recently, representation learning on graphs has attracted much attention, since it greatly facilitates tasks such as classiﬁcation and clustering (Wu et al., 2019; Cai et al., 2017). Current works on Graph Convolutional Networks (GCNs) (Hamilton et al., 2017; Chen et al., 2018b; Gao et al., 2018; Huang et al., 2018; Chen et al., 2018a) mostly focus on shallow models (2 layers) on relatively small graphs. Scaling GCNs to larger datasets and deeper layers still requires fast alternate training methods. In a GCN, data to be gathered for one output node comes from its neighbors in the previous layer. Each of these neighbors in turn, gathers its output from the previous layer, and so on. Clearly, the deeper we back track, the more multi-hop neighbors to support the computation of the root. The number of support nodes (and thus the training time) potentially grows exponentially with the GCN depth. To mitigate such “neighbor explosion”, state-of-the-art methods use variouslayer sampling techniques. The works by Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a) ensure that only a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. Chen et al. (2018b) and Huang et al. (2018) further propose samplers to restrict the neighbor expansion factor to 1, by ensuring a ﬁxed sample size in all layers. While these methods signiﬁcantly speed up training, they face challenges in scalability, accuracy or computation complexity. ∗Equal contribution 1 arXiv:1907.04931v4  [cs.LG]  16 Feb 2020Published as a conference paper at ICLR 2020 Present work We present GraphSAINT (Graph SAmpling based INductive learning meThod) to efﬁciently train deep GCNs. GraphSAINT is developed from a fundamentally different way of minibatch construction. Instead of building a GCN on the full training graph and then sampling across the layers, we sample the training graph ﬁrst and then build a full GCN on the subgraph. Our method is thus graph samplingbased. Naturally, GraphSAINT resolves “neighbor explosion”, since every GCN of the minibatches is a small yet complete one. On the other hand, graph sampling based method also brings new challenges in training. Intuitively, nodes of higher inﬂuence on each other should have higher probability to form a subgraph. This enables the sampled nodes to “support” each other without going outside the minibatch. Unfortunately, such strategy results in non-identical node sampling probability, and introduces bias in the minibatch estimator. To address this issue, we develop normalization techniques so that the feature learning does not give preference to nodes more frequently sampled. To further improve training quality, we perform variance reduction analysis, and design light-weight sampling algorithms by quantifying “inﬂuence” of neighbors. Experiments on GraphSAINT using ﬁve large datasets show signiﬁcant performance gain in both training accuracy and time. We also demonstrate the ﬂexibility of GraphSAINT by integrating our minibatch method with popular GCN architectures such as JK-net (Xu et al., 2018) and GAT (Veliˇckovi´c et al., 2017). The resulting deep models achieve new state-of-the-art F1 scores on PPI (0.995) and Reddit (0.970). 2 R ELATED WORK A neural network model that extends convolution operation to the graph domain is ﬁrst proposed by Bruna et al. (2013). Further, Kipf & Welling (2016); Defferrard et al. (2016) speed up graph convolution computation with localized ﬁlters based on Chebyshev expansion. They target relatively small datasets and thus the training proceeds in full batch. In order to scale GCNs to large graphs, layer sampling techniques (Hamilton et al., 2017; Chen et al., 2018b; Ying et al., 2018a; Chen et al., 2018a; Gao et al., 2018; Huang et al., 2018) have been proposed for efﬁcient minibatch training. All of them follow the three meta steps: 1. Construct a complete GCN on the full training graph. 2. Sample nodes or edges of each layer to form minibatches. 3. Propagate forward and backward among the sampled GCN. Steps (2) and (3) proceed iteratively to update the weights via stochastic gradient descent. The layer sampling algorithm of GraphSAGE (Hamilton et al., 2017) performs uniform node sampling on the previous layer neighbors. It enforces a pre-deﬁned budget on the sample size, so as to bound the minibatch computation complexity. Ying et al. (2018a) enhances the layer sampler of Hamilton et al. (2017) by introducing an importance score to each neighbor. The algorithm presumably leads to less information loss due to weighted aggregation. S-GCN (Chen et al., 2018a) further restricts neighborhood size by requiring only two support nodes in the previous layer. The idea is to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN (Chen et al., 2018b) performs sampling from another perspective. Instead of tracking down the inter-layer connections, node sampling is performed independently for each layer. It applies importance sampling to reduce variance, and results in constant sample size in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. Huang et al. (2018) improves FastGCN by an additional sampling neural network. It ensures high accuracy, since sampling is conditioned on the selected nodes in the next layer. Signiﬁcant overhead may be incurred due to the expensive sampling algorithm and the extra sampler parameters to be learned. Instead of sampling layers, the works of Zeng et al. (2018) and Chiang et al. (2019) build mini- batches from subgraphs. Zeng et al. (2018) proposes a speciﬁc graph sampling algorithm to ensure connectivity among minibatch nodes. They further present techniques to scale such training on shared-memory multi-core platforms. More recently, ClusterGCN (Chiang et al., 2019) proposes graph clustering based minibatch training. During pre-processing, the training graph is partitioned into densely connected clusters. During training, clusters are randomly selected to form minibatches, and intra-cluster edge connections remain unchanged. Similar to GraphSAINT, the works of Zeng et al. (2018) and Chiang et al. (2019) do not sample the layers and thus “neighbor explosion” is avoided. Unlike GraphSAINT, both works are heuristic based, and do not account for bias due to the unequal probability of each node / edge appearing in a minibatch. Another line of research focuses on improving model capacity. Applying attention on graphs, the architectures of Veliˇckovi´c et al. (2017); Zhang et al. (2018); Lu et al. (2019) better capture neighbor features by dynamically adjusting edge weights. Klicpera et al. (2018) combines PageRank with GCNs to enable efﬁcient information propagation from many hops away. To develop deeper models, 2Published as a conference paper at ICLR 2020 “skip-connection” is borrowed from CNNs (He et al., 2015; Huang et al., 2017) into the GCN context. In particular, JK-net Xu et al. (2018) demonstrates signiﬁcant accuracy improvement on GCNs with more than two layers. Note, however, that JK-net (Xu et al., 2018) follows the same sampling strategy as GraphSAGE (Hamilton et al., 2017). Thus, its training cost is high due to neighbor explosion. In addition, high order graph convolutional layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) also help propagate long-distance features. With the numerous architectural variants developed, the question of how to train them efﬁciently via minibatches still remains to be answered. 3 P ROPOSED METHOD : GraphSAINT Graph sampling based method is motivated by the challenges in scalability (in terms of model depth and graph size). We analyze the bias (Section 3.2) and variance (Section 3.3) introduced by graph sampling, and thus, propose feasible sampling algorithms (Section 3.4). We show the applicability of GraphSAINT to other architectures, both conceptually (Section 4) and experimentally (Section 5.2). In the following, we deﬁne the problem of interest and the corresponding notations. A GCN learns representation of an un-directed, attributed graph G(V,E), where each node v ∈V has a length-f attribute xv. Let A be the adjacency matrix and ˜A be the normalized one (i.e., ˜A = D−1A, and D is the diagonal degree matrix). Let the dimension of layer-ℓinput activation be f(ℓ). The activation of node v is x(ℓ) v ∈Rf(ℓ) , and the weight matrix is W(ℓ) ∈Rf(ℓ)×f(ℓ+1) . Note that xv = x(1) v . Propagation rule of a layer is deﬁned as follows: x(ℓ+1) v = σ (∑ u∈V ˜Av,u ( W(ℓ) )T x(ℓ) u ) (1) where ˜Av,u is a scalar, taking an element of ˜A. And σ(·) is the activation function (e.g., ReLU). We use subscript “s” to denote parameterd of the sampled graph (e.g.,Gs, Vs, Es). GCNs can be applied under inductive and transductive settings. While GraphSAINT is applicable to both, in this paper, we focus on inductive learning. It has been shown that inductive learning is especially challenging (Hamilton et al., 2017) — during training, neither attributes nor connections of the test nodes are present. Thus, an inductive model has to generalize to completely unseen graphs. 3.1 M INIBATCH BY GRAPH SAMPLING 0 1 2 3 4 5 6 8 9 7 0 1 2 3 5 7 0 1 2 3 5 7 0 1 2 3 5 7 Gs = SAMPLE(G) Full GCN on Gs Figure 1: GraphSAINT training algorithm GraphSAINT follows the design philosophy of directly sampling the training graph G, rather than the corresponding GCN. Our goals are to 1. extract appropriately connected subgraphs so that little information is lost when propagating within the subgraphs, and 2. combine information of many subgraphs together so that the training process overall learns good representation of the full graph. Figure 1 and Algorithm 1 illustrate the training algorithm. Before training starts, we perform light-weight pre-processing on Gwith the given sampler SAMPLE. The pre-processing estimates the probability of a node v∈V and an edge e∈E being sampled by SAMPLE. Such probability is later used to normalize the subgraph neighbor aggregation and the minibatch loss (Section 3.2). Afterwards, 3Published as a conference paper at ICLR 2020 Algorithm 1GraphSAINT training algorithm Input: Training graph G(V,E,X); Labels Y ; Sampler SAMPLE; Output: GCN model with trained weights 1: Pre-processing: Setup SAMPLE parameters; Compute normalization coefﬁcients α, λ. 2: for each minibatch do 3: Gs (Vs,Es) ←Sampled sub-graph of Gaccording to SAMPLE 4: GCN construction on Gs. 5: {yv |v∈Vs}← Forward propagation of {xv |v∈Vs}, normalized by α 6: Backward propagation from λ-normalized loss L(yv,yv). Update weights. 7: end for training proceeds by iterative weight updates via SGD. Each iteration starts with an independently sampled Gs (where |Vs| ≪|V|). We then build a full GCN on Gs to generate embedding and calculate loss for every v∈Vs. In Algorithm 1, node representation is learned by performing node classiﬁcation in the supervised setting, and each training node vcomes with a ground truth label yv. Intuitively, there are two requirements for SAMPLE: 1. Nodes having high inﬂuence on each other should be sampled in the same subgraph. 2. Each edge should have non-negligible probability to be sampled. For requirement (1), an ideal SAMPLE would consider the joint information from node connections as well as attributes. However, the resulting algorithm may have high complexity as it would need to infer the relationships between features. For simplicity, we deﬁne “inﬂuence” from the graph connectivity perspective and design topology based samplers. Requirement (2) leads to better generalization since it enables the neural net to explore the full feature and label space. 3.2 N ORMALIZATION A sampler that preserves connectivity characteristic of Gwill almost inevitably introduce bias into minibatch estimation. In the following, we present normalization techniques to eliminate biases. Analysis of the complete multi-layer GCN is difﬁcult due to non-linear activations. Thus, we analyze the embedding of each layer independently. This is similar to the treatment of layers independently by prior work (Chen et al., 2018b; Huang et al., 2018). Consider a layer-(ℓ+ 1)node vand a layer-ℓ node u. If vis sampled (i.e., v∈Vs), we can compute the aggregated feature of vas: ζ(ℓ+1) v = ∑ u∈V ˜Av,u αu,v ( W(ℓ) )T x(ℓ) u 1 u|v = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u 1 u|v, (2) where ˜x(ℓ) u = ( W(ℓ))T x(ℓ) u , and 1 u|v ∈{0,1}is the indicator function given vis in the subgraph (i.e., 1 u|v = 0if v∈Vs ∧(u,v) ̸∈Es; 1 u|v = 1if (u,v) ∈Es; 1 u|v not deﬁned if v̸∈Vs). We refer to the constant αu,v as aggregator normalization. Deﬁne pu,v = pv,u as the probability of an edge (u,v) ∈E being sampled in a subgraph, and pv as the probability of a node v∈V being sampled. Proposition 3.1. ζ(ℓ+1) v is an unbiased estimator of the aggregation ofvin the full(ℓ+ 1)th GCN layer, ifαu,v = pu,v pv . i.e.,E ( ζ(ℓ+1) v ) = ∑ u∈V ˜Av,u ˜x(ℓ) u . Assuming that each layer independently learns an embedding, we use Proposition 3.1 to normalize feature propagation of each layer of the GCN built byGraphSAINT. Further, let Lv be the loss on vin the output layer. The minibatch loss is calculated as Lbatch = ∑ v∈Gs Lv/λv, where λv is a constant that we term loss normalization. We set λv = |V|· pv so that: E(Lbatch) = 1 |G| ∑ Gs∈G ∑ v∈Vs Lv λv = 1 |V| ∑ v∈V Lv. (3) Feature propagation within subgraphs thus requires normalization factors αand λ, which are com- puted based on the edge and node probability pu,v, pv. In the case of random node or random edge samplers, pu,v and pv can be derived analytically. For other samplers in general, closed form expression is hard to obtain. Thus, we perform pre-processing for estimation. Before training starts, 4Published as a conference paper at ICLR 2020 we run the sampler repeatedly to obtain a set of N subgraphs G. We setup a counter Cv and Cu,v for each v∈V and (u,v) ∈E, to count the number of times the node or edge appears in the subgraphs of G. Then we set αu,v = Cu,v Cv = Cv,u Cv and λv = Cv N . The subgraphs Gs ∈G can all be reused as minibatches during training. Thus, the overhead of pre-processing is small (see Appendix D.2). 3.3 V ARIANCE We derive samplers for variance reduction. Letebe the edge connectingu, v, and b(ℓ) e = ˜Av,u ˜x(ℓ−1) u + ˜Au,v ˜x(ℓ−1) v . It is desirable that variance of all estimators ζ(ℓ) v is small. With this objective, we deﬁne: ζ = ∑ ℓ ∑ v∈Gs ζ(ℓ) v pv = ∑ ℓ ∑ v,u ˜Av,u pvαu,v ˜x(ℓ) u 1 v1 u|v = ∑ ℓ ∑ e b(ℓ) e pe 1 (ℓ) e . (4) where 1 e = 1if e∈Es; 1 e = 0if e̸∈Es. And 1 v = 1if v∈Vs; 1 v = 0if v̸∈Vs. The factor pu in the ﬁrst equality is present so that ζis an unbiased estimator of the sum of all node aggregations at all layers: E(ζ) =∑ ℓ ∑ v∈VE ( ζ(ℓ) v ) . Note that 1 (ℓ) e = 1 e,∀ℓ, since once an edge is present in the sampled graph, it is present in all layers of our GCN. We deﬁne the optimal edge sampler to minimize variance for every dimension of ζ. We restrict ourselves to independent edge sampling. For each e∈E, we make independent decision on whether it should be in Gs or not. The probability of including eis pe. We further constrain ∑pe = m, so that the expected number of sampled edges equals to m. The budget mis a given sampling parameter. Theorem 3.2. Under independent edge sampling with budgetm, the optimal edge probabilities to minimize the sum of variance of eachζ’s dimension is given by:pe = m∑ e′ ∑ ℓ b(ℓ) e′  ∑ ℓ b(ℓ) e . To prove Theorem 3.2, we make use of the independence among graph edges, and the dependence among layer edges to obtain the covariance of 1 (ℓ) e . Then using the fact that sum of pe is a constant, we use the Cauchy-Schwarz inequality to derive the optimal pe. Details are in Appendix A. Note that calculating b(ℓ) e requires computing ˜x(ℓ−1) v , which increases the complexity of sampling. As a reasonable simpliﬁcation, we ignore ˜x(ℓ) v to make the edge probability dependent on the graph topology only. Therefore, we choose pe ∝ ˜Av,u + ˜Au,v = 1 deg(u) + 1 deg(v) . The derived optimal edge sampler agrees with the intuition in Section 3.1. If two nodes u, v are connected and they have few neighbors, then uand vare likely to be inﬂuential to each other. In this case, the edge probability pu,v = pv,u should be high. The above analysis on edge samplers also inspires us to design other samplers, which are presented in Section 3.4. Remark We can also apply the above edge sampler to perform layer sampling. Under the indepen- dent layer sampling assumption of Chen et al. (2018b), one would sample a connection ( u(ℓ),v(ℓ+1)) with probability p(ℓ) u,v ∝ 1 deg(u) + 1 deg(v) . For simplicity, assume a uniform degree graph (of degree d). Then p(ℓ) e = p. For an already sampled u(ℓ) to connect to layer ℓ+ 1, at least one of its edges has to be selected by the layer ℓ+ 1sampler. Clearly, the probability of an input layer node to “survive” the Lnumber of independent sampling process is ( 1 −(1 −p)d )L−1 . Such layer sampler potentially returns an overly sparse minibatch for L> 1. On the other hand, connectivity within a minibatch of GraphSAINT never drops with GCN depth. If an edge is present in layer ℓ, it is present in all layers. 3.4 S AMPLERS Based on the above variance analysis, we present several light-weight and efﬁcient samplers that GraphSAINT has integrated. Detailed sampling algorithms are listed in Appendix B. Random node sampler We sample |Vs|nodes from Vrandomly, according to a node probability distribution P(u) ∝ ˜A:,u  2 . This sampler is inspired by the layer sampler of Chen et al. (2018b). 5Published as a conference paper at ICLR 2020 Random edge sampler We perform edge sampling as described in Section 3.3. Random walk based samplersAnother way to analyze graph sampling based multi-layer GCN is to ignore activations. In such case, Llayers can be represented as a single layer with edge weights given by B = ˜AL. Following a similar approach as Section 3.3, if it were possible to pick pairs of nodes (whether or not they are directly connected in the original ˜A) independently, then we would set pu,v ∝Bu,v + Bv,u, where Bu,v can be interpreted as the probability of a random walk to start at uand end at v in Lhops (and Bv,u vice-versa). Even though it is not possible to sample a subgraph where such pairs of nodes are independently selected, we still consider a random walk sampler with walk length Las a good candidate for L-layer GCNs. There are numerous random walk based samplers proposed in the literature (Ribeiro & Towsley, 2010; Leskovec & Faloutsos, 2006; Hu & Lau, 2013; Li et al., 2015). In the experiments, we implement a regular random walk sampler (with rroot nodes selected uniformly at random and each walker goes hhops), and also a multi-dimensional random walk sampler deﬁned in Ribeiro & Towsley (2010). For all the above samplers, we return the subgraph induced from the sampled nodes. The induction step adds more connections into the subgraph, and empirically helps improve convergence. 4 D ISCUSSION Extensions GraphSAINT admits two orthogonal extensions. First, GraphSAINT can seamlessly integrate other graph samplers. Second, the idea of training by graph sampling is applicable to many GCN architecture variants: 1. Jumping knowledge(Xu et al., 2018): since our GCNs constructed during training are complete, applying skip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods (Chen et al., 2018b; Huang et al., 2018), extra modiﬁcation to their samplers is required, since the jumping knowledge architecture requires layer-ℓ samples to be a subset of layer-(ℓ−1) samples∗. 2. Attention (Veliˇckovi´c et al., 2017; Fey, 2019; Zhang et al., 2018): while explicit variance reduction is hard due to the dynamically updated attention values, it is reasonable to apply attention within the subgraphs which are considered as representatives of the full graph. Our loss and aggregator normalizations are also applicable†. 3. Others: To support high order layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) or even more complicated networks for the task of graph classiﬁcation (Ying et al., 2018b), we replace the full adjacency matrix A with the (normalized) one for the subgraph As to perform layer propagation. Comparison GraphSAINT enjoys: 1. high scalability and efﬁciency, 2. high accuracy, and 3. low training complexity. Point (1) is due to the signiﬁcantly reduced neighborhood size compared with Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a). Point (2) is due to the better inter- layer connectivity compared with Chen et al. (2018b), and unbiased minibatch estimator compared with Chiang et al. (2019). Point (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of Huang et al. (2018) and clustering of Chiang et al. (2019). 5 E XPERIMENTS Setup Experiments are under the inductive, supervised learning setting. We evaluate GraphSAINT on the following tasks: 1. classifying protein functions based on the interactions of human tissue proteins (PPI), 2. categorizing types of images based on the descriptions and common properties of online images (Flickr), 3. predicting communities of online posts based on user comments (Reddit), 4. categorizing types of businesses based on customer reviewers and friendship (Yelp), and 5. classifying product categories based on buyer reviewers and interactions (Amazon). For PPI, we use the small version for the two layer convergence comparison (Table 2 and Figure 2), since Hamilton et al. (2017) and Chen et al. (2018a) report accuracy for this version in their original papers. We use the large version for additional comparison with Chiang et al. (2019) to be consistent with its reported accuracy. All datasets follow “ﬁxed-partition” splits. Appendix C.2 includes further details. ∗The skip-connection design proposed by Huang et al. (2018) does not have such “subset” requirement, and thus is compatible with both graph sampling and layer sampling based methods. †When applying GraphSAINT to GAT (Veliˇckovi´c et al., 2017), we remove the softmax step which normalizes attention values within the same neighborhood, as suggested by Huang et al. (2018). See Appendix C.3. 6Published as a conference paper at ICLR 2020 Table 1: Dataset statistics (“m” stands formulti-class classiﬁcation, and “s” for single-class.) Dataset Nodes Edges Degree Feature Classes Train / Val / Test PPI 14,755 225,270 15 50 121 (m) 0.66 / 0.12 / 0.22 Flickr 89,250 899,756 10 500 7 (s) 0.50 / 0.25 / 0.25 Reddit 232,965 11,606,919 50 602 41 (s) 0.66 / 0.10 / 0.24 Yelp 716,847 6,977,410 10 300 100 (m) 0.75 / 0.10 / 0.15 Amazon 1,598,960 132,169,734 83 200 107 (m) 0.85 / 0.05 / 0.10 PPI (large version) 56,944 818,716 14 50 121 (m) 0.79 / 0.11 / 0.10 We open source GraphSAINT‡. We compare with six baselines: 1. vanilla GCN (Kipf & Welling, 2016), 2. GraphSAGE (Hamilton et al., 2017), 3. FastGCN (Chen et al., 2018b), 4. S-GCN (Chen et al., 2018a), 5. AS-GCN (Huang et al., 2018), and 6. ClusterGCN (Chiang et al., 2019). All baselines are executed with their ofﬁcially released code (see Appendix C.3 for downloadable URLs and commit numbers). Baselines and GraphSAINT are all implemented in Tensorﬂow with Python3. We run experiments on a NVIDIA Tesla P100 GPU (see Appendix C.1 for hardware speciﬁcation). 5.1 C OMPARISON WITH STATE-OF-THE -ART Table 2 and Figure 2 show the accuracy and convergence comparison of various methods. All results correspond to two-layer GCN models (for GraphSAGE, we use its mean aggregator). For a given dataset, we keep hidden dimension the same across all methods. We describe the detailed architecture and hyperparameter search procedure in Appendix C.3. The mean and conﬁdence interval of the accuracy values in Table 2 are measured by three runs under the same hyperparameters. The training time of Figure 2 excludes the time for data loading, pre-processing, validation set evaluation and model saving. Our pre-processing incurs little overhead in training time. See Appendix D.2 for cost of graph sampling. For GraphSAINT, we implement the graph samplers described in Section 3.4. In Table 2, “Node” stands for random node sampler; “Edge” stands for random edge sampler; “RW” stands for random walk sampler; “MRW” stands for multi-dimensional random walk sampler. Table 2: Comparison of test set F1-micro score with state-of-the-art methods Method PPI Flickr Reddit Yelp Amazon GCN 0.515 ±0.006 0.492 ±0.003 0.933 ±0.000 0.378 ±0.001 0.281 ±0.005 GraphSAGE 0.637 ±0.006 0.501 ±0.013 0.953 ±0.001 0.634 ±0.006 0.758 ±0.002 FastGCN 0.513 ±0.032 0.504 ±0.001 0.924 ±0.001 0.265 ±0.053 0.174 ±0.021 S-GCN 0.963 ±0.010 0.482 ±0.003 0.964 ±0.001 0.640 ±0.002 — ‡ AS-GCN 0.687 ±0.012 0.504 ±0.002 0.958 ±0.001 — ‡ — ‡ ClusterGCN 0.875 ±0.004 0.481 ±0.005 0.954 ±0.001 0.609 ±0.005 0.759 ±0.008 GraphSAINT-Node 0.960±0.001 0.507 ±0.001 0.962 ±0.001 0.641 ±0.000 0.782 ±0.004 GraphSAINT-Edge 0.981±0.007 0.510 ±0.002 0.966±0.001 0.653±0.003 0.807 ±0.001 GraphSAINT-RW 0.981±0.004 0.511±0.001 0.966±0.001 0.653±0.003 0.815±0.001 GraphSAINT-MRW 0.980±0.006 0.510 ±0.001 0.964 ±0.000 0.652 ±0.001 0.809 ±0.001 Table 3: Additional comparison with ClusterGCN (test set F1-micro score) PPI (large version) Reddit 2 ×512 5 ×2048 2 ×128 4 ×128 ClusterGCN 0.903 ±0.002 0.994 ±0.000 0.954 ±0.001 0.966 ±0.001 GraphSAINT 0.941±0.003 0.995±0.000 0.966±0.001 0.970±0.001 ‡Open sourced code: https://github.com/GraphSAINT/GraphSAINT ‡The codes throw runtime error on the large datasets (Yelp or Amazon). 7Published as a conference paper at ICLR 2020 0 20 40 600.4 0.6 0.8 1 Validation F1-micro PPI 0 10 20 30 40 0.44 0.46 0.48 0.5 0.52 Flickr 0 50 100 1500.9 0.92 0.94 0.96 0.98 Reddit 0 200 400 600 800 0.25 0.45 0.65 Yelp 0 200 400 0.2 0.4 0.6 0.8 Training time (second) Amazon GCN GraphSAGE FastGCN* S-GCN AS-GCN ClusterGCN GraphSAINT *: CPU execution time -RW Figure 2: Convergence curves of 2-layer models on GraphSAINT and baselines Clearly, with appropriate graph samplers, GraphSAINT achieves signiﬁcantly higher accuracy on all datasets. For GraphSAINT-Node, we use the same node probability as FastGCN. Thus, the accuracy improvement is mainly due to the switching from layer sampling to graph sampling (see “Remark” in Section 3.3). Compared with AS-GCN, GraphSAINT is signiﬁcantly faster. The sampler of AS-GCN is expensive to execute, making its overall training time even longer than vanilla GCN. We provide detailed computation complexity analysis on the sampler in Appendix D.2. For S-GCN on Reddit, it achieves similar accuracy as GraphSAINT, at the cost of over 9×longer training time. The released code of FastGCN only supports CPU execution, so its convergence curve is dashed. Table 3 presents additional comparison with ClusterGCN. We useL×f to specify the architecture, where Land f denote GCN depth and hidden dimension, respectively. The four architectures are the ones used in the original paper (Chiang et al., 2019). Again, GraphSAINT achieves signiﬁcant accuracy improvement. To train models with L> 2 often requires additional architectural tweaks. ClusterGCN uses its diagonal enhancement technique for the 5-layer PPI and 4-layer Reddit models. GraphSAINT uses jumping knowledge connection (Xu et al., 2018) for 4-layer Reddit. Evaluation on graph samplers From Table 2, random edge and random walk based samplers achieve higher accuracy than the random node sampler. Figure 3 presents sensitivity analysis on parameters of “RW”. We use the same hyperparameters (except the sampling parameters) and network architecture as those of the “RW” entries in Table 2. We ﬁx the length of each walker to2 (i.e., GCN depth), and vary the number of roots rfrom 250 to 2250. For PPI, increasing rfrom 250 to 750 signiﬁcantly improves accuracy. Overall, for all datasets, accuracy stabilizes beyond r= 750. 5.2 GraphSAINT ON ARCHITECTURE VARIANTS AND DEEP MODELS In Figure 4, we train a 2-layer and a 4-layer model of GAT (Veliˇckovi´c et al., 2017) and JK-net (Xu et al., 2018), by using minibatches of GraphSAGE and GraphSAINT respectively. On the two 4-layer architectures, GraphSAINT achieves two orders of magnitude speedup than GraphSAGE, indicating much better scalability on deep models. From accuracy perspective, 4-layer GAT-SAGE and JK- SAGE do not outperform the corresponding 2-layer versions, potentially due to the smoothening effect caused by the massive neighborhood size. On the other hand, with minibatches returned by our edge sampler, increasing model depth of JK-SAINT leads to noticeable accuracy improvement (from 0.966 of 2-layer to 0.970 of 4-layer). Appendix D.1 contains additional scalability results. 6 C ONCLUSION We have presented GraphSAINT, a graph sampling based training method for deep GCNs on large graphs. We have analyzed bias and variance of the minibatches deﬁned on subgraphs, and proposed 8Published as a conference paper at ICLR 2020 0 1,000 2,0000.4 0.6 0.8 1 Number of walkers Test F1-micro PPI Flickr Reddit Yelp Amazon Figure 3: Sensitivity analysis 100 102 104 0.93 0.94 0.95 0.96 0.97 Training time (second) Validation F1-micro GAT 100 102 104 JK-net GraphSAINT 2-layer GraphSAINT 4-layer GraphSAGE 2-layer GraphSAGE 4-layer Figure 4: GraphSAINT with JK-net and GAT (Reddit) normalization techniques and sampling algorithms to improve training quality. We have conducted extensive experiments to demonstrate the advantage of GraphSAINT in accuracy and training time. An interesting future direction is to develop distributed training algorithms using graph sampling based minibatches. After partitioning the training graph in distributed memory, sampling can be performed independently on each processor. Afterwards, training on the self-supportive subgraphs can signiﬁcantly reduce the system-level communication cost. To ensure the overall convergence quality, data shufﬂing strategy for the graph nodes and edges can be developed together with each speciﬁc graph sampler. Another direction is to perform algorithm-system co-optimization to accelerate the training of GraphSAINT on heterogeneous computing platforms (Zeng et al., 2018; Zeng & Prasanna, 2019). The resolution of “neighbor explosion” by GraphSAINT not only reduces the training computation complexity, but also improves hardware utilization by signiﬁcantly less data trafﬁc to the slow memory. In addition, task-level parallelization is easy since the light-weight graph sampling is completely decoupled from the GCN layer propagation. ACKNOWLEDGEMENT This material is based on work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract Number FA8750-17-C-0086 and National Science Foundation (NSF) under Contract Numbers CCF-1919289 and OAC-1911229. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of DARPA or NSF. REFERENCES Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard, Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolution architec- tures via sparsiﬁed neighborhood mixing. arXiv preprint arXiv:1905.00067, 2019. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. CoRR, abs/1312.6203, 2013. URL http://arxiv.org/abs/ 1312.6203. HongYun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques and applications. CoRR, abs/1709.07604, 2017. URL http://arxiv.org/abs/1709.07604. Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In ICML, pp. 941–949, 2018a. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR), 2018b. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An ef- ﬁcient algorithm for training deep and large graph convolutional networks. CoRR, abs/1905.07953, 2019. URL http://arxiv.org/abs/1905.07953. 9Published as a conference paper at ICLR 2020 Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016. Matthias Fey. Just jump: Dynamic neighborhood aggregation in graph neural networks. CoRR, abs/1904.04849, 2019. URL http://arxiv.org/abs/1904.04849. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, pp. 1416–1424, New York, NY , USA, 2018. ACM. ISBN 978-1-4503-5552-0. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30, pp. 1024–1034. 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. Pili Hu and Wing Cheong Lau. A survey and taxonomy of graph sampling. arXiv preprint arXiv:1308.5865, 2013. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558–4567, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907. Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Personalized embedding propa- gation: Combining neural networks on graphs with personalized pagerank. CoRR, abs/1810.05997, 2018. URL http://arxiv.org/abs/1810.05997. John Boaz Lee, Ryan A. Rossi, Xiangnan Kong, Sungchul Kim, Eunyee Koh, and Anup Rao. Higher- order graph convolutional networks. CoRR, abs/1809.07697, 2018. URL http://arxiv.org/ abs/1809.07697. Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 631–636. ACM, 2006. R. Li, J. X. Yu, L. Qin, R. Mao, and T. Jin. On random walk based graph sampling. In 2015 IEEE 31st International Conference on Data Engineering, pp. 927–938, April 2015. doi: 10.1109/ICDE. 2015.7113345. Haonan Lu, Seth H. Huang, Tian Ye, and Xiuyan Guo. Graph star net for generalized multi-task learning. CoRR, abs/1906.12330, 2019. URL http://arxiv.org/abs/1906.12330. Bruno Ribeiro and Don Towsley. Estimating and sampling graphs with multidimensional random walks. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pp. 390–403. ACM, 2010. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. 10Published as a conference paper at ICLR 2020 Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. CoRR, abs/1901.00596, 2019. URL http: //arxiv.org/abs/1901.00596. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, 2018a. ISBN 978-1-4503-5552-0. Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 4805–4815, USA, 2018b. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id= 3327345.3327389. Hanqing Zeng and Viktor Prasanna. GraphACT: Accelerating GCN training on CPU-FPGA hetero- geneous platforms. arXiv preprint arXiv:2001.02498, 2019. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Accurate, efﬁcient and scalable graph embedding. CoRR, abs/1810.11899, 2018. URL http: //arxiv.org/abs/1810.11899. Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated atten- tion networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018. Zhenpeng Zhou. Graph convolutional networks for molecules. CoRR, abs/1706.09916, 2017. URL http://arxiv.org/abs/1706.09916. A P ROOFS Proof of Proposition 3.1.Under the condition that vis sampled in a subgraph: E ( ζ(ℓ+1) v ) =E (∑ u∈V ˜Av,u αu,v ˜x(ℓ) u 1 u|v ) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u E ( 1 u|v ) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u P((u,v) sampled|vsampled) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u P((u,v) sampled) P(vsampled) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u pu,v pv (5) where the second equality is due to linearity of expectation, and the third equality (conditional edge probability) is due to the initial condition that vis sampled in a subgraph. It directly follows that, when αu,v = pu,v pv , E ( ζ(ℓ+1) v ) = ∑ u∈V ˜Av,u ˜x(ℓ) u 11Published as a conference paper at ICLR 2020 Proof of Theorem 3.2.Below, we use Cov (·) to denote covariance and Var(·) to denote variance. For independent edge sampling as deﬁned in Section 3.3, Cov ( 1 (ℓ1) e1 ,1 (ℓ2) e2 ) = 0,∀e1 ̸= e2. And for a full GCN on the subgraph, Cov ( 1 (ℓ1) e ,1 (ℓ2) e ) = pe −p2 e. To start the proof, we ﬁrst assume that the b(ℓ) e is one dimensional (i.e., a scalar) and denote it by b(ℓ) e . Now, Var(ζ) = ∑ e,ℓ ( b(ℓ) e pe )2 Var ( 1 (ℓ) e ) + 2 ∑ e,ℓ1<ℓ2 b(ℓ1) e b(ℓ2) e p2e Cov ( 1 (ℓ1) e ,1 (ℓ2) e ) = ∑ e,ℓ ( b(ℓ) e )2 pe − ∑ e,ℓ ( b(ℓ) e )2 + 2 ∑ e,ℓ1<ℓ2 b(ℓ1) e b(ℓ2) e p2e ( pe −p2 e ) = ∑ e (∑ ℓ b(ℓ) e )2 pe − ∑ e (∑ ℓ b(ℓ) e )2 (6) Let a given constant m= ∑ e pe be the expected number of sampled edges. By Cauchy-Schwarz in- equality: ∑ e ( ∑ ℓ b(ℓ) e ) 2 pe m= ∑ e (∑ ℓ b(ℓ) e√pe )2 ∑ e (√pe )2 ≥ (∑ e,ℓ b(ℓ) e )2 . The equality is achieved when ⏐⏐⏐ ∑ ℓ b(ℓ) e√pe ⏐⏐⏐∝√pe. i.e., variance is minimized when pe ∝ ⏐⏐⏐∑ ℓ b(ℓ) e ⏐⏐⏐. It directly follows that: pe = m ∑ e′ ⏐⏐⏐∑ ℓ b(ℓ) e′ ⏐⏐⏐ ⏐⏐⏐⏐⏐ ∑ ℓ b(ℓ) e ⏐⏐⏐⏐⏐ For the multi-dimensional case of b(ℓ) e , following similar steps as above, it is easy to show that the optimal edge probability to minimize ∑ i Var(ζi) (where iis the index for ζ’s dimensions) is: pe = m ∑ e′ ∑ ℓ b(ℓ) e′   ∑ ℓ b(ℓ) e  B S AMPLING ALGORITHM Algorithm 2 lists the four graph samplers we have integrated into GraphSAINT. The naming of the samplers follows that of Table 2. Note that the sampling parameters nand mspecify a budget rather than the actual number of nodes and edges in the subgraph Gs. Since certain nodes or edges in the training graph Gmay be repeatedly sampled under a single invocation of the sampler, we often have |Vs|<n for node and MRW samplers, |Vs|<2mfor edge sampler, and |Vs|<r ·hfor RW sampler. Also note that the edge sampler presented in Algorithm 2 is an approximate version of the independent edge sampler deﬁned in Section 3.4. Complexity (excluding the subgraph induction step) of the original version in Section 3.4 is O(|E|), while complexity of the approximate one is O(m). When m≪|E|, the approximate version leads to identical accuracy as the original one, for a given m. C D ETAILED EXPERIMENTAL SETUP C.1 H ARDWARE SPECIFICATION AND ENVIRONMENT We run our experiments on a single machine with Dual Intel Xeon CPUs (E5-2698 v4 @ 2.2Ghz), one NVIDIA Tesla P100 GPU (16GB of HBM2 memory) and 512GB DDR4 memory. The code is written in Python 3.6.8 (where the sampling part is written with Cython 0.29.2). We use Tensorﬂow 1.12.0 on CUDA 9.2 with CUDNN 7.2.1 to train the model on GPU. Since the subgraphs are sampled independently, we run the sampler in parallel on 40 CPU cores. 12Published as a conference paper at ICLR 2020 Algorithm 2Graph sampling algorithms by GraphSAINT Input: Training graph G(V,E); Sampling parameters: node budget n; edge budget m; number of roots r; random walk length h Output: Sampled graph Gs (Vs,Es) 1: function NODE (G,n) ⊿Node sampler 2: P(v) := ˜A:,v  2 /∑ v′∈V ˜A:,v′  2 3: Vs ←nnodes randomly sampled (with replacement) from Vaccording to P 4: Gs ←Node induced subgraph of Gfrom Vs 5: end function 6: function EDGE (G,m) ⊿Edge sampler (approximate version) 7: P((u,v)) := ( 1 deg(u) + 1 deg(v) ) /∑ (u′,v′)∈E ( 1 deg(u′) + 1 deg(v′) ) 8: Es ←medges randomly sampled (with replacement) from Eaccording to P 9: Vs ←Set of nodes that are end-points of edges in Es 10: Gs ←Node induced subgraph of Gfrom Vs 11: end function 12: function RW(G,r,h) ⊿Random walk sampler 13: Vroot ←rroot nodes sampled uniformly at random (with replacement) from V 14: Vs ←Vroot 15: for v∈Vroot do 16: u←v 17: for d= 1to hdo 18: u←Node sampled uniformly at random from u’s neighbor 19: Vs ←Vs ∪{u} 20: end for 21: end for 22: Gs ←Node induced subgraph of Gfrom Vs 23: end function 24: function MRW(G,n,r) ⊿Multi-dimensional random walk sampler 25: VFS ←rroot nodes sampled uniformly at random (with replacement) from V 26: Vs ←VFS 27: for i= r+ 1to ndo 28: Select u∈VFS with probability deg(u)/∑ v∈VFS deg(v) 29: u′←Node randomly sampled from u’s neighbor 30: VFS ←(VFS \\{u}) ∪{u′} 31: Vs ←Vs ∪{u} 32: end for 33: Gs ←Node induced subgraph of Gfrom Vs 34: end function 13Published as a conference paper at ICLR 2020 100 101 102 103 104 105 10−6 10−4 10−2 100 Degree P(degree ≥k) PPI Flickr Reddit Yelp Amazon Figure 5: Degree Distribution C.2 A DDITIONAL DATASET DETAILS Here we present the detailed procedures to prepare the Flickr, Yelp and Amazon datasets. The Flickr dataset originates from NUS-wide §. The SNAP website ¶collected Flickr data from four different sources including NUS-wide, and generated an un-directed graph. One node in the graph represents one image uploaded to Flickr. If two images share some common properties (e.g., same geographic location, same gallery, comments by the same user, etc.), there is an edge between the nodes of these two images. We use as the node features the 500-dimensional bag-of-word representation of the images provided by NUS-wide. For labels, we scan over the 81 tags of each image and manually merged them to 7 classes. Each image belongs to one of the 7 classes. The Yelp dataset is prepared from the rawjson data of businesses, users and reviews provided in the open challenge website∥. For nodes and edges, we scan the friend list of each user in the raw json ﬁle of users. If two users are friends, we create an edge between them. We then ﬁlter out all the reviews by each user and separate the reviews into words. Each review word is converted to a 300-dimensional vector using the Word2Vec model pre-trained on GoogleNews∗∗. The word vectors of each node are added and normalized to serve as the node feature (i.e., xv). As for the node labels, we scan the raw json ﬁle of businesses, and use the categories of the businesses reviewed by a user vas the multi-class label of v. For the Amazon dataset, a node is a product on the Amazon website and an edge (u,v) is created if products uand vare bought by the same customer. Each product contains text reviews (converted to 4-gram) from the buyer. We use SVD to reduce the dimensionality of the 4-gram representation to 200, and use the obtained vectors as the node feature. The labels represent the product categories (e.g., books, movies, shoes). Figure 5 shows the degree distribution of the ﬁve graphs. A point (k,p) in the plot means the probability of a node having degree at least kis p. C.3 A DDITIONAL DETAILS IN EXPERIMENTAL CONFIGURATION Table 4 summarizes the URLs to download the baseline codes. The optimizer for GraphSAINT and all baselines is Adam (Kingma & Ba, 2014). For all baselines and datasets, we perform grid search on the hyperparameter space deﬁned by: •Hidden dimension: {128,256,512} §http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm ¶https://snap.stanford.edu/data/web-flickr.html ∥https://www.yelp.com/dataset ∗∗https://code.google.com/archive/p/word2vec/ 14Published as a conference paper at ICLR 2020 Table 4: URLs and commit number to run baseline codes Baseline URL Commit Vanilla GCN github.com/williamleif/GraphSAGE a0fdef GraphSAGE github.com/williamleif/GraphSAGE a0fdef FastGCN github.com/matenure/FastGCN b8e6e6 S-GCN github.com/thu-ml/stochastic_gcn da7b78 AS-GCN github.com/huangwb/AS-GCN 5436ec ClusterGCNgithub.com/google-research/google-research/tree/master/cluster_gcn99021e Table 5: Training conﬁguration of GraphSAINT for Table 2 Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length Node PPI 0.01 0.0 6000 — — — Flickr 0.01 0.2 8000 — — — Reddit 0.01 0.1 8000 — — — Yelp 0.01 0.1 5000 — — — Amazon 0.01 0.1 4500 — — — Edge PPI 0.01 0.1 — 4000 — — Flickr 0.01 0.2 — 6000 — — Reddit 0.01 0.1 — 6000 — — Yelp 0.01 0.1 — 2500 — — Amazon 0.01 0.1 — 2000 — — RW PPI 0.01 0.1 — — 3000 2 Flickr 0.01 0.2 — — 6000 2 Reddit 0.01 0.1 — — 2000 4 Yelp 0.01 0.1 — — 1250 2 Amazon 0.01 0.1 — — 1500 2 MRW PPI 0.01 0.1 8000 — 2500 — Flickr 0.01 0.2 12000 — 3000 — Reddit 0.01 0.1 8000 — 1000 — Yelp 0.01 0.1 2500 — 1000 — Amazon 0.01 0.1 4500 — 1500 — •Dropout: {0.0,0.1,0.2,0.3} • Learning rate: {0.1,0.01,0.001,0.0001} The hidden dimensions used for Table 2, Figure 2, Figure 3 and Figure 4 are: 512 for PPI, 256 for Flickr, 128 for Reddit, 512 for Yelp and 512 for Amazon. All methods terminate after a ﬁxed number of epochs based on convergence. We save the model producing the highest validation set F1-micro score, and reload it to evaluate the test set accuracy. For vanilla GCN and AS-GCN, we set the batch size to their default value 512. For GraphSAGE, we use the mean aggregator with the default batch size 512. For S-GCN, we set the ﬂag -cv -cvd (which stand for “control variate” and “control variate dropout”) with pre-computation of the ﬁrst layer aggregation. According to the paper (Chen et al., 2018a), such pre-computation signiﬁcantly reduces training time without affecting accuracy. For S-GCN, we use the default batch size 1000, and for FastGCN, we use the default value 400. For ClusterGCN, its batch size is determined by two parameters: the cluster size and the number of clusters per batch. We sweep the cluster size from 500 to 10000 with step 500, and the number of clusters per batch from {1,10,20,40}to determine the optimal conﬁguration for each dataset / architecture. Considering that for ClusterGCN, the cluster structure may be sensitive to the cluster size, and for FastGCN, the minibatch connectivity may increase with the sample size, we present additional experimental results to reveal the relation between accuracy and batch size in Appendix D.3. 15Published as a conference paper at ICLR 2020 Table 6: Training conﬁguration of GraphSAINT for Table 3 Arch. Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length 2×512 MRW PPI (large) 0.01 0.1 1500 — 300 — 5×2048 RW PPI (large) 0.01 0.1 — — 3000 2 2×128 Edge Reddit 0.01 0.1 — 6000 — — 4×128 Edge Reddit 0.01 0.2 — 11000 — — Table 7: Training conﬁguration of GraphSAINT for Figure 4 (Reddit) 2-layer GAT-SAINT 4-layer GAT-SAINT 2-layer JK-SAINT 4-layer JK-SAINT Hidden dimension 128 128 128 128 AttentionK 8 8 — — Aggregation⨁ — — Concat. Concat. Sampler RW RW Edge Edge (root: 3000; length: 2) (root: 2000; length: 4) (budget: 6000) (budget: 11000) Learning rate 0.01 0.01 0.01 0.01 Dropout 0.2 0.2 0.1 0.2 Conﬁguration of GraphSAINT to reproduce Table 2 results is shown in Table 5. Conﬁguration of GraphSAINT to reproduce Table 3 results is shown in Table 6. Below we describe the conﬁguration for Figure 4. The major difference between a normal GCN and a JK-net (Xu et al., 2018) is that JK-net has an additional ﬁnal layer that aggregates all the output hidden features of graph convolutional layers 1 to L. Mathematically, the additional aggregation layer outputs the ﬁnal embedding xJK as follows: xJK = σ ( WT JK · L⨁ ℓ=1 x(ℓ) v ) (7) where based on Xu et al. (2018), ⨁is the vector aggregation operator: max-pooling, concatenation or LSTM (Hochreiter & Schmidhuber, 1997) based aggregation. The graph attention of GAT (Veli ˇckovi´c et al., 2017) calculates the edge weights for neighbor aggregation by an additional neural network. With multi-head ( K) attention, the layer- (ℓ−1) features propagate to layer-(ℓ) as follows: x(ℓ) v =  K k=1 σ   ∑ u∈neighbor(v) αk u,vWkx(ℓ−1) v   (8) where ∥is the vector concatenation operation, and the coefﬁcient αis calculated with the attention weights ak by: αk u,v = LeakyReLU (( ak)T [ Wkxu∥Wkxv ]) (9) Note that the αcalculation is slightly different from the original equation in Veliˇckovi´c et al. (2017). Namely, GAT-SAINT does not normalize αby softmax across all neighbors of v. We make such modiﬁcation since under the minibatch setting, node vdoes not see all its neighbors in the training graph. The removal of softmax is also seen in the attention design of Huang et al. (2018). Note that during the minibatch training, GAT-SAINT further applies another edge coefﬁcient on top of attention for aggregator normalization. Table 7 shows the conﬁguration of the GAT-SAINT and JK-SAINT curves in Figure 4. 16Published as a conference paper at ICLR 2020 2 3 4 5 60 2 4 6 8 GCN depth Normalized training time GraphSAINT: Reddit S-GCN: Reddit GraphSAINT: Yelp S-GCN: Yelp Figure 6: Comparison of training efﬁciency PPI Flickr Reddit Yelp Amazon 0 0.5 1 1.5 Fraction of training time Node Edge RW MRW Figure 7: Fraction of training time on sampling D A DDITIONAL EXPERIMENTS D.1 T RAINING EFFICIENCY ON DEEP MODELS We evaluate the training efﬁciency for deeper GCNs. We only compare with S-GCN, since implemen- tations for other layer sampling based methods have not yet supported arbitrary model depth. The batch size and hidden dimension are the same as Table 2. On the two large graphs (Reddit and Yelp), we increase the number of layers and measure the average time per minibatch execution. In Figure 6, training cost of GraphSAINT is approximately linear with GCN depth. Training cost of S-GCN grows dramatically when increasing the depth. This reﬂects the “neighbor explosion” phenomenon (even though the expansion factor of S-GCN is just 2). On Yelp, S-GCN gives “out-of-memory” error for models beyond 5 layers. D.2 C OST OF SAMPLING AND PRE-PROCESSING Cost of graph samplers ofGraphSAINT Graph sampling introduces little training overhead. Let ts be the average time to sample one subgraph on a multi-core machine. Let tt be the average time to perform the forward and backward propagation on one minibatch on GPU. Figure 7 shows the ratio ts/tt for various datasets. The parameters of the samplers are the same as Table 2. For Node, Edge and RW samplers, we observe that time to sample one subgraph is in most cases less than 25% of the training time. The MRW sampler is more expensive to execute. Regarding the complete pre-processing procedure, we repeatedly run the sampler for N = 50·|V| /|Vs|times before training, to estimate the node and edge probability as discussed in Section 3.2 (where |Vs|is the average subgraph size). These sampled subgraphs are reused as training minibatches. Thus, if training runs for more than N iterations, the pre-processing is nearly zero-cost. Under the setting of Table 2, pre-processing on PPI and Yelp and Amazon does not incur any overhead in training time. Pre-processing on Flickr and Reddit (with RW sampler) takes less than 40% and 15% of their corresponding total training time. Cost of layers sampler of AS-GCNAS-GCN uses an additional neural network to estimate the conditional sampling probability for the previous layer. For a node v already sampled in layer ℓ, features of layer-(ℓ−1) corresponding to all v’s neighbors need to be fed to the sampling neural network to obtain the node probability. For sake of analysis, assume the sampling network is a single layer MLP, whose weight WMLP has the same shape as the GCN weights W(ℓ). Then we can show, for a L-layer GCN on a degree-dgraph, per epoch training complexity of AS-GCN is approximately γ = (d·L) /∑L−1 ℓ=0 dℓ times that of vanilla GCN. For L = 2, we have γ ≈2. This explains the observation that AS-GCN is slower than vanilla GCN in Figure 2. Additional, Table 8 shows the training time breakdown for AS-GCN. Clearly, its sampler is much more expensive than the graph sampler of GraphSAINT. 17Published as a conference paper at ICLR 2020 Table 8: Per epoch training time breakdown for AS-GCN Dataset Sampling time (sec) Forward / Backward propagation time (sec) PPI 1.1 0.2 Flickr 5.3 1.1 Reddit 20.7 3.5 Cost of clustering of ClusterGCNClusterGCN uses the highly optimized METIS software††to perform clustering. Table 9 summarizes the time to obtain the clusters for the ﬁve graphs. On the large and dense Amazon graph, the cost of clustering increase dramatically. The pre-processing time of ClusterGCN on Amazon is more than 4×of the total training time. On the other hand, the sampling cost of GraphSAINT does not increase signiﬁcantly for large graphs (see Figure 7). Table 9: Clustering time of ClusterGCN PPI Flickr Reddit Yelp Amazon Time (sec) 2.2 11.6 40.0 106.7 2254.2 Taking into account the pre-processing time, sampling time and training time altogether, we sum- marize the total convergence time of GraphSAINT and ClusterGCN in Table 10 (corresponding to Table 2 conﬁguration). On graphs that are large and dense (e.g., Amazon), GraphSAINT achieves signiﬁcantly faster convergence. Note that both the sampling of GraphSAINT and clustering of ClusterGCN can be performed ofﬂine. Table 10: Comparison of total convergence time (pre-processing + sampling + training, unit: second) PPI Flickr Reddit Yelp Amazon GraphSAINT-Edge 91.0 7.0 16.6 273.9 401.0 GraphSAINT-RW 103.6 7.5 17.2 310.1 425.6 ClusterGCN 163.2 12.9 55.3 256.0 2804.8 D.3 E FFECT OF BATCH SIZE Table 11 shows the change of test set accuracy with batch sizes. For each row of Table 11, we ﬁx the batch size, tune the other hyperparameters according to Appendix C.3, and report the highest test set accuracy achieved. For GraphSAGE, S-GCN and AS-GCN, their default batch sizes (512,1000 and 512, respectively) lead to the highest accuracy on all datasets. For FastGCN, increasing the default batch size (from 400 to 4000) leads to noticeable accuracy improvement. For ClusterGCN, different datasets correspond to different optimal batch sizes. Note that the accuracy in Section 5.1 is already tuned by identifying the optimal batch size on a per graph basis. For FastGCN, intuitively, increasing batch size may help with accuracy improvement since the minibatches may become better connected. Such intuition is veriﬁed by the rows of 400 and 2000. However, increasing the batch size from 2000 to 4000 does not further improve accuracy signiﬁcantly. For ClusterGCN, the optimal batch size depends on the cluster structure of the training graph. For PPI, small batches are better, while for Amazon, batch size does not have signiﬁcant impact on accuracy. For GraphSAGE, overly large batches may have negative impact on accuracy due to neighbor explosion. Approximately, GraphSAGE expand 10×more neighbors per layer. For a 2-layer GCN, a size 2 ×103 minibatch would then require the support of 2 ×105 nodes from the ††http://glaros.dtc.umn.edu/gkhome/metis/metis/download ∗Default batch size ¶The training does not converge. ‡The codes throw runtime error on the large datasets (Yelp or Amazon). 18Published as a conference paper at ICLR 2020 Table 11: Test set F1-micro for the baselines under various batch sizes Method Batch size PPI Flickr Reddit Yelp Amazon GraphSAGE 256 0.600 0.474 0.934 0.563 0.428 512∗ 0.637 0.501 0.953 0.634 0.758 1024 0.610 0.482 0.935 0.632 0.705 2048 0.625 0.374 0.936 0.563 0.447 FastGCN 400∗ 0.513 0.504 0.924 0.265 0.174 2000 0.561 0.506 0.934 0.255 0.196 4000 0.564 0.507 0.934 0.260 0.195 S-GCN 500 0.519 0.462 — ¶ — ¶ — ‡ 1000∗ 0.963 0.482 0.964 0.640 — ‡ 2000 0.646 0.482 0.949 0.614 — ‡ 4000 0.804 0.482 0.949 0.594 — ‡ 8000 0.694 0.481 0.950 0.613 — ‡ AS-GCN 256 0.682 0.504 0.950 — ‡ — ‡ 512∗ 0.687 0.504 0.958 — ‡ — ‡ 1024 0.687 0.502 0.951 — ‡ — ‡ 2048 0.670 0.502 0.952 — ‡ — ‡ ClusterGCN 500 0.875 0.481 0.942 0.604 0.752 1000 0.831 0.478 0.947 0.602 0.756 1500 0.865 0.480 0.954 0.602 0.752 2000 0.828 0.469 0.954 0.609 0.759 2500 0.849 0.476 0.954 0.598 0.745 3000 0.840 0.473 0.954 0.607 0.754 3500 0.846 0.473 0.952 0.602 0.754 4000 0.853 0.472 0.949 0.605 0.756 input layer. Note that the full training graph size of Reddit is just around 1.5 ×105. Thus, no matter which nodes are sampled in the output layer, GraphSAGE would almost always propagate features within the full training graph for initial layers. We suspect this would lead to difﬁculties in learning. For S-GCN, with batch size of 500, it fails to learn properly on Reddit and Yelp. The accuracy ﬂuctuates in a region of very low value, even after appropriate hyperparameter tuning. For AS-GCN, its accuracy is not sensitive to the batch size, since AS-GCN addresses neighbor explosion and also ensures good inter-layer connectivity within the minibatch. 19",
      "meta_data": {
        "arxiv_id": "1907.04931v4",
        "authors": [
          "Hanqing Zeng",
          "Hongkuan Zhou",
          "Ajitesh Srivastava",
          "Rajgopal Kannan",
          "Viktor Prasanna"
        ],
        "published_date": "2019-07-10T21:11:13Z",
        "pdf_url": "https://arxiv.org/pdf/1907.04931v4.pdf"
      }
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      }
    },
    {
      "title": "Accelerating Gossip SGD with Periodic Global Averaging",
      "abstract": "Communication overhead hinders the scalability of large-scale distributed\ntraining. Gossip SGD, where each node averages only with its neighbors, is more\ncommunication-efficient than the prevalent parallel SGD. However, its\nconvergence rate is reversely proportional to quantity $1-\\beta$ which measures\nthe network connectivity. On large and sparse networks where $1-\\beta \\to 0$,\nGossip SGD requires more iterations to converge, which offsets against its\ncommunication benefit. This paper introduces Gossip-PGA, which adds Periodic\nGlobal Averaging into Gossip SGD. Its transient stage, i.e., the iterations\nrequired to reach asymptotic linear speedup stage, improves from\n$\\Omega(\\beta^4 n^3/(1-\\beta)^4)$ to $\\Omega(\\beta^4 n^3 H^4)$ for non-convex\nproblems. The influence of network topology in Gossip-PGA can be controlled by\nthe averaging period $H$. Its transient-stage complexity is also superior to\nLocal SGD which has order $\\Omega(n^3 H^4)$. Empirical results of large-scale\ntraining on image classification (ResNet50) and language modeling (BERT)\nvalidate our theoretical findings.",
      "full_text": "Accelerating Gossip SGD with Periodic Global Averaging Yiming Chen * 1 Kun Yuan* 1 Yingya Zhang 1 Pan Pan 1 Yinghui Xu 1 Wotao Yin1 Abstract Communication overhead hinders the scalability of large-scale distributed training. Gossip SGD, where each node averages only with its neigh- bors, is more communication-efﬁcient than the prevalent parallel SGD. However, its convergence rate is reversely proportional to quantity 1 −β which measures the network connectivity. On large and sparse networks where 1 −β → 0, Gossip SGD requires more iterations to converge, which offsets against its communication beneﬁt. This paper introduces Gossip-PGA, which adds Periodic Global Averaging into Gossip SGD. Its transient stage, i.e., the iterations required to reach asymptotic linear speedup stage, improves from Ω(β4n3/(1−β)4) to Ω(β4n3H4) for non-convex problems. The inﬂuence of network topology in Gossip-PGA can be controlled by the averaging period H. Its transient-stage complexity is also superior to Local SGD which has orderΩ(n3H4). Empirical results of large-scale training on image classiﬁcation (ResNet50) and language modeling (BERT) validate our theoretical ﬁndings. 1. Introduction The scale of deep learning nowadays calls for efﬁcient large- scale distributed training across multiple computing nodes in the data-center clusters. In distributed optimization, a network of nnodes cooperate to solve the problem min x∈Rd 1 n n∑ i=1 [fi(x) := Eξi∼DiFi(x; ξi)] (1) where each component fi is local and private to node iand the random variable ξi denotes the local data that follows distribution Di. We assume each node ican locally evaluate stochastic gradients ∇Fi(x; ξi) where ξi ∼Di, but must communicate to access information from other nodes. *Equal contribution 1Alibaba Group, Hangzhou, China. Corre- spondence to: Kun Yuan <kun.yuan@alibaba-inc.com>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). METHOD EPOCH ACC.% T IME (HRS .) PARALLEL SGD 120 76.26 2.22 GOSSIP SGD ( RING ) 120 74.86 1.56 GOSSIP SGD ( EXPO ) 120 75.34 1.55 GOSSIP SGD ( RING ) 240 75.62 3.02 GOSSIP SGD ( EXPO ) 240 76.18 3.03 Table 1.Top-1 validation accuracy for ImageNet with 256 GPUs connected with the ring or one-peer exponential network. Gossip SGD takes more time to reach the same accuracy as Parallel SGD. Parallel SGD methods are leading algorithms to solve (1), in which every node processes local training samples inde- pendently, and synchronize gradients every iteration either using a central Parameter Server (PS)(Li et al., 2014) or the All-Reduce communication primitive (Patarasuk & Yuan, 2009). The global synchronization in Parallel SGD either incurs signiﬁcant bandwidth cost or high latency, which hampers the training scalability. Many alternative methods have been proposed to reduce communication overhead in distributed training. Gossip SGD, also known as decentralized SGD (Nedic & Ozdaglar, 2009; Chen & Sayed, 2012; Lian et al., 2017; 2018; Ass- ran et al., 2019), recently received lots of attention. This line of work lets each node communicate with (some of) their direct neighbors. In a sparse topology such as one-peer exponential graph (Assran et al., 2019), each node only com- municates with one neighbor each time. This gossip-style communication is much faster than PS and All-Reduce but the computed average can be highly inaccurate. Local SGD (Stich, 2019; Yu et al., 2019; Lin et al., 2018) is another line of work that increases the computation-to-communication ratio. Local SGD lets each node to run local gradient de- scent for multiple rounds and only average their parameters globally once in a while. By communicating less frequently, Local SGD reduces the communication overhead. The reduced communication in Gossip and Local SGDs comes at a cost: slower convergence rate. While both al- gorithms are proved to have convergence linear speedup asymptotically, they are sensitive to network topology and synchronization period, respectively. For Gossip SGD, the convergence rate is inversely proportional to 1 −β (β is deﬁned in Remark 1). Since β →1 on the large and sparse network topology which is most valuable for deep training, Gossip SGD will converge very slow and require more iter- ations than Parallel SGD to achieve a desired solution. This arXiv:2105.09080v1  [cs.LG]  19 May 2021Accelerating Gossip SGD with Periodic Global Averaging GOSSIP SGD G OSSIP -PGA IID NON -IID IID NON -IID (PROPOSED ) SMALL OR DENSE NETWORK (WHEN 1 1−β <H ) Ω( n3β4 (1−β)2 ) Ω( n3β4 (1−β)4 ) Ω( n3β4C2 β) Ω( n3β4C2 β (1−β)2 ) LARGE OR SPARSE NETWORK (WHEN 1 1−β ≥H) Ω( n3β4 (1−β)2 ) Ω( n3β4 (1−β)4 ) Ω( n3β4C2 β) Ω( n3β4C2 βH2) Table 2.The lengths of the transient stages of Gossip SGD and Gossip-PGA. Since Cβ = ∑H−1 k=0 βk = (1 −βH)/(1 −β) < min{1/(1 −β),H}, Gossip-PGA always has shorter transient stage, more evident on large and sparse networks where β →1. LOCAL SGD G OSSIP -PGA IID SCENARIO Ω(n3H2) Ω( n3β4C2 β) NON -IID SCENARIO Ω(n3H4) Ω( n3β4C2 βH2) Table 3.The lengths of the transient stages of Local SGD and Gossip-PGA. Gossip-PGA always has shorter transient stages than Local SGD since β <1 and Cβ <H . Such superiority becomes more signiﬁcant on well-connected networks where β →0. may nullify its communication efﬁciency and result in even more training time (see Table 1). Local SGD with a large averaging period meets the same issue. This paper proposes Gossip-PGA, which adds periodic All- Reduce global averaging into Gossip to accelerate its conver- gence especially on large and sparse networks. Gossip-PGA also extends Local SGD with fast gossip-style communica- tion after local updates. When the same averaging period H is used, the additional gossip communication in Gossip- PGA endows it with faster convergence than Local SGD. Challenges. Gossip-PGA can be regarded as a special form of the topology-changing Gossip SGD (Koloskova et al., 2020) and SlowMo (Wang et al., 2019) (in which the base optimizer is set as Gossip SGD, and the momentum coefﬁ- cient β = 0). However, its theory and practical performance were not carefully investigated in literature. Unanswered im- portant questions include how much acceleration can PGA bring to Gossip and Local SGDs, in what scenario can PGA beneﬁts most, how to adjust the averaging period effectively, and how Gossip-PGA performs in large-scale deep learning systems. Providing quantitative answers to these questions requires new understanding on the interplay between gossip communication and global averaging period. Simply follow- ing existing analysis in (Koloskova et al., 2020) will result in incomplete conclusions, see Remark 5. Also, the analysis in SlowMo (Wang et al., 2019) does not consider heteroge- neous data distributions and cannot cover our results. 1.1. Main Results This paper proves that Gossip-PGA converges at O ( σ√ nT SGD rate + C 1 3 ββ 2 3 (σ 2 3 + D 1 3 βb 2 3 ) T 2 3 + βDβ T )    Extra overhead (2) for both smooth convex and non-convex functions fi (the metrics used for both scenarios can be referred to Theorems 1 and 2), where nis the network size, T is the total number of iterations, σ2 denotes gradient noise, b2 gauges data heterogeneity, β ∈(0,1) measures how well the network is connected, H is the global averaging period, and we deﬁne Cβ = ∑H−1 k=0 βk and Dβ = min{H,1/(1 −β)}. Linear speedup. When T is sufﬁciently large, the ﬁrst term 1/ √ nT dominates (2). This also applies to Parallel, Local, and Gossip SGDs. Gossip-PGA and these algorithms all require T = Ω(1 /(nϵ2)) iterations to reach a desired accuracy ϵ, which is inversely proportional to n. We say an algorithm is in its linear-speedup stage at Tth iteration if, for this T, the term involving nT is dominating the rate. Transient stage. Transient stage is referred to those itera- tions before an algorithm reaches its linear-speedup stage, that is iterations 1,...,T where T is relatively small so non- nT terms (i.e., the extra overhead terms in (2)) still domi- nate the rate. We take Gossip-PGA in the non-iid scenario (b2/3 ≥σ) as example. To reach linear speedup, T has to satisfy T 2 3 /(C 1 3 ββ 2 3 D1/3 β ) ≥n 1 2 T 1 2 , i.e., T ≥n3β4C2 βD2 β. So, the transient stage has Ω(n3β4C2 βD2 β) iterations. Tran- sient stage is an important metric to measure the scala- bility of distributed algorithms. Shorter transient stage than Gossip SGD . The transient stage comparison between Gossip SGD and Gossip-PGA is shown in Table 2. Since Cβ = (1 −βH)/(1 −β) < min{H,1/(1 −β)}, we conclude Gossip-PGA always has a shorter transient stage than Gossip SGD for any βand H. Moreover, the superiority of Gossip-PGA becomes evident when the network is large and sparse, i.e., 1 −β → 0. In this case, the transient stage of Gossip SGD can grow dramatically (see the second line in Table 2) while Gossip- PGA is controlled by the global period Hbecause Cβ <H . As a result, Gossip-PGA improves the transient stage of Gossip-SGD from O(n3/(1 −β)4) (or O(n3/(1 −β)2 in the iid scenario) to O(n3) when β →1. Shorter transient stage than Local SGD . The transient stage comparison between Local SGD and Gossip-PGA is shown in Table 3. Using Cβ <H , we ﬁnd Gossip-PGA is always endowed with a shorter transient stage than Local SGD. Moreover, when the network is well-connected suchAccelerating Gossip SGD with Periodic Global Averaging that β →0, it holds that Cβ →1. Gossip-PGA will have a signiﬁcantly shorter transient stage than Local SGD. 1.2. Contributions • We establish the convergence rate of Gossip-PGA for both smooth convex and non-convex problems. Our results clarify how gossip communication and periodic global averaging collaborate to improve the transient stage of Gossip and Local SGDs. We also established shorter wall-clock training times of Gossip-PGA. • We propose Gossip-AGA, which has adaptive global averaging periods. Gossip-AGA automatically adjusts H and has convergence guarantees. • We conduct various experiments (convex logistic re- gression and large-scale deep learning tasks) to vali- date all established theoretical results. In particular, the proposed Gossip-PGA/AGA achieves a similar conver- gence speed to parallel SGD in iterations, but provides 1.3 ∼1.9×runtime speed-up. The introduced global averaging steps in Gossip-PGA/AGA remedy the accu- racy degradation in Gossip SGD and Local SGD. 2. Related Work Decentralized optimization algorithms can be tracked back to (Tsitsiklis et al., 1986). After that, decentralized optimiza- tion has been intensively studied in signal processing and control community. Decentralized gradient descent (DGD) (Nedic & Ozdaglar, 2009), diffusion (Chen & Sayed, 2012) and dual averaging (Duchi et al., 2011) are among the ﬁrst decentralized algorithms that target on general optimization problems. However, these algorithms suffer from a bias caused by data heterogeneity (Yuan et al., 2016). Various primal-dual algorithms are proposed to overcome this issue, and they are based on alternating direction method of multi- pliers (ADMM) (Shi et al., 2014), explicit bias-correction (Shi et al., 2015; Yuan et al., 2019; Li et al., 2019c), gradient tracking (Xu et al., 2015; Di Lorenzo & Scutari, 2016; Nedic et al., 2017; Qu & Li, 2018), coordinate-descent methods (He et al., 2018), and dual acceleration (Scaman et al., 2017; 2018; Uribe et al., 2020). In the context of machine learning, decentralized SGD, also known as Gossip SGD, have gained a lot of attention re- cently. (Lian et al., 2017) ﬁrst proves Gossip SGD can reach the same linear speedup as vanilla parallel SGD. Af- ter that, (Assran et al., 2019) comes out to extend Gossip SGD to directed topology. A recent work (Koloskova et al., 2020) proposes a uniﬁed framework to analyze algorithms with changing topology and local updates. While it covers Gossip-PGA as a special form, the theoretical and practi- cal beneﬁts of periodic global averaging were not studied therein. The data heterogeneity issue suffered in Gossip SGD is discussed and addressed in (Tang et al., 2018; Yuan et al., 2020; Lu et al., 2019; Xin et al., 2020). Gossip SGD is also extended to asynchronous scenarios in (Lian et al., 2018; Luo et al., 2020). Local SGD can be traced back to (Zinkevich et al., 2010) which proposed a one-shot averaging. More frequent aver- aging strategy is proposed in (Zhang et al., 2016), and the convergence property of Local SGD is established in (Yu et al., 2019; Stich, 2019; Bayoumi et al., 2020). Local SGD is also widely-used in federated learning (McMahan et al., 2017; Li et al., 2019a). Another closely related work (Wang et al., 2019) proposes a slow momentum (SlowMo) framework, where each node, similar to the Gossip-PGA algorithm proposed in this pa- per, periodically synchronizes across the network and per- forms a momentum update. The analysis in SlowMo cannot cover the convergence results in this paper due to its data- homogeneous setting. In addition, we will clarify some new questions such as how much acceleration can PGA bring to Gossip and Local SGDs, and how to adjust the averaging period effectively. Various techniques can be integrated to Gossip SGD to improve its communication efﬁciency. This paper does not consider quantization (Alistarh et al., 2017; Bernstein et al., 2018), gradient compression (Tang et al., 2019; Koloskova et al., 2019b;a) and lazy communication (Chen et al., 2018; Liu et al., 2019), but these orthogonal techniques can be added to our methods. 3. Gossip SGD with Periodic Global Average Assume all computing nodes are connected over a graph G = {V,E}where V = {1,2,··· ,n}denote the node index and Edenote the communication links between all nodes. Similar to existing decentralized algorithms (Nedic & Ozdaglar, 2009; Chen & Sayed, 2012; Lian et al., 2017; Assran et al., 2019), information exchange in the gossip step is only allowed to occur between connected neighbors. To characterize the decentralized communication, we let W ∈Rn×n be a doubly stochastic matrix, i.e., W ≥0, W1n = 1n and 1T nW = 1T n. The (i,j)-th element wij is the weight to scale information ﬂowing from node j to node i. If nodes iand jare not neighbors then wij = 0, and if they are neighbors or identical then the weight wij >0. Furthermore, we deﬁne Ni as the set of neighbors of node i which also includes node iitself. The Gossip-PGA algorithm is listed in Algorithm 1. In the gossip step, every node icollects information from all its connected neighbors. For global average step, nodes synchronize their model parameters using the efﬁcient All- Reduce primitives. When H →∞, Gossip-PGA will re- duce to standard Gossip SGD; when W = 1 n11n, Gossip-Accelerating Gossip SGD with Periodic Global Averaging Algorithm 1 Gossip-PGA Require: Initialize learning rate γ >0, weight matrix W, global averaging period H, and let each x(0) i to be equivalent to each other. for k= 0,1,2,...,T −1, every node ido Sample ξ(k+1) i , update g(k) i =∇Fi(x(k) i ; ξ(k+1) i ) x (k+ 1 2 ) i = x(k) i −γg(k) i ⊿Local SGD update if mod(k+ 1,H) = 0 then x(k+1) i = 1 n ∑n j=1 x (k+ 1 2 ) j ⊿global average else x(k+1) i = ∑ j∈Ni wijx (k+ 1 2 ) j ⊿one gossip step PGA will reduce to vanilla parallel SGD; when W = I, Gossip-PGA will reduce to Local SGD. All-Reduce v.s. multiple Gossips. In a computing cluster with nnodes, global averaging is typically conducted in an efﬁcient Ring All-Reduce manner, rather than via multiple gossip steps as in (Berahas et al., 2018). The communi- cation time comparison between a single gossip and Ring All-Reduce step is listed in Appendix H. In the one-peer ex- ponential network, the exact global average can be achieved via ln(n) gossip communications, which generally takes more wall-clock time than a single Ring All-Reduce opera- tion. Therefore, we recommend exploiting All-Reduce to conduct global averaging in Gossip-PGA. Data-center v.s. wireless network. This paper considers deep training within high-performance data-center clusters, in which all GPUs are connected with high-bandwidth chan- nels and the network topology can be fully controlled. Under such setting, the periodic global averaging conducted with Ring All-Reduce has tolerable communication cost, see Ap- pendix H. For scenarios where global averaging is extremely expensive to conduct such as in wireless sensor network, the global averaging can be approximated via multiple gossip steps, or may not be recommended. 3.1. Assumptions and analysis highlights We now establish convergence rates for Gossip-PGA on smooth convex and non-convex problems. For all our theo- retical results we make the following standard assumptions. Assumption 1 (L-SMOOTHNESS ). Each local cost function fi(x) is differentiable, and there exists a constant Lsuch that for each x,y∈Rd: ∥∇fi(x) −∇fi(y)∥≤ L∥x−y∥. (3) Assumption 2 (GRADIENT NOISE ). Recall g(k) i is the stochastic gradient noise deﬁned in line 2 of Algorithm 1. It is assumed that for any kand ithat E[g(k) i −∇fi(x(k) i )|F(k−1)] = 0, (4) E[∥g(k) i −∇fi(x(k) i )∥2|F(k−1)] ≤σ2 (5) for some constant σ2 >0. Moreover, we assume ξ(k) i is in- dependent of each other for anykand i. Filtration is deﬁned as F(k)= { {x(k) i }n i=1,{ξ(k) i }n i=1,··· ,{x(0) i }n i=1,{ξ(0) i }n i=1 } Assumption 3 (WEIGHTING MATRIX ). The network is strongly connected and the weight matrix W satisﬁes W1n = 1n, 1T nW = 1T n, null(I −W) = span( 1n). We also assume ∥W −1 n11T∥2 ≤βfor some β ∈(0,1). Remark 1. Quantity β ∈ (0,1) indicates how well the topology is connected. Smaller βindicates better-connected network while larger βimplies worse-connected topology. Analysis highlights. To derive the inﬂuence of periodic global averaging, we have to exploit all useful algorithm structures to achieve its superiority. These structures are: • x(k) i = ¯x(k) when mod (k,H) = 0. This structure relieves the inﬂuence of network topology; • Gossip communications within each period also con- tribute to consensus among nodes. This structure is crucial to establish superiority to Local SGD; • When network is large and sparse, i.e., H < 1 1−β, the global averaging is more critical to drive consen- sus. This structure is crucial to establish superiority to Gossip SGD when H < 1 1−β. • When network is small or dense, i.e., H > 1 1−β, gos- sip communication is more critical to drive consensus. This structure is crucial to establish superiority to Gos- sip SGD when H > 1 1−β. Ignoring any of the above structures in the analysis will result in incomplete conclusions on comparison among Gossip-PGA, Gossip SGD and Local SGD. 3.2. Convergence analysis: convex scenario Assumption 4 (CONVEXITY ). Each fi(x) is convex. Deﬁnition 1 (DATA HETEROGENEITY ). When each fi(x) is convex, we let b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2 denote the data heterogeneity. When each local data follows the same distribution, it holds that fi(x) = f(x) ∀iand hence ∇fi(x⋆) = ∇f(x⋆) = 0 which also implies b2 = 0. With Assumption 4, we let x⋆ be one of the global solutions to problem (1).Accelerating Gossip SGD with Periodic Global Averaging GOSSIP SGD (K OLOSKOVA ET AL ., 2020) G OSSIP -PGA RATES (GENERAL FORM ) O ( σ√ nT+ β 2 3 σ 2 3 T 2 3 (1−β) 1 3 + β 2 3 b 2 3 T 2 3 (1−β) 2 3 + β (1−β)T ) O ( σ√ nT+ C 1 3 β β 2 3 σ 2 3 T 2 3 + C 1 3 β D 1 3 β β 2 3 b 2 3 T 2 3 +βDβ T ) RATES (WHEN 1 1−β <H ) O ( σ√ nT+ β 2 3 σ 2 3 T 2 3 (1−β) 1 3 + β 2 3 b 2 3 T 2 3 (1−β) 2 3 + β (1−β)T ) O ( σ√ nT+ C 1 3 β β 2 3 σ 2 3 T 2 3 + C 1 3 β β 2 3 b 2 3 (1−β) 1 3 T 2 3 + β (1−β)T ) RATES (WHEN 1 1−β ≥H) O ( σ√ nT+ β 2 3 σ 2 3 T 2 3 (1−β) 1 3 + β 2 3 b 2 3 T 2 3 (1−β) 2 3 + β (1−β)T ) O ( σ√ nT+ C 1 3 β β 2 3 σ 2 3 T 2 3 + C 1 3 β H 1 3 β 2 3 b 2 3 T 2 3 +βH T ) Table 4.Convergence rate comparison between Gossip SGD and Gossip-PGA for smooth convex/non-convex problems. We use notation b2 to indicate the data heterogeneity for both convex and non-convex scenarios. Theorem 1. Under Assumptions 1–4, if γis chosen as γ= min { 1 12βLDβ , ( r0 r1(T + 1) )1 2 , ( r0 r2(T + 1) )1 3} (6) with constants r0 = 2 E∥¯x(0) −x⋆∥2,r1 = 2 σ2/n, and r2 = 6Lβ2Cβσ2 + 18Lβ2CβDβ, it holds for any T that Ef(ˆx(T)) −f(x⋆) = O ( σ√ nT + C 1 3 ββ 2 3 (σ 2 3 + D 1 3 βb 2 3 ) T 2 3 + βDβ T ) (7) where ¯x(k) = 1 n ∑n i=1 x(k) i , ˆx(T) = 1 T+1 ∑T k=0 ¯x(k), Cβ = ∑H−1 k=0 βk and Dβ = min{H,1/(1 −β)}. (Proof is in Appendix B.) Remark 2. When β →0, i.e., the network tends to be fully connected, Gossip-PGA will converge at rateO(σ/ √ nT), which recovers the rate of parallel SGD. Remark 3. When β →1, i.e., the information exchange via gossip communication is inefﬁcient, it holds that Cβ →H and Dβ = min{H,1/(1 −β)}= H. Substituting them to (7) will recover the rate of Local SGD, see Table 6. Remark 4. When H → ∞, i.e., the networked agents tend not to conduct global synchronization, it holds that Cβ →1/(1 −β) and Dβ = 1 1−β. Substituting these values to (7) will recover the rate of Gossip SGD, see Table 4. 3.3. Convergence analysis: non-convex scenario We ﬁrst introduce an assumption about data heterogeneity speciﬁcally for non-convex problems: Assumption 5 (DATA HETEROGENEITY ). There exists con- stant ˆb >0 such that 1 n ∑n i=1 ∥∇fi(x) −∇f(x)∥2 ≤ˆb2 for any x∈Rd. If local data follows the same distribution, it holds that ˆb= 0. Theorem 2. Under Assumptions 1–3 and 5, ifγsatisﬁes the condition (6) (replace b2 with ˆb2 and use r0 = 4Ef(¯x(0))), it holds for any T >0 that 1 T + 1 T∑ k=0 E∥∇f(¯x(k))∥2 GOSSIP SGD G OSSIP -PGA TRANSIENT ITER . O(n7) O(n5) SINGLE COMM . O(θd+ α) O(θd+ √nα) TRANSIENT TIME O(n7θd+ n7α) O(n5θd+ n5.5α) Table 5.Transient time comparison between non-iid Gossip SGD and Gossip-PGA over the speciﬁc grid (1−β = O(1/n)) topology. We choose H = √nas the period in Gossip-PGA. = O ( σ√ nT + C 1 3 ββ 2 3 (σ 2 3 + D 1 3 βb 2 3 ) T 2 3 + βDβ T ) (8) where ¯x(k) = 1 n ∑n i=1 x(k) i . (Proof is in Appendix C.) 3.4. Comparison with Gossip SGD To better illustrate how periodic global averaging helps re- lieve the affects of network topology in Gossip SGD, we list convergence rates of Gossip SGD and Gossip-PGA for smooth convex or non-convex problems in Table 4. The ﬁrst line is the general rate expression for both algorithms. In the second line we let Dβ = min{H,1/(1 −β)}= 1/(1 −β) for Gossip-PGA, and in the third line we let Dβ = H. Ac- cording to this table, we derive the transient stages of Gossip SGD and Gossip-PGA for each scenarios (i.e., large/small network, iid/non-iid data distributions) in Table 2 (see the derivation detail in Appendix D). As we have explained in Main Results subsection in the introduction, it is observed from Tables 2 and 4 that: (i) Gossip-PGA always converges faster (or has shorter transient stages) than Gossip SGD for any β and H value. (ii) Such superiority gets evident for large and sparse networks where β →1. Remark 5. The convergence analysis in topology-changing Gossip SGD (Koloskova et al., 2020) covers Gossip-PGA. By letting p = 1 and τ = H in Theorem 2 of (Koloskova et al., 2020), it is derived that Gossip-PGA has a transient stage on the order of Ω(n3H4) for non-convex non-iid sce- nario. Such transient stage cannot quantify the superiority to Gossip and Local SGDs. In fact, it may even show PGA can do harm to Gossip SGD when H > 1 1−β, which is counter-intuitive. This is because (Koloskova et al., 2020) is for the general time-varying topology. It does not utilizeAccelerating Gossip SGD with Periodic Global Averaging RATES L-SGD O ( σ√ nT+H 1 3 σ 2 3 T 2 3 +H 2 3 b 2 3 T 2 3 +H T ) G-PGA O ( σ√ nT+ C 1 3 β β 2 3 σ 2 3 T 2 3 + C 1 3 β H 1 3 β 2 3 b 2 3 T 2 3 +βH T ) Table 6.Convergence rate comparison between Local SGD (L- SGD) and Gossip-PGA (G-PGA) over smooth convex/non-convex problems. The rate for Local SGD is from (Koloskova et al., 2020; Yu et al., 2019; Li et al., 2019b). the structures listed in Sec. 3.1. Transient stage in runtime . Table 2 compares transient stages between Gossip-PGA and Gossip SGD in iterations. But what people really care about in practice is runtime. Since both Gossip SGD and Gossip-PGA have the same computational overhead per iteration, we will focus on com- munication time spent in the transient stage. Given the bandwidth in a computing cluster with size n, we let αdenote the point-to-point latency in the network, and θ denote the communication time cost to transmit a scalar variable. Since variable x in problem (1) has di- mension d, it will take θdtime to transmit xbetween two nodes. Under this setting, the All-Reduce global averag- ing step will take 2θd + nα = O(θd + nα) time (see section 2.5 in (Ben-Nun & Hoeﬂer, 2019)). The gossip- style communication time varies with different network topologies. For the commonly-used ring or grid topology, it takes |Ni|θd+ α= O(θd+ α) for one gossip communi- cation, where |Ni|is the neighborhood size of node i, and |Ni|= 3 for the ring and 5 for the grid. As to Gossip-PGA, if we amortize the periodic All-Reduce cost into each com- munication, it will have |Ni|θd+ α+ (2θd+ nα)/H = O(θd+ √nα) when we set H = √n. With the formula Total time = transient stage (in iteration) ×comm. per iter. We calculate and compare the transient time between non- iid Gossip-PGA and Gossip-SGD (over the grid topology) in Table 5. Other comparisons for iid scenario or the ring topology can be found in Appendix D. It is observed in all tables that Gossip-PGA has shorter transient time. 3.5. Comparison with Local SGD The convergence rates of Gossip-PGA and Local SGD are listed in Table 6, from which we derive the transient stages of them in Table 3 (details are in Appendix D). As we have explained in the introduction, it is observed from Tables 3 and 6 that (i) Gossip-PGA always converges faster (or has shorter transient stages) than Local SGD for any βand H value, and (ii) Such superiority gets more evident for well-connected network where β →0. As to the wall-clock transient time of Local SGD, if we amortize the periodic All-Reduce cost into each local up- date, it will take (2θd+nα)/H = O(θd/H+nα/H) com- munication time per iteration. Using the transient iteration derived in Table 3, the total transient time for Local SGD (non-iid scenario) will be O(n3H3(θd+ nα)). Comparing it with the total transient time O(n3HC2 ββ4(Hθd + nα)) for Gossip-PGA, we ﬁnd Gossip-PGA always has shorter transient runtime for a large H >β4C2 β. Remark 6. While we discuss in detail that the transient time of Gossip-PGA is shorter than Gossip and Local SGDs, it is worth noting that the communication time during the linear speedup stage (i.e., after the transient stage) also contributes to the total training time. In this stage, Gossip- PGA is less efﬁcient due to its periodic global averaging. However, we illustrate that Gossip-PGA is always endowed with shorter total training time than Gossip and Local SGDs with extensive deep learning experiments in Sec. 5. 4. Gossip SGD with Adaptive Global Average Gossip-PGA suffers from the burden of tuning H by hand. A small Hwill incur more communication overhead while a large value can slow down the convergence. We further pro- pose Gossip-AGA, an adaptive extension of Gossip-PGA. Intuition. A small consensus variance ∑n i=1 E∥xi −¯x∥2 would accelerate Gossip-PGA. To see that, if∑n i=1 E∥xi− ¯x∥2 = 0 for each iteration, then Gossip-PGA reduces to parallel SGD and can reach its fastest convergence. Recall from Lemma 8 in the appendix that the aver- aged consensus 1 T+1 ∑T k=0 E∥x(k) −¯x(k)∥2 is bounded by d1γ2 T+1 ∑T k=0 E∥∇f(¯x(k))∥2+d2γ2 where d1 and d2 are constants. It is observed that the initial consensus vari- ance (when T is small) can be signiﬁcant due to large γ and E∥∇f(¯x(k))∥2. In the later stage when T is sufﬁ- ciently large, both the diminishing step-size γand gradient E∥∇f(¯x(k))∥2 go to 0 and hence leading to a small consen- sus variance naturally. With these observations, it is intuitive to take global synchronizations more frequently in initial stages to reduce the overall consensus variance. Convergence. We denote H(ℓ) as the duration of the ℓ-th period. The following corollary establishes convergence for Gossip-PGA with any time-varying but ﬁnite global averaging period sequence {H(ℓ)}: Corollary 1. Suppose Assumptions 1–3 and 5 hold and the time-varying period H(ℓ) is upper bounded by Hmax = maxℓ≥0{H(ℓ)}. If γ satisﬁes the condition in Theorem 1 with H = Hmax, then Gossip-AGA converges at rate(8) in which H is replaced by Hmax. (Proof is in Appendix E.) Adaptive Strategy. This subsection will propose an adap- tive strategy that is inspired by (Wang & Joshi, 2019). If we recover the inﬂuence of the initial value F0 = Ef(¯x(0)) on convergence rate (8), Gossip-PGA for non-convex problemsAccelerating Gossip SGD with Periodic Global Averaging will converge at O (σF 1 2 0√ nT + H 1 3 β 2 3 σ 2 3 F 2 3 0 T 2 3 + H 2 3 β 2 3 ˆb 2 3 F 2 3 0 T 2 3 + βDβF0 T ) . For a ﬁxed T, a period H = σ 3 2 T 1 4 /(βˆbF 1 4 0 n 3 4 ) will guar- antee the linear speedup. Therefore, the initial period H(0) can be chosen asH(0) = d1/[Ef(¯x(0))] 1 4 for some constant d1. Similarly, for the ℓ-th period, workers can be viewed as restarting training at a new initial point ¯x(Tℓ−1) where Tℓ−1 = H(0) + ··· + H(ℓ−1). As a result, the ℓ-th period H(ℓ) can be chosen as H(ℓ) = d1/[Ef(¯x(Tℓ−1))] 1 4 . With such choice of H(0) and H(ℓ), it is not difﬁcult to have H(ℓ) = ( Ef(¯x(0)) Ef(¯x(Tℓ−1)) )1 4 H(0). (9) Since Ef(¯x(k)) will decrease as kincreases, (9) will gener- ate an increasing sequence of period H(ℓ). We list Gossip- AGA as Algorithm 2 in Appendix G and elaborate on im- plementation details there. 5. Experimental Results In this section, we ﬁrst examine how the transient stage dif- fers for Gossip-PGA, Gossip and Local SGDs on networks with different topology and size on convex logistic regres- sion. Next, we systematically evaluate the aforementioned methods on two typical large-scale deep learning tasks: im- age classiﬁcation (over 256 GPUs) and language modeling (over 64 GPUs). See Appendix F for implementation details. 5.1. Logistic Regression We consider a distributed logistic regression problem with fi(x) = 1 M ∑M m=1 ln[1 + exp( −yi,mhi,m)Tx], where {hi,m,yi,m}M m=1 are local data samples at agent i with hi,m ∈Rd being the feature vector and yi,m ∈{+1,−1} being the corresponding label. Each hi,m is generated from the normal distribution N(0; 10Id). To generate yi,m, we ﬁrst generate an auxiliary random vector x⋆ i ∈Rd with each entry following N(0,1). Next, we generate yi,m from a uni- form distribution U(0,1). If yi,m ≤1/[1 + exp(−hT i,mx⋆ i)] then yi,m is set as +1; otherwise yi,m is set as −1. We let x⋆ i = x⋆ ∀i to generate data for iid scenario and x⋆ i ̸= x⋆ j ∀i,j for non-iid scenario. Each x⋆ i is normalized. Figure 1 compares how Gossip-PGA performs against paral- lel and Gossip SGD over the ring topology and non-iid data distribution. The network sizes are set as n= 20,50,100 which results in β = 0.967,0.995,0.998. We set d = 10 and M = 8000. H is set as 16 in Gossip-PGA. The step- size γ is initialized as 0.2 and gets decreased by half for every 1000 iterations. We repeat all simulations 50 times and illustrate the mean of all trials with solid curve and METHOD ACC.% H RS EPOCHS /HRS TO 76%. PARALLEL SGD 76.26 2.22 94 / 1.74 LOCAL SGD 74.20 1.05 N.A. LOCAL SGD ×3 75.41 3 N.A. GOSSIP SGD 75.34 1.55 N.A. GOSSIP SGD ×2 76.18 3 198/2.55 OSGP 75.04 1.32 N.A. OSGP ×2 76.07 2.59 212/2.28 GOSSIP -PGA 76.28 1.66 109/1.50 GOSSIP -AGA 76.25 1.57 91/1.20 Table 7.Comparison of Top-1 validation accuracy (Column 2) and wall-clock training time (Column 3) on different methods after ﬁnishing all epochs. We also report the epochs and training time required to reach 76% accuracy (Column 4). “N.A.” implies that the target accuracy is not reached when all epochs are completed. standard deviation with shaded area. It is observed that both Gossip SGD and Gossip-PGA will asymptotically converge at the same rate as parallel SGD (i.e., the linear speedup stage), albeit with different transient stages. Gossip-PGA always has shorter transient stages than Gossip SGD, and such superiority gets more evident when network size in- creases (recall that 1 −β = O(1/n2)). For experiments on different topologies such as grid and exponential graph, on iid data distribution, and comparison with Local SGD, see Appendix F. All experiments are consistent with the theoretical transient stage comparisons in Tables 2 and 3. 5.2. Image Classiﬁcation The ImageNet-1k (Deng et al., 2009) 1 dataset consists of 1,281,167 training images and 50,000 validation images in 1000 classes. We train ResNet-50 (He et al., 2016) model (∼25.5M parameters) following the training protocol of (Goyal et al., 2017). We train total 120 epochs. The learning rate is warmed up in the ﬁrst 5 epochs and is decayed by a factor of 10 at 30, 60 and 90 epochs. We set the period to 6 for both Local SGD and Gossip-PGA. In Gossip-AGA, the period is set to 4 initially and changed adaptively afterwards, roughly 9% iterations conduct global averaging. Table 7 shows the top-1 validation accuracy and wall-clock training time of aforementioned methods. It is observed both Gossip-PGA and Gossip-AGA can reach comparable accuracy with parallel SGD after all 120 epochs but with roughly 1.3x ∼1.4x training time speed-up. On the other hand, while local and Gossip SGD completes all120 epochs faster than Gossip-PGA/AGA and parallel SGD, they suffer from a 2.06% and 0.92% accuracy degradation separately. Moreover, both algorithms cannot reach the 76% top-1 accu- racy within 120 epochs. We also compare with OSGP (Ass- ran et al., 2019), which adding overlapping on the Gossip 1The usage of ImageNet dataset in this paper is for non- commercial research purposes only.Accelerating Gossip SGD with Periodic Global Averaging Figure 1.Convergence comparison between Gossip-PGA, Gossip and parallel SGDs on the logistic regression problem over ring topology. The transient stage is determined by counting iterations before an algorithm exactly matches the convergence curve of Parallel SGD. Note that the transient stage for Gossip SGD in the middle and right sub-ﬁgures is beyond the plotting canvas. 0 2500 5000 7500 10000 12500 15000 17500 Iterations 2 4 6Training Loss 16000 18000 0.8 1.0 Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA 0 1000 2000 3000 4000 5000 6000 7000 8000 Training Time 2 4 6Training Loss Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA Figure 2.Convergence results on the ImageNet in terms of iteration and runtime. More results are in Appendix F.3. SGD. We ﬁnd OSGP ×2, while faster than Gossip SGD×2, still needs more time than Gossip-PGA to achieve 76% ac- curacy. To further illustrate how much time it will take local and Gossip SGD to reach the target accuracy, we run another Local SGD and Gossip SGD experiments with ex- tended epochs (i.e., Gossip SGD ×2 trains total 240 epochs and the learning rate is decayed at 60, 120, and 180 epoch. Local SGD ×3 trains total 360 epochs and the learning rate is decayed at 90, 180, and 270 epochs). It is observed that Gossip-SGD ×2 can reach the target with notably more time expense than Gossip-PGA/AGA and parallel SGD, and Local SGD ×3 still cannot reach the 76% accuracy. All these observations validate that periodic global averaging can accelerate Gossip SGD signiﬁcantly. Figure 2 shows the iteration-wise and runtime-wise con- vergence in terms of training loss. In the left ﬁgure, it is observed Gossip-PGA/AGA converges faster (in iteration) and more accurate than local and Gossip SGD, which is consistent with our theory. In the right ﬁgure, it is observed that Gossip-PGA/AGA is the fastest method (in time) that can reach the same training loss as parallel SGD. Compare with SlowMo. Gossip-PGA is an instance of SlowMo, in which the base optimizer is set as Gossip SGD, slow momentum β = 0, and slow learning rate α= 1. We made experiments to compare Gossip-PGA with SlowMo. It is observed the additional slow momentum update helps SlowMo with large H but degrades it when H is small. This observation is consistent with Fig. 3(a) in (Wang et al., 2019). This observation implies that the slow momentum update may not always be beneﬁcial in SlowMo. Period Gossip-PGA SlowMo H = 6 76.28 75.23 H = 48 75 .66 75.81 Table 8.Comparison of Top-1 validation accuracy with SlowMo with different periods. Ring Topology. While the convergence property of Gossip- PGA is established over the static network topology, we utilize the dynamic one-peer exponential topology in the above deep experiments because it usually achieves better accuracy. To illustrate the derived theoretical results, we make an additional experiment, over the static ring topology, to compare Gossip-PGA with Gossip SGD in Table 9. It is observed that Gossip-PGA can achieve better accuracy than Gossip SGD after running the same epochs, which coincides with our analysis that Gossip-PGA has faster convergence. Scalability. We establish in Theorem 2 that Gossip-PGA can achieve linear speedup in the non-convex setting. ToAccelerating Gossip SGD with Periodic Global Averaging 0 20000 40000 60000 80000 100000 Iterations 2 4 6 8 10Training Loss Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA 0 50000 100000 150000 200000 Training time 2 4 6 8 10Training Loss Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA Figure 3.Convergence results of BERT on the language modeling task in terms of iteration and runtime. Method Epoch Acc % Time(Hrs.) Gossip SGD 120 74.86 1.56 Gossip PGA 120 75.94 1.68 Table 9.Comparison of Top-1 validation accuracy on Gossip-PGA and Gossip SGD with ring topology. validate it, we conduct a scaling experiment and list the result in Table 10. Figures represent the ﬁnal accuracy and hours to ﬁnish training. It is observed that Gossip-PGA can achieve a roughly linear speedup in training time without notably performance degradation. Method 4 nodes 8 nodes 16 nodes 32 nodes Parallel SGD 76.3/11.6 76 .4/6.3 76 .3/3.7 76 .2/2.2 Gossip SGD 76.3/11.1 76 .4/5.7 75 .9/2.8 75 .0/1.5 Gossip PGA 76.4/11.2 76 .7/5.9 76 .3/3.0 76 .2/1.6 Table 10.Scaling effects on different methods with different num- bers of nodes. Figures represent the ﬁnal accuracy and hours to complete training. 5.3. Language Modeling BERT (Devlin et al., 2018) is a widely used pre-training language representation model for NLP tasks. We train a BERT-Large model (∼330M parameters) on the Wikipedia METHOD FINAL LOSS RUNTIME (HRS ) PARALLEL SGD 1.75 59.02 LOCAL SGD 2.85 20.93 LOCAL SGD ×3 1.88 60 GOSSIP SGD 2.17 29.7 GOSSIP SGD ×2 1.81 59.7 GOSSIP -PGA 1.82 35.4 GOSSIP -AGA 1.77 30.4 Table 11.Comparison of training loss and training time of BERT training on different algorithms after completing all training steps. and BookCorpus datasets. We set the period to 6 for both Local SGD and Gossip-PGA. In Gossip-AGA, the period is set to 4 initially and changed adaptively afterwards, roughly 9.6% iterations conduct global averaging. Table 11 shows the ﬁnal training loss and training runtime of the aforementioned methods. Gossip-AGA can reach com- parable training loss with parallel SGD, but with roughly 1.94 x training time speed-up. Gossip SGD and Local SGD cannot reach training loss that below 1.8 even if they are trained over 60 hours (see Local SGD ×3 and Gossip SGD ×2.) Figure 3 shows the iteration-wise and runtime-wise convergence w.r.t training loss of the aforementioned meth- ods. The left plot shows Gossip-PGA/AGA has almost the same convergence as Gossip SGD in iterations; the right plot shows that Gossip-AGA is the fastest method in training time that can reach the same accuracy as parallel SGD. 6. Conclusion We introduce Gossip-PGA/AGA to mitigate the slow con- vergence rate of Gossip SGD in distributed training. Theo- retically, we prove the convergence improvement in smooth convex and non-convex problem. Empirically, experimental results of large-scale training validate our theories. References Alistarh, D., Grubic, D., Li, J., Tomioka, R., and V ojnovic, M. Qsgd: Communication-efﬁcient sgd via gradient quan- tization and encoding. In Advances in Neural Information Processing Systems, pp. 1709–1720, 2017. Assran, M., Loizou, N., Ballas, N., and Rabbat, M. Stochas- tic gradient push for distributed deep learning. In Inter- national Conference on Machine Learning (ICML), pp. 344–353, 2019. Bayoumi, A. K. R., Mishchenko, K., and Richtarik, P. Tighter theory for local sgd on identical and heteroge-Accelerating Gossip SGD with Periodic Global Averaging neous data. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 4519–4529, 2020. Ben-Nun, T. and Hoeﬂer, T. Demystifying parallel and dis- tributed deep learning: An in-depth concurrency analysis. ACM Computing Surveys (CSUR), 52(4):1–43, 2019. Berahas, A. S., Bollapragada, R., Keskar, N. S., and Wei, E. Balancing communication and computation in distributed optimization. IEEE Transactions on Automatic Control, 64(8):3141–3155, 2018. Bernstein, J., Zhao, J., Azizzadenesheli, K., and Anandku- mar, A. signsgd with majority vote is communication efﬁ- cient and fault tolerant. arXiv preprint arXiv:1810.05291, 2018. Chen, J. and Sayed, A. H. Diffusion adaptation strategies for distributed optimization and learning over networks. IEEE Transactions on Signal Processing , 60(8):4289– 4305, 2012. Chen, T., Giannakis, G., Sun, T., and Yin, W. LAG: Lazily aggregated gradient for communication-efﬁcient distributed learning. In Advances in Neural Information Processing Systems, pp. 5050–5060, 2018. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248–255. Ieee, 2009. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Di Lorenzo, P. and Scutari, G. Next: In-network nonconvex optimization. IEEE Transactions on Signal and Informa- tion Processing over Networks, 2(2):120–136, 2016. Duchi, J. C., Agarwal, A., and Wainwright, M. J. Dual aver- aging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):592–606, 2011. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. He, L., Bian, A., and Jaggi, M. Cola: Decentralized linear learning. In Advances in Neural Information Processing Systems, pp. 4536–4546, 2018. Koloskova, A., Lin, T., Stich, S. U., and Jaggi, M. De- centralized deep learning with arbitrary communication compression. In International Conference on Learning Representations, 2019a. Koloskova, A., Stich, S., and Jaggi, M. Decentralized stochastic optimization and gossip algorithms with com- pressed communication. In International Conference on Machine Learning, pp. 3478–3487, 2019b. Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich, S. U. A uniﬁed theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning (ICML), pp. 1–12, 2020. Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V ., Long, J., Shekita, E. J., and Su, B.-Y . Scaling distributed machine learning with the parameter server. In 11th {USENIX}Symposium on Operating Systems Design and Implementation ( {OSDI}14), pp. 583–598, 2014. Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the convergence of fedavg on non-iid data. In International Conference on Learning Representations, 2019a. Li, X., Yang, W., Wang, S., and Zhang, Z. Communica- tion efﬁcient decentralized training with multiple local updates. arXiv preprint arXiv:1910.09126, 2019b. Li, Z., Shi, W., and Yan, M. A decentralized proximal- gradient method with network independent step-sizes and separated convergence rates. IEEE Transactions on Signal Processing, July 2019c. early acces. Also available on arXiv:1704.07807. Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J. Can decentralized algorithms outperform central- ized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Infor- mation Processing Systems, pp. 5330–5340, 2017. Lian, X., Zhang, W., Zhang, C., and Liu, J. Asynchronous decentralized parallel stochastic gradient descent. In In- ternational Conference on Machine Learning, pp. 3043– 3052, 2018. Lin, T., Stich, S. U., Patel, K. K., and Jaggi, M. Don’t use large mini-batches, use local sgd. arXiv preprint arXiv:1808.07217, 2018. Liu, Y ., Xu, W., Wu, G., Tian, Z., and Ling, Q. Communication-censored admm for decentralized con- sensus optimization. IEEE Transactions on Signal Pro- cessing, 67(10):2565–2579, 2019.Accelerating Gossip SGD with Periodic Global Averaging Lu, S., Zhang, X., Sun, H., and Hong, M. Gnsd: A gradient- tracking based nonconvex stochastic algorithm for de- centralized optimization. In 2019 IEEE Data Science Workshop (DSW), pp. 315–321. IEEE, 2019. Luo, Q., He, J., Zhuo, Y ., and Qian, X. Prague: High- performance heterogeneity-aware asynchronous decen- tralized training. In Proceedings of the Twenty-Fifth In- ternational Conference on Architectural Support for Pro- gramming Languages and Operating Systems, pp. 401– 416, 2020. McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelli- gence and Statistics, pp. 1273–1282. PMLR, 2017. Nedic, A. and Ozdaglar, A. Distributed subgradient meth- ods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48–61, 2009. Nedic, A., Olshevsky, A., and Shi, W. Achieving geomet- ric convergence for distributed optimization over time- varying graphs. SIAM Journal on Optimization, 27(4): 2597–2633, 2017. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), pp. 8024–8035, 2019. Patarasuk, P. and Yuan, X. Bandwidth optimal all-reduce al- gorithms for clusters of workstations. Journal of Parallel and Distributed Computing, 69(2):117–124, 2009. Qu, G. and Li, N. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network Systems, 5(3):1245–1260, 2018. Scaman, K., Bach, F., Bubeck, S., Lee, Y . T., and Massouli´e, L. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In International Conference on Machine Learning, pp. 3027–3036, 2017. Scaman, K., Bach, F., Bubeck, S., Massouli´e, L., and Lee, Y . T. Optimal algorithms for non-smooth distributed opti- mization in networks. In Advances in Neural Information Processing Systems, pp. 2740–2749, 2018. Shi, W., Ling, Q., Yuan, K., Wu, G., and Yin, W. On the lin- ear convergence of the admm in decentralized consensus optimization. IEEE Transactions on Signal Processing, 62(7):1750–1761, 2014. Shi, W., Ling, Q., Wu, G., and Yin, W. EXTRA: An exact ﬁrst-order algorithm for decentralized consensus opti- mization. SIAM Journal on Optimization, 25(2):944–966, 2015. Stich, S. U. Local sgd converges fast and communicates little. In International Conference on Learning Represen- tations (ICLR), 2019. Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J. d2: De- centralized training over decentralized data. In Interna- tional Conference on Machine Learning, pp. 4848–4856, 2018. Tang, H., Yu, C., Lian, X., Zhang, T., and Liu, J. Dou- blesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression. In Interna- tional Conference on Machine Learning, pp. 6155–6165. PMLR, 2019. Tsitsiklis, J., Bertsekas, D., and Athans, M. Distributed asynchronous deterministic and stochastic gradient op- timization algorithms. IEEE transactions on automatic control, 31(9):803–812, 1986. Uribe, C. A., Lee, S., Gasnikov, A., and Nedi´c, A. A dual approach for optimal algorithms in distributed optimiza- tion over networks. Optimization Methods and Software, pp. 1–40, 2020. Wang, J. and Joshi, G. Adaptive communication strategies to achieve the best error-runtime trade-off in local-update sgd. In Systems and Machine Learning (SysML) Confer- ence, 2019. Wang, J., Tantia, V ., Ballas, N., and Rabbat, M. SlowMo: Improving communication-efﬁcient distributed sgd with slow momentum. arXiv preprint arXiv:1910.00643 , 2019. Xin, R., Khan, U. A., and Kar, S. An improved convergence analysis for decentralized online stochastic non-convex optimization. arXiv preprint arXiv:2008.04195, 2020. Xu, J., Zhu, S., Soh, Y . C., and Xie, L. Augmented dis- tributed gradient methods for multi-agent optimization under uncoordinated constant stepsizes. In IEEE Con- ference on Decision and Control (CDC), pp. 2055–2060, Osaka, Japan, 2015. You, Y ., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2019. Yu, H., Yang, S., and Zhu, S. Parallel restarted sgd with faster convergence and less communication: Demystify- ing why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelli- gence, volume 33, pp. 5693–5700, 2019.Accelerating Gossip SGD with Periodic Global Averaging Yuan, K., Ling, Q., and Yin, W. On the convergence of decentralized gradient descent. SIAM Journal on Opti- mization, 26(3):1835–1854, 2016. Yuan, K., Ying, B., Zhao, X., and Sayed, A. H. Exact dffusion for distributed optimization and learning – Part I: Algorithm development. IEEE Transactions on Signal Processing, 67(3):708 – 723, 2019. Yuan, K., Alghunaim, S. A., Ying, B., and Sayed, A. H. On the inﬂuence of bias-correction on distributed stochastic optimization. IEEE Transactions on Signal Processing, 2020. Zhang, J., De Sa, C., Mitliagkas, I., and R ´e, C. Paral- lel sgd: When does averaging help? arXiv preprint arXiv:1606.07365, 2016. Zinkevich, M., Weimer, M., Li, L., and Smola, A. J. Paral- lelized stochastic gradient descent. In Advances in neural information processing systems, pp. 2595–2603, 2010.Accelerating Gossip SGD with Periodic Global Averaging A. Preliminary Notation. We ﬁrst introduce necessary notations as follows. • x(k) = [(x(k) 1 )T; (x(k) 2 )T; ··· ; (x(k) n )T] ∈Rn×d • ∇F(x(k); ξ(k+1)) = [∇F1(x(k) 1 ; ξ(k+1) 1 )T; ··· ; ∇Fn(x(k) n ; ξ(k+1) n )T] ∈Rn×d • ∇f(x(k)) = [∇f1(x(k) 1 )T; ∇f2(x(k) 2 )T; ··· ; ∇fn(x(k) n )T] ∈Rn×d • ¯x(k) = [(¯x(k))T; (¯x(k))T; ··· ; (¯x(k))T] ∈Rn×d where ¯x(k) = 1 n ∑n i=1 x(k) i • x⋆ = [(x⋆)T; (x⋆)T; ··· ; (x⋆)T] ∈Rn×d where x⋆ is the global solution to problem (1). • W = [wij] ∈Rn×n. • 1n = col{1,1,··· ,1}∈ Rn. • Given two matrices x,y ∈Rn×d, we deﬁne inner product ⟨x,y⟩= tr(xTy) and the Frobenius norm ∥x∥2 F = ⟨x,x⟩. • Given W ∈Rn×n, we let ∥W∥2 = σmax(W) where σmax(·) denote the maximum sigular value. Gossip-PGA in matrix notation. For ease of analysis, we rewrite the main recursion of Gossip-PGA in matrix notation: x(k+1) = { W(x(k) −γ∇F(x(k); ξ(k+1))) If mod(k+ 1,H) ̸= 0 1 n1n1T n(x(k) −γ∇F(x(k); ξ(k+1))) otherwise (10) Gradient noise. We repeat the deﬁnition of ﬁltration in Assumption 2 here for convenience. F(k)= { {x(k) i }n i=1,{ξ(k) i }n i=1,··· ,{x(0) i }n i=1,{ξ(0) i }n i=1 } (11) • With Assumption 2, we can evaluate the magnitude of the averaged gradient noise: E[∥1 n n∑ i=1 ∇Fi(x(k−1) i ; ξ(k) i ) −1 n n∑ i=1 ∇fi(x(k−1) i )∥2|F(k−1)] (a) = 1 n2 n∑ i=1 E[∥∇Fi(x(k−1) i ; ξ(k) i ) −∇fi(x(k−1) i )∥2|F(k−1)] (5) ≤σ2 n (12) where (a) holds because ξ(k) i is independent for any iand E[∇Fi(x(k−1) i ; ξ(k) i ) −∇fi(x(k−1) i )|F(k−1)] = 0. • We deﬁne gradient noise as s(k) i = ∇Fi(x(k−1) i ; ξ(k) i ) −∇fi(x(k−1) i ). For any 0 ≤j ≤k<ℓ , it holds that E[ ( s(k) i )T s(ℓ) i |F(j)] (a) = EF(ℓ−1)/F(j) [ E[ ( s(k) i )T s(ℓ) i |F(ℓ−1)] ] = EF(ℓ−1)/F(j) [( s(k) i )T E[s(ℓ) i |F(ℓ−1)] ](4) = 0 (13) where F(ℓ−1)/F(j) := { {x(j+1) i }n i=1,{ξ(j+1) i }n i=1,··· ,{x(ℓ−1) i }n i=1,{ξ(ℓ−1) i }n i=1 } and (a) holds due to the law of total expectation. • For any 0 ≤k<ℓ , it holds that E[∥s(ℓ) i ∥2|F(k)] = EF(ℓ−1)/F(k) [ E[∥s(ℓ) i ∥2|Fℓ−1] ](5) ≤EF(ℓ−1)/F(k) [ σ2] = σ2 (14)Accelerating Gossip SGD with Periodic Global Averaging Smoothness. Since each fi(x) is assumed to be L-smooth in Assumption 1, it holds that f(x) = 1 n ∑n i=1 fi(x) is also L-smooth. As a result, the following inequality holds for any x,y∈Rd: fi(x) −fi(y) −L 2 ∥x−y∥2 ≤⟨∇fi(y),x−y⟩ (15) Smoothness and convexity. If each fi(x) is further assumed to be convex (see Assumption 4), it holds that f(x) = 1 n ∑n i=1 fi(x) is also convex. For this scenario, it holds for any x,y∈Rd that: ∥∇f(x) −∇f(x⋆)∥2 ≤2L ( f(x) −f(x⋆) ) (16) fi(x) −fi(y) ≤⟨∇fi(x),x−y⟩ (17) Network weighting matrix. Suppose a weighting matrix W ∈Rn×n satisﬁes Assumption 3, it holds that ∥W −1 n11T∥2 ≤β, ∥(W −1 n11T)k∥2 ≤βk, ∀k. (18) Submultiplicativity of the Frobenius norm. Given matrices W ∈Rn×n and y ∈Rn×d, it holds that ∥Wy∥F ≤∥W∥2∥y∥F. (19) To verify it, by lettingyj be the j-th column of y, we have ∥Wy∥2 F = ∑d j=1 ∥Wyj∥2 2 ≤∑d j=1 ∥W∥2 2∥yj∥2 2 = ∥W∥2 2∥y∥2 F. B. Convergence analysis for convex scenario B.1. Proof Outline for Theorem 1 The following lemma established in (Koloskova et al., 2020, Lemma 8) shows howE∥¯x(k) −x⋆∥2 evolves with iterations. Lemma 1 (DESCENT LEMMA (Koloskova et al., 2020)). When Assumptions 1–4 hold and step-size γ < 1 4L, it holds for k= 1,2,··· that E∥¯x(k) −x⋆∥2 ≤E∥¯x(k−1) −x⋆∥2 −γ ( Ef(¯x(k−1)) −f(x⋆) ) + 3Lγ 2n E∥x(k−1) −¯x(k−1)∥2 F + γ2σ2 n , (20) where ¯x(k) = [(¯x(k))T; ··· ; (¯x(k))T] ∈Rn×d. Remark 7. It is worth noting that Lemma 1 also holds for the standard Gossip SGD algorithm. The periodic global averaging step does not affect this descent lemma. Next we establish the consensus lemmas in which Gossip-PGA is fundamentally different from Gossip SGD. Note that Gossip-PGA takes global average every H iterations. For any k= 0,1,···, we deﬁne τ(k) = max{ℓ: ℓ≤kand mod(ℓ,H) = 0} (21) as the most recent iteration when global average is conducted. In Gossip-PGA, it holds that ¯xτ(k) = xτ(k) i for any i∈[n]. This is different from Gossip SGD in which ¯x(k) = x(k) i can only happen when k= 0. For Gossip-PGA, the real challenge is to investigate how the periodic global averaging helps reduce consensus error and hence accelerate the convergence rate. In fact, there are two forces in Gossip-PGA that drive local model parameters to reach consensus: the gossip communication and the periodic global averaging. Each of these two forces is possible to dominate the consensus controlling in different scenarios: Scenario I. Global averaging is more critical to guarantee consensus on large or sparse network, or when global averaging is conducted frequently. Scenario II. Gossip communication is more critical to guarantee consensus on small or dense network, or when global averaging is conducted infrequently. Ignoring either of the above scenario will lead to incomplete or even incorrect conclusions, as shown in Remark 5. In the following, we will establish a speciﬁc consensus lemma for each scenario and then unify them into one that precisely characterize how the consensus distance evolves with iterations in Gossip-PGA.Accelerating Gossip SGD with Periodic Global Averaging Lemma 2 (CONSENSUS LEMMA : G LOBAL AVERAGING DOMINATING ). Under Assumptions 1–4, it holds for k = τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E∥x(k+1) −¯x(k+1)∥2 F ≤6Hγ2β2L2 k∑ ℓ=τ(k) βk−ℓE∥x(ℓ)−¯x(ℓ)∥2 F + 12nHγ2β2L k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆))+2nγ2β2Cβ(3b2+σ2) (22) where b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2 implies data heterogeneity and Cβ = ∑H−1 k=0 βk = (1 −βH)/(1 −β). Lemma 3 (CONSENSUS LEMMA : G OSSIP DOMINATING ). Under Assumptions 1–4, it holds for k = τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E∥x(k+1) −¯x(k+1)∥2 F ≤6γ2β2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ)−¯x(ℓ)∥2 F + 12nγ2β2L 1 −β k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆))+2nγ2β2Cβ( 3b2 1 −β+σ2) (23) Observing Lemmas 2 and 3, it is found that bounds (22) and (23) are in the same shape except for some critical coefﬁcients. With the following relation: { y≤a1x+ b y≤a2x+ b =⇒ y≤min{a1,a2}x+ b, (24) we can unify Lemmas 2 and 3 into: Lemma 4 (UNIFIED CONSENSUS LEMMA ). Under Assumptions 1–4, it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E∥x(k+1) −¯x(k+1)∥2 F ≤6Dβγ2β2L2 k∑ ℓ=τ(k) βk−ℓE∥x(ℓ)−¯x(ℓ)∥2 F + 12nDβγ2β2L k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆))+2nγ2β2Cβ(3Dβb2+σ2) (25) where b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2 implies data heterogeneity, Cβ = ∑H−1 k=0 βk = (1 −βH)/(1 −β), and Dβ = min{ 1 1−β,H}. Remark 8. This lemma reﬂects how the network topology and the global averaging period contribute to the consensus controlling. For scenario I where the network is large or sparse such that 1/(1 −β) > H, Lemma 4 indicates that the consensus error is mainly controlled by the global averaging period (i.e., Dβ = H). On the other hand, for scenario II where the network is small or dense such that 1/(1 −β) < H, Lemma 4 indicates that the consensus error is mainly controlled by gossip communication (i.e., Dβ = 1/(1 −β)). Using Lemma 4, we derive the upper bound of the weighted running average of E∥xk −¯xk∥2 F: Lemma 5 (RUNNING CONSENSUS LEMMA ). Suppose Assumptions 1–4 hold and step-size γ <1/(4LβDβ), it holds for T >0 that 1 T + 1 T∑ k=0 E∥x(k) −¯x(k)∥2 F ≤2c2Dβ T + 1 T∑ k=0 ( Ef(¯x(k)) −f(x⋆) ) + 2c3 (26) where c2 and c3 are constants deﬁned as c2 = 12nβ2Dβγ2L, (27) c3 = 2nβ2γ2Cβ(3Dβb2 + σ2) (28)Accelerating Gossip SGD with Periodic Global Averaging With Lemmas 1 and 5, we can establish the ﬁnal convergence Theorem 1, see the proof in Sec. B.5. B.2. Proof of Lemma 1. This lemma was ﬁrst established in (Koloskova et al., 2020, Lemma 8). We made slight improvement to tight constants appeared in step-size ranges and upper bound (20). For readers’ convenience, we repeat arguments here. Recall the algorithm in (10). By taking the average on both sides, we reach that ¯x(k) −x⋆ = ¯x(k−1) −x⋆ −γ n n∑ i=1 ∇Fi(x(k−1) i ; ξ(k) i ), ∀k= 1,2,··· (29) By taking expectation over the square of both sides of the above recursion conditioned on F(k−1), we have E[∥¯x(k) −x⋆∥2|F(k−1)] (4) = ∥¯x(k−1) −x⋆ −γ n n∑ i=1 ∇fi(x(k−1) i )∥2 + γ2E[∥1 n n∑ i=1 ∇Fi(x(k−1) i ; ξ(k) i ) −1 n n∑ i=1 ∇fi(x(k−1) i )∥2|F(k−1)] (12) ≤∥ ¯x(k−1) −x⋆ −γ n n∑ i=1 ∇fi(x(k−1) i )∥2 + γ2σ2 n (30) Note that the ﬁrst term can be expanded as follows. ∥¯x(k−1) −x⋆ −γ n n∑ i=1 ∇fi(x(k−1) i )∥2 = ∥¯x(k−1) −x⋆ −γ n n∑ i=1 [∇fi(x(k−1) i ) −∇fi(x⋆)]∥2 = ∥¯x(k−1) −x⋆∥2 −2γ n n∑ i=1 ⟨¯x(k−1) −x⋆,∇fi(x(k−1) i ) −∇fi(x⋆)⟩    (A) + γ2∥1 n n∑ i=1 [∇fi(x(k−1) i ) −∇fi(x⋆)] ∥2    (B) (31) We now bound the term (A): 2γ n n∑ i=1 ⟨¯x(k−1) −x⋆,∇fi(x(k−1) i ) −∇fi(x⋆)⟩ = 2γ n n∑ i=1 ⟨¯x(k−1) −x⋆,∇fi(x(k−1) i )⟩ = 2γ n n∑ i=1 ⟨¯x(k−1) −x(k−1) i ,∇fi(x(k−1) i )⟩+ 2γ n n∑ i=1 ⟨x(k−1) i −x⋆,∇fi(x(k−1) i )⟩ (a) ≥ 2γ n n∑ i=1 ( fi(¯x(k−1)) −fi(x(k−1) i ) −L 2 ∥¯x(k−1) −x(k−1) i ∥2 ) + 2γ n n∑ i=1 ( fi(x(k−1) i ) −fi(x⋆) ) = 2γ n n∑ i=1 ( fi(¯x(k−1)) −fi(x⋆) ) −γL n ∥¯x(k−1) −x(k−1)∥2 F = 2γ ( f(¯x(k−1)) −f(x⋆) ) −γL n ∥¯x(k−1) −x(k−1)∥2 F (32) where (a) holds because of the inequality (15) and (17). We next bound term (B) in (31): γ2∥1 n n∑ i=1 [∇fi(x(k−1) i ) −∇fi(x⋆)] ∥2Accelerating Gossip SGD with Periodic Global Averaging = γ2∥1 n n∑ i=1 [∇fi(x(k−1) i ) −∇fi(¯x(k−1)) + ∇fi(¯x(k−1)) −∇fi(x⋆)] ∥2 (3) ≤2γ2L2 n ∥x(k−1) −¯x(k−1)∥2 F + 2γ2∥∇f(¯x(k−1)) −∇f(x⋆)∥2 (16) ≤ 2γ2L2 n ∥x(k−1) −¯x(k−1)∥2 F + 4Lγ2( f(¯x(k−1)) −f(x⋆) ) . (33) Substituting (33) and (32) into (31), we have ∥¯x(k−1) −x⋆ −γ n n∑ i=1 ∇fi(x(k−1) i )∥2 ≤∥¯x(k−1) −x⋆∥2 −2γ(1 −2Lγ) ( f(¯x(k−1)) −f(x⋆) ) + (γL n + 2γ2L2 n ) ∥¯x(k−1) −x(k−1)∥2 F ≤∥¯x(k−1) −x⋆∥2 −γ ( f(¯x(k−1)) −f(x⋆) ) + 3γL 2n ∥¯x(k−1) −x(k−1)∥2 F (34) where the last inequality holds when γ < 1 4L. Substituting the above inequality into (30) and taking expectation over the ﬁltration, we reach the result in (20). B.3. Proofs of Lemma 2 and 3. Note the gossip averaging is conducted when k= τ(k),τ(k) + 1,··· ,τ(k) + H−1, i.e., x(k+1) = W(x(k) −γ∇F(x(k); ξ(k+1))). (35) Since ¯x(k+1) = 1 n11Tx(k+1), it holds that ¯x(k+1) = 1 n11T(x(k) −γ∇F(x(k); ξ(k+1))). (36) With the above two recursions, we have x(k+1) −¯x(k+1) = (W −1 n11T)(x(k) −γ∇F(x(k); ξ(k+1))) (37) In the following we will derive two upper bounds for E∥x(k+1) −¯x(k+1)∥2. Bound in Lemma 2. With (37), we have x(k+1) −¯x(k+1) = (W −1 n11T)(x(k) −γ∇F(x(k); ξ(k+1))) = (W −1 n11T)(x(k) −¯x(k) −γ∇F(x(k); ξ(k+1))) = (W −1 n11T)k+1−τ(k)(x(τ(k)) −¯x(τ(k))) −γ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇F(x(ℓ); ξ(ℓ+1)) = −γ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇F(x(ℓ); ξ(ℓ+1)) (38) where the last equality holds becausex(τ(k)) = ¯x(τ(k)) after the global averaging at iterationτ(k). With the above inequality, we have E[∥x(k+1) −¯x(k+1)∥2 F|F(τ(k))] = γ2E[∥ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇F(x(ℓ); ξ(ℓ+1))∥2 F|F(τ(k))]Accelerating Gossip SGD with Periodic Global Averaging ≤2γ2E[∥ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇f(x(ℓ))∥2 F|F(τ(k))] +2γ2E [ ∥ k∑ ℓ=τ(k) (W−1 n11T)k+1−ℓ[∇F(x(ℓ); ξ(ℓ+1))−∇f(xℓ)]∥2 F|F(τ(k))] (13) = 2 γ2E[∥ k∑ ℓ=τ(k) (W −1 n11T)k+1−ℓ∇f(x(ℓ))∥2 F|F(τ(k))] +2γ2 k∑ ℓ=τ(k) E [ ∥(W−1 n11T)k+1−ℓ[∇F(x(ℓ); ξ(ℓ+1))−∇f(xℓ)]∥2 F|F(τ(k))] (a) ≤2γ2(k+ 1 −τ(k)) k∑ ℓ=τ(k) β2(k+1−ℓ)E[∥∇f(x(ℓ))∥2 F|F(τ(k))] + 2nγ2σ2 k∑ ℓ=τ(k) β2(k+1−ℓ) ≤2γ2H k∑ ℓ=τ(k) β2(k+1−ℓ)E[∥∇f(x(ℓ))∥2 F|F(τ(k))] + 2nγ2β2σ2Cβ (39) where inequality (a) holds because of (14), (18) and (19), and the last inequality holds because ∑k ℓ=τ(k) β2(k+1−ℓ) ≤ ∑H ℓ=1 β2ℓ = β2(1−β2H)/(1−β2) ≤β2(1−βH)/(1−β) = β2Cβ where we deﬁne Cβ = ∑H−1 ℓ=0 βℓ = (1−βH)/(1−β). Note that ∥∇f(x(ℓ))∥2 F = ∥∇f(x(ℓ)) −∇f(¯x(ℓ)) + ∇f(¯x(ℓ)) −∇f(x⋆) + ∇f(x⋆)∥2 F ≤3∥∇f(x(ℓ)) −∇f(¯x(ℓ))∥2 F + 3∥∇f(¯x(ℓ)) −∇f(x⋆)∥2 F + 3∥∇f(x⋆)∥2 F ≤3L2∥x(ℓ) −¯x(ℓ)∥2 F + 6nL(f(¯x(ℓ)) −f(x⋆)) + 3nb2 (40) where the last inequality holds because of (3) and (16). Notation b2 is deﬁned as b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2. Substituting (40) into (39), it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E[∥x(k+1) −¯x(k+1)∥2 F|F(τ(k))] ≤6Hγ2L2 k∑ ℓ=τ(k) β2(k+1−ℓ)E[∥x(ℓ) −¯x(ℓ)∥2 F|F(τ(k))] + 12nHγ2L k∑ ℓ=τ(k) β2(k+1−ℓ)E[f(¯x(ℓ)) −f(x⋆)|F(τ(k))]+2nγ2β2Cβ(3Hb2 + σ2) (41) By taking expectations over the ﬁltration F(τ(k)), we have E∥x(k+1) −¯x(k+1)∥2 F ≤6Hβ2γ2L2 k∑ ℓ=τ(k) β2(k−ℓ)E∥x(ℓ) −¯x(ℓ)∥2 F + 12nHβ2γ2L k∑ ℓ=τ(k) β2(k−ℓ)E(f(¯x(ℓ)) −f(x⋆)) + 2nγ2β2Cβ(3Hb2 + σ2) ≤6Hβ2γ2L2 k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 12nHβ2γ2L k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆)) + 2nγ2β2Cβ(3Hb2 + σ2) (42) Bound in Lemma 3. With (37), it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E[∥x(k+1) −¯x(k+1)∥2 F|F(k)]Accelerating Gossip SGD with Periodic Global Averaging = E[∥(W −1 n11T) ( x(k) −¯x(k) −γ∇F(x(k); ξ(k+1)) ) ∥2 F|F(k)] (4) = ∥(W −1 n11T) ( x(k) −¯x(k) −γ∇f(x(k)) ) ∥2 F + γ2E[∥(W −1 n11T) ( ∇F(x(k); ξ(k+1)) −∇f(x(k)) ) ∥2 F|F(k)] ≤∥(W −1 n11T) ( x(k) −¯x(k) −γ∇f(x(k)) ) ∥2 F + nγ2β2σ2 (43) where the last inequality holds because of (5) and (18). We now bound the ﬁrst term: ∥(W −1 n11T) ( x(k) −¯x(k) −γ∇f(x(k)) ) ∥2 F (a) ≤1 t∥(W −1 n11T) ( x(k) −¯x(k)) ∥2 F + γ2 1 −t∥(W −1 n11T)∇f(x(k))∥2 F (b) = β∥x(k) −¯x(k)∥2 F + β2γ2 1 −β∥∇f(x(k))∥2 F = β∥x(k) −¯x(k)∥2 F + β2γ2 1 −β∥∇f(x(k)) −∇f(¯x(k)) + ∇f(¯x(k)) −∇f(x⋆) + ∇f(x⋆)∥2 F (c) ≤β∥x(k) −¯x(k)∥2 F + 3β2γ2L2 1 −β ∥x(k) −¯x(k)∥2 F + 6nβ2γ2L 1 −β ( f(¯x(k)) −f(x⋆) ) + 3nβ2γ2b2 1 −β (44) where (a) holds because of the Jensen’s inequality for anyt∈(0,1), (b) holds by setting t= β, and (c) holds because of (3) and (16). Quantity b2 = 1 n ∑n i=1 ∥∇fi(x⋆)∥2 in the last inequality. Substituting (44) into (43), we have E[∥x(k+1) −¯x(k+1)∥2 F|F(k)] ≤β∥x(k) −¯x(k)∥2 F + 3β2γ2L2 1 −β ∥x(k) −¯x(k)∥2 F + 6nβ2γ2L 1 −β ( f(¯x(k)) −f(x⋆) ) + nγ2β2σ2 + 3nβ2γ2b2 1 −β = βk+1−τ(k)∥x(τ(k)) −¯x(τ(k))∥2 F + 3β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓ∥x(ℓ) −¯x(ℓ)∥2 F + 6nβ2γ2L 1 −β k∑ ℓ=τ(k) βk−ℓ(f(¯x(ℓ)) −f(x⋆)) + nγ2β2Cβ( 3b2 1 −β + σ2) = 3β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓ∥x(ℓ) −¯x(ℓ)∥2 F + 6nβ2γ2L 1 −β k∑ ℓ=τ(k) βk−ℓ(f(¯x(ℓ)) −f(x⋆)) + nγ2β2Cβ( 3b2 1 −β + σ2) (45) By taking expectation over the ﬁltration F(k), we have E∥x(k+1) −¯x(k+1)∥2 F ≤3β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 6nβ2γ2L 1 −β k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆)) + nγ2β2Cβ( 3b2 1 −β + σ2) < 6β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 12nβ2γ2L 1 −β k∑ ℓ=τ(k) βk−ℓE(f(¯x(ℓ)) −f(x⋆)) + 2nγ2β2Cβ( 3b2 1 −β + σ2) (46) B.4. Proof of Lemma 5. To simplify the notation, we deﬁne A(k) = E∥x(k) −¯x(k)∥2 F, B (k) = Ef(¯x(k)) −f(x⋆), c1 = 6Dββ2γ2L2, c 2 = 12nDββ2γ2L, c 3 = 2nβ2γ2Cβ(3Dβb2 + σ2). (47)Accelerating Gossip SGD with Periodic Global Averaging Using these notations, we rewrite (22) for any k= 0,1,2,··· that { A(k) ≤c1 ∑k−1 ℓ=τ(k) βk−1−ℓA(ℓ) + c2 ∑k−1 ℓ=τ(k) βk−1−ℓB(ℓ) + c3 if k>τ (k) A(k) = 0 if k= τ(k) (48) We next deﬁne ΓT := {k|0 ≤k≤T and mod(k,H) = 0}, Γc T := {k|0 ≤k≤T and mod(k,H) ̸= 0}. (49) By taking the running average over both sides in (48) and recalling A(τ(k)) = 0, it holds that T∑ k=0 A(k) ≤c1 ∑ k∈Γc T k−1∑ ℓ=τ(k) βk−1−ℓA(ℓ) + c2 ∑ k∈Γc T k−1∑ ℓ=τ(k) βk−1−ℓB(ℓ) + c3(T + 1) (50) We further deﬁne ΘT := {ℓ|0 ≤ℓ≤T −1 and mod(ℓ+ 1,H) = 0}, Θc T := {ℓ|0 ≤ℓ≤T −1 and mod(ℓ+ 1,H) ̸= 0}. (51) With these notations, we have T∑ k=0 A(k) ≤c1 ∑ k∈Γc T k−1∑ ℓ=τ(k) βk−1−ℓA(ℓ) + c2 ∑ k∈Γc T k−1∑ ℓ=τ(k) βk−1−ℓB(ℓ) + c3(T + 1) = c1 ∑ ℓ∈Θc T A(ℓ)(τ(ℓ)+H−1∑ k=ℓ+1 βk−1−ℓ) + c2 ∑ ℓ∈Θc T B(ℓ)(τ(ℓ)+H−1∑ k=ℓ+1 βk−1−ℓ) + c3(T + 1) (a) ≤c1Cβ ∑ ℓ∈Θc T A(ℓ) + c2Cβ ∑ ℓ∈Θc T B(ℓ) + c3(T + 1) (b) ≤c1Cβ T∑ k=0 A(k) + c2Cβ T∑ k=0 B(k) + c3(T + 1) (c) ≤c1Dβ T∑ k=0 A(k) + c2Dβ T∑ k=0 B(k) + c3(T + 1) (52) where (a) holds because ∑τ(ℓ)+H−1 k=ℓ+1 βk−1−ℓ ≤∑H−1 k=0 βk = Cβ, (b) holds because A(k) ≥0 and B(k) ≥0, and (c) holds because Cβ = (1 −βH)/(1 −β) ≤min{ 1 1−β,H}= Dβ. If step-size γis sufﬁciently small such that 1 −c1Dβ ≥1 2 , it holds that T∑ k=0 A(k) ≤2c2Dβ T∑ k=0 B(k) + 2c3(T + 1). (53) To guarantee 1 −c1Dβ ≥1 2 , it is enough to let γ ≤1/(4LβDβ). B.5. Proof of Theorem 1 Following the notation in (47), we further deﬁne F(k) := E∥¯x(k) −x⋆∥2 F. With these notations, the inequality (20) becomes B(k) ≤F(k) γ −F(k+1) γ + γσ2 n + 3L 2nA(k) (54) Taking weighted running average over the above inequality to get 1 T + 1 T∑ k=0 B(k) ≤ F(0) (T + 1)γ + 3L 2n(T + 1) T∑ k=0 A(k) + γσ2 nAccelerating Gossip SGD with Periodic Global Averaging (53) ≤ F(0) (T + 1)γ + 6Lc2Dβ n(T + 1) T∑ k=0 B(k) + 3Lc3 n + γσ2 n ≤ F(0) (T + 1)γ + 1 2(T + 1) T∑ k=0 B(k) + 3Lc3 n + γσ2 n (55) where the last inequality holds when γ ≤1/(12LβDβ). Substituting c3 into the above inequality, we have 1 T + 1 T∑ k=0 B(k) ≤ 2F(0) (T + 1)γ + 2γσ2 n + 12Lβ2γ2Cβσ2 + 36Lβ2γ2CβDβb2. (56) The way to choose step-size γis adapted from Lemma 15 in (Koloskova et al., 2020). For simplicity, we let r0 = 2F(0), r 1 = 2σ2 n , r 2 = 12Lβ2Cβσ2 + 36Lβ2CβDβb2, (57) and inequality (56) becomes 1 T + 1 T∑ k=0 B(k) ≤ r0 (T + 1)γ + r1γ+ r2γ2. (58) Now we let γ = min { 1 12βLDβ , ( r0 r1(T+1) )1 2 , ( r0 r2(T+1) )1 3 } : • If ( r0 r2(T+1) )1 3 is the smallest, we let γ = ( r0 r2(T+1) )1 3 . With ( r0 r2(T+1) )1 3 ≤ ( r0 r1(T+1) )1 2 , (58) becomes 1 T + 1 T∑ k=0 B(k) ≤2r 1 3 2 ( r0 T + 1 )2 3 + r1 ( r0 r2(T + 1) )1 3 ≤2r 1 3 2 ( r0 T + 1 )2 3 + ( r0r1 T + 1 )1 2 . (59) • If ( r0 r1(T+1) )1 2 is the smallest, we let γ = ( r0 r1(T+1) )1 2 . With ( r0 r1(T+1) )1 2 ≤ ( r0 r2(T+1) )1 3 , (58) becomes 1 T + 1 T∑ k=0 B(k) ≤2 ( r0r1 T + 1 )1 2 + r0r2 r1(T + 1) ≤2 ( r0r1 T + 1 )1 2 + r 1 3 2 ( r0 T + 1 )2 3 . (60) • If 1 12βLDβ ≤ ( r0 r1(T+1) )1 2 and 1 12βLDβ ≤ ( r0 r2(T+1) )1 3 , we let γ = 1 12βLDβ and (58) becomes 1 T + 1 T∑ k=0 B(k) ≤12βLDβr0 T + 1 + ( r0r1 T + 1 )1 2 + r 1 3 2 ( r0 T + 1 )2 3 . (61) Combining (59), (60) and (61), we have 1 T + 1 T∑ k=0 B(k) ≤12r0LDββ T + 1 + 2 ( r0r1 T + 1 )1 2 + 2r 1 3 2 ( r0 T + 1 )2 3 . (62) Substituting constants r0, r1, and r2, we have the ﬁnal result: 1 T + 1 T∑ k=0 B(k) = O ( σ√ nT + (Cβ) 1 3 β 2 3 σ 2 3 T 2 3 + (Cβ) 1 3 (Dβ) 1 3 β 2 3 b 2 3 T 2 3 + βDβ T ) . (63)Accelerating Gossip SGD with Periodic Global Averaging C. Convergence analysis for non-convex scenario C.1. Proof Outline for Theorem 2. The proof outline for Theorem 2 is similar to that for Theorem 1. The descent lemma (Koloskova et al., 2020, Lemma 10) was established follows. Lemma 6 (DESCENT LEMMA (Koloskova et al., 2020)) . Under Assumption 1–3 and step-size γ < 1 4L, it holds for k= 1,2,··· that Ef(¯x(k)) ≤Ef(¯x(k−1)) −γ 4 E∥∇f(¯x(k−1))∥2 + γ2Lσ2 2n + 3γL2 4n E∥x(k) −¯x(k)∥2 F. (64) The consensus distance is examined in the following two lemmas. Similar to Lemma 4, we use Dβ = min{H,1/(1 −β)}. Lemma 7 (UNIFIED CONSENSUS LEMMA ). Under Assumptions 1–3 and 5, it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E∥x(k+1) −¯x(k+1)∥2 F ≤6Dββ2γ2L2 k∑ ℓ=τ(k) β2(k+1−ℓ)E∥x(ℓ) −¯x(ℓ)∥2 F + 6nDββ2γ2 k∑ ℓ=τ(k) β2(k+1−ℓ)E∥∇f(¯x(ℓ))∥2 + 2nβ2γ2Cβ(3Hˆb2 + σ2) (65) where Dβ = min{H, 1 1−β}. Lemma 8 (RUNNING CONSENSUS LEMMA ). When Assumptions 1–3 and 5 hold and step-size γ < 1 4LβDβ , it holds for any T >0 that 1 T + 1 T∑ k=0 E∥x(k) −¯x(k)∥2 F ≤ 2c2Dβ T + 1 T∑ k=0 E∥∇f(¯x(k))∥2 + 2c3 (66) where c2 and c3 are constants deﬁned as c2 = 6nDββ2γ2, (67) c3 = 2nβ2γ2Cβ(3Dβˆb2 + σ2). (68) With Lemmas 6 and 8, we can establish the convergence rate in Theorem 2. C.2. Proof of Lemma 6. This lemma was ﬁrst established in (Koloskova et al., 2020, Lemma 10). We made slight improvement to tight constants appeared in step-size ranges and upper bound (64). For readers’ convenience, we repeat arguments here. Recall that ¯x(k+1) = ¯x(k) −γ n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i ). (69) Since f(x) is L-smooth, it holds that E[f(¯x(k+1))|Fk] (15) ≤f(¯x(k)) −E [ ⟨∇f(¯x(k)),γ n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i )⟩|Fk] + γ2L 2 E[∥1 n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i )∥2|Fk] (4) = f(¯x(k)) −⟨∇f(¯x(k)),γ n n∑ i=1 ∇fi(x(k) i )⟩+ γ2L 2 E[∥1 n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i )∥2|Fk] (a) ≤f(¯x(k)) −⟨∇f(¯x(k)),γ n n∑ i=1 ∇fi(x(k) i )⟩+ γ2Lσ2 2n + γ2L 2 ∥1 n n∑ i=1 ∇fi(x(k) i )∥2 (70)Accelerating Gossip SGD with Periodic Global Averaging where (a) holds because E[∥1 n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i ) −∇fi(x(k) i ) + ∇fi(x(k) i )∥2|Fk] (4) = E[∥1 n n∑ i=1 ∇Fi(x(k) i ; ξ(k+1) i ) −∇fi(x(k) i )∥2|Fk] + ∥1 n n∑ i=1 ∇fi(x(k) i )∥2 (12) ≤ σ2 n + ∥1 n n∑ i=1 ∇fi(x(k) i )∥2. (71) Note that −⟨∇f(¯x(k)),γ n n∑ i=1 ∇fi(x(k) i )⟩= −⟨∇f(¯x(k)),γ n n∑ i=1 [∇fi(x(k) i ) −∇fi(¯x(k)) + ∇fi(¯x(k))]⟩ ≤−γ∥∇f(¯x(k))∥2 + γ 2 ∥∇f(¯x(k))∥2 + γ 2n n∑ i=1 ∥∇fi(x(k) i ) −∇fi(¯x(k))∥2 ≤−γ 2 ∥∇f(¯x(k))∥2 + γL2 2n ∥x(k) −¯x(k)∥2 F (72) and ∥1 n n∑ i=1 ∇fi(x(k) i )∥2 ≤2L2 n ∥x(k) −¯x(k)∥2 F + 2∥∇f(¯x(k))∥2 (73) Substituting (72) and (73) into (70), taking expectations over F(k) and using the fact that γ < 1 4L, we reach the result in (64). C.3. Proof of Lemma 7. Similar to the proof of Lemmas 2 and 3, we will derive two bounds for E∥x(k+1) −¯x(k+1)∥2 F: Bound 1. Following (35)-(39), it holds for k= τ(k),τ(k) + 1,··· ,τ(k) + H−1 that E[∥x(k+1) −¯x(k+1)∥2 F|F(τ(k))] ≤2γ2H k∑ ℓ=τ(k) β2(k+1−ℓ)E[∥∇f(x(ℓ))∥2 F|F(τ(k))] + 2nγ2β2σ2Cβ (74) Note that ∥∇f(x(k))∥2 F = n∑ i=1 ∥∇fi(x(k) i )∥2 = n∑ i=1 ∥∇fi(x(k) i ) −∇fi(¯x(k)) + ∇fi(¯x(k)) −∇f(¯xk) + ∇f(¯xk)∥2 ≤3L2∥x(k) −¯x(k)∥2 F + 3nˆb2 + 3n∥∇f(¯xk)∥2. (75) where the last inequality holds because of Assumption 5. Substituting (75) into (74) and taking expectation on F(τ(k)), we get E∥x(k+1) −¯x(k+1)∥2 F ≤6Hβ2γ2L2 k∑ ℓ=τ(k) β2(k−ℓ)E∥x(ℓ) −¯x(ℓ)∥2 F+6nHβ2γ2 k∑ ℓ=τ(k) β2(k−ℓ)∥∇f(¯x(ℓ))∥2+2nγ2β2Cβ(3Hˆb2 + σ2) ≤6Hγ2L2β2 k∑ ℓ=τ(k) βk−ℓ∥x(ℓ) −¯x(ℓ)∥2 F+6nHγ2β2 k∑ ℓ=τ(k) βk−ℓ∥∇f(¯x(ℓ))∥2+2nγ2β2Cβ(3Hˆb2 + σ2) (76)Accelerating Gossip SGD with Periodic Global Averaging Bound 2. Following (43) and ﬁrst two lines in (44), it holds for any k= τ(k),··· ,τ(k) + H−1 that E[∥x(k+1) −¯x(k+1)∥2 F|F(k)] ≤β∥x(k) −¯x(k)∥2 F + β2γ2 1 −β∥∇f(x(k))∥2 F + nγ2β2σ2. (77) Substituting (75) into (77), we get E[∥x(k+1) −¯x(k+1)∥2 F|F(k)] ≤ ( β+ 3β2γ2L2 1 −β ) ∥x(k) −¯x(k)∥2 F + 3nβ2γ2∥∇f(¯xk)∥2 1 −β + nγ2β2σ2 + 3nβ2γ2ˆb2 1 −β . (78) We next follow (43)–(46) and take expectation onF(k) to get E∥x(k+1) −¯x(k+1)∥2 F ≤3β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 3nβ2γ2 1 −β k∑ ℓ=τ(k) βk−ℓE∥∇f(¯x(ℓ))∥2 + nγ2β2Cβ( 3ˆb2 1 −β + σ2) ≤6β2γ2L2 1 −β k∑ ℓ=τ(k) βk−ℓE∥x(ℓ) −¯x(ℓ)∥2 F + 6nβ2γ2 1 −β k∑ ℓ=τ(k) βk−ℓE∥∇f(¯x(ℓ))∥2 + 2nγ2β2Cβ( 3ˆb2 1 −β + σ2) (79) With bounds (76) and (79), we reach the result (65). C.4. Proof of Lemma 8. We ﬁrst simplify the notation as follows: A(k) = E∥x(k) −¯x(k)∥2, B (k) = E∥∇f(¯x(k))∥2, c1 = 6Dββ2γ2L2, c 2 = 6nDββ2γ2, c 3 = 2nβ2γ2Cβ(3Dβˆb2 + σ2). (80) With these notations, we can follow the proof of Lemma 5 to get the ﬁnal result. C.5. Proof of Theorem 2. Following the notation in (80), we further deﬁne F(k) := Ef(¯x(k)). With these notations, the inequality (64) becomes B(k) ≤4F(k) γ −4F(k+1) γ + 2γLσ2 n + 3L2 n A(k) (81) Taking the weighted running average over the above inequality and divideT + 1 to get 1 T + 1 T∑ k=0 B(k) ≤ 4F(0) (T + 1)γ + 3L2 n(T + 1) T∑ k=0 A(k) + 2γLσ2 n (66) ≤ 4F(0) (T + 1)γ + 6β2L2Hc2 n(T + 1) T∑ k=0 B(k) + 6L2c3 n + 2γLσ2 n ≤ 4F(0) (T + 1)γ + 1 2(T + 1) T∑ k=0 B(k) + 6L2c3 n + 2γLσ2 n (82) where the last inequality holds when γ ≤ 1 9LHβ. Substituting c3 into the above inequality, we have 1 T + 1 T∑ k=0 B(k) ≤ 8F(0) (T + 1)γ + 4γLσ2 n + 24L2γ2β2Cβσ2 + 72L2γ2β2CβDβˆb2. (83) By following the arguments (57) – (63), we reach the result in (8).Accelerating Gossip SGD with Periodic Global Averaging D. Transient stage and transient time D.1. Transient stage derivation. (i) Gossip SGD. We ﬁrst consider the iid scenario where b2 = 0. To make the ﬁrst term dominate the other terms (see the ﬁrst line in Table 4), T has to be sufﬁciently large such that (ignoring the affects of σ) max { β 2 3 T 2 3 (1 −β) 1 3 , β (1 −β)T } ≤ 1√ nT =⇒ T ≥max { n3β4 (1 −β)2 , nβ2 (1 −β)2 } . (84) We next consider the non-iid scenario whereb2 ̸= 0. To make the ﬁrst term dominate the other terms, T has to be sufﬁciently large such that (ignoring the affects of σand b) max { β 2 3 T 2 3 (1 −β) 1 3 , β 2 3 T 2 3 (1 −β) 2 3 , β (1 −β)T } ≤ 1√ nT =⇒ T ≥max { n3β4 (1 −β)2 , n3β4 (1 −β)4 , nβ2 (1 −β)2 } . (85) When nβ >1 which usually holds for most commonly-used network topologies, inequalities (84) and (85) will result in the transient stage T = Ω( n3β4 (1−β)2 ) and T = Ω( n3β4 (1−β)4 ) for iid and non-iid scenarios, respectively. (ii) Gossip-PGA. We ﬁrst consider the iid scenario where b2 = 0. To make the ﬁrst term dominate the other terms (see the ﬁrst line in Table 4), T has to be sufﬁciently large such that (ignoring the affects of σ) max {C 1 3 ββ 2 3 T 2 3 ,βDβ T } ≤ 1√ nT =⇒ T ≥max { n3β4C2 β,nβ2D2 β } = Ω(n3β4C2 β). (86) We next consider the non-iid scenario whereb2 ̸= 0. To make the ﬁrst term dominate the other terms, T has to be sufﬁciently large such that (ignoring the affects of σand b) max {C 1 3 ββ 2 3 T 2 3 , C 1 3 βD 1 3 ββ 2 3 T 2 3 ,βDβ T } ≤ 1√ nT =⇒ T ≥max { n3β4C2 β,n3β4C2 βD2 β,nβ2D2 β } = Ω(n3β4C2 βD2 β) (87) when nβ >1. (iii) Local SGD. We ﬁrst consider the iid scenario where b2 = 0. To make the ﬁrst term dominate the other terms (see the ﬁrst line in Table 4), T has to be sufﬁciently large such that (ignoring the affects of σ) max {H 1 3 T 2 3 ,H T } ≤ 1√ nT =⇒ T ≥max { n3H2,nH2} = Ω(n3H2). (88) We next consider the non-iid scenario whereb2 ̸= 0. To make the ﬁrst term dominate the other terms, T has to be sufﬁciently large such that (ignoring the affects of σand b) max {H 1 3 T 2 3 ,H 2 3 T 2 3 ,H T } ≤ 1√ nT =⇒ T ≥max { n3H2,n3H4,nH2} = Ω(n3H4) (89) D.2. Transient time comparison The transient time comparisons between Gossip SGD and Gossip-PGA for the iid or non-iid scenario over the grid or ring topology are listed in Tables 12, 13 and 14. E. Proof of Corollary 1 The proof of Corollary 1 closely follows Theorem 1. First, the descent lemma 7 still holds for time-varying period. Second, with the facts that k+ 1 −τ(k) ≤Hmax, ∑k ℓ=τ(k) βk+1−ℓ ≤∑Hmax k=0 βk := Cβ, and ∑τ(ℓ)+H(ℓ)−1 k=ℓ+1 βk−1−ℓ ≤Accelerating Gossip SGD with Periodic Global Averaging ∑Hmax k=0 βk = Cβ, we follow Appendix C.3 and C.4 to reach the consensus distance inequality: 1 T + 1 T∑ k=0 E∥x(k) −¯x(k)∥2 ≤ 2c2Dβ T + 1 T∑ k=0 E∥∇f(¯x(k))∥2 + 2c3 (90) where c2 and c3 are constants deﬁned as c2 = 6nDββ2γ2, (91) c3 = 2nβ2γ2Cβ(3Dβˆb2 + σ2) (92) and Dβ = min{1/(1 −β),Hmax}, Cβ = ∑Hmax k=0 βk. With Lemma 6 and inequality (90), we can follow Appendix B.5 to reach the result in Corollary 1. F. Additional Experiments F.1. Implementation Details. We implement all the aforementioned algorithms with PyTorch (Paszke et al., 2019) 1.5.1 using NCCL 2.5.7 (CUDA 10.1) as the communication backend. Each server contains 8 V100 GPUs in our cluster and is treated as one node. The inter-node network fabrics are chosen from 25 Gbps TCP (which is a common distributed training platform setting) and 4×100 Gbps RoCE (which is a high-performance distributed training platform setting). All deep learning experiments are trained in the mixed precision using Pytorch extension package NVIDIA apex (https://github.com/NVIDIA/apex). For Gossip SGD related training, we use the time-varying one-peer exponential graph following (Assran et al., 2019). Workers send and receive a copy of the model’s parameters to and from its peer, thus keeping the load balancing among workers. All data are stored in the cloud storage service and downloaded to workers using HTTP during training. Image Classﬁcation The Nesterov momentum SGD optimizer is used with a linear scaling learning rate strategy. 32 nodes (each node is with 8 V100 GPUs) are used in all the experiments and the batch-size is set as 256 per node (8,192 in total). The learning rate is warmed up in the ﬁrst 5 epochs and is decayed by a factor of 10 at 30, 60 and 90 epochs. We train 120 epochs by default (unless speciﬁed otherwise) in every experiment and record the epoch and runtime when a 76% top-1 accuracy in the validation set has reached. 25 Gbps TCP network is used for inter-node communication in ResNet-50 training. In 4×100 Gbps RoCE network, the communication overhead is negligible given the high computation-to-communication ratio nature of ResNet models and Parallel SGD with computation and communication pipeline is recommended. We use a period 6 for both Local SGD and Gossip-PGA. In Gossip-AGA, the averaging period is set to 4 in the warm-up stage and changed adaptively afterwards, roughly 9% iterations conduct global averaging. Language Modeling All experiments are based on NVIDIA BERT implementation with mixed precision support and LAMB optimizer (You et al., 2019). 8 nodes are used in all the experiments with a batch-size 64 per GPU (4096 in total). We do not use gradient accumulation as it is not vertical with Local SGD. We only do phase 1 training and indicate the decreasing of training loss as convergence speed empirically. The learning rate is scaled to 3.75e−4 initially and decayed in a polynomial policy with warm-up. The phase 1 training consists of 112,608 steps in all experiments. We use a period 6 for both Local SGD and Gossip-PGA. In Gossip-AGA, the averaging period is set to 4 in the warm-up phase and changed adaptively afterwards, roughly 9.6% iterations conduct global averaging. GOSSIP SGD G OSSIP -PGA TRANSIENT ITER . O(n5) O(n4) SINGLE COMM . O(θd+ α) O(θd+ √nα) TRANSIENT TIME O(n5θd+ n5α) O(n4θd+ n4.5α) Table 12.Transient time comparison between Gossip SGD and Gossip-PGA for iid scenario over the speciﬁc grid (1 −β = O(1/n)) topologiy. We choose H = √nas the period in Gossip-PGA.Accelerating Gossip SGD with Periodic Global Averaging GOSSIP SGD G OSSIP -PGA TRANSIENT ITER . O(n11) O(n5) SINGLE COMM . O(θd+ α) O(θd+ √nα) TRANSIENT TIME O(n11θd+ n11α) O(n5θd+ n5.5α) Table 13.Transient time comparison between Gossip SGD and Gossip-PGA for non-iid scenario over the speciﬁc ring (1−β = O(1/n2)) topologiy. We choose H = √nas the period in Gossip-PGA. GOSSIP SGD G OSSIP -PGA TRANSIENT ITER . O(n7) O(n4) SINGLE COMM . O(θd+ α) O(θd+ √nα) TRANSIENT TIME O(n7θd+ n7α) O(n4θd+ n4.5α) Table 14.Transient time comparison between Gossip SGD and Gossip-PGA for iid scenario over the speciﬁc ring (1 −β = O(1/n2)) topologiy. We choose H = √nas the period in Gossip-PGA. Transient stage (Gossip-PGA)Transient stage (Gossip SGD)Transient stage (Gossip SGD)Transient stage(Gossip-PGA) Transient stage (Gossip-PGA)Transient stage (Gossip SGD) Figure 4.Convergence comparison between Gossip-PGA, Gossip SGD and parallel SGD on the logistic regression problem in iid data distributed setting over the ring topology. 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 error n=100; non-iid data Gossip SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 error n=100; non-iid data Gossip SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 2 10 1 100 error n=100; non-iid data Gossip SGD Gossip-PGA Parallel SGD Figure 5.Convergence comparison between Gossip-PGA, Gossip SGD and parallel SGD on the logistic regression problem in non-iid data distributed setting over the exponential graph (left), grid (middle) and ring (right) topology. F.2. More experiments on convex logistic regression. In this subsection we will test the performance of Gossip-PGA with iid data distribution and on different topologies. We will also compare it with Local SGD. Experiments on iid dataset. Figure 4 illustrates how Gossip SGD and Gossip-PGA converges under the iid data distributed setting over the ring topology. Similar to the non-iid scenario shown in Figure 1, it is observed that Gossip-PGA always converges faster (or has shorter transient stages) than Gossip SGD. When network size gets larger and henceβ →1, the superiority of Gossip-PGA gets more evident. Moreover, it is also noticed that the transient stage gap between Gossip SGDAccelerating Gossip SGD with Periodic Global Averaging 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Exponential Local SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Grid Local SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Ring Local SGD Gossip-PGA Parallel SGD Figure 6.Convergence comparison between Gossip-PGA, Local SGD and parallel SGD on the logistic regression problem in non-iid data distributed setting over the exponential graph (left), grid (middle) and ring (right) topology. 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Grid; H=16 Local SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Grid; H=32 Local SGD Gossip-PGA Parallel SGD 0 1000 2000 3000 4000 5000 6000 7000 8000 iteration 10 3 10 2 10 1 100 101 error n=50; Grid; H=64 Local SGD Gossip-PGA Parallel SGD Figure 7.Convergence comparison between Gossip-PGA, Local SGD and parallel SGD on the logistic regression problem in non-iid data distributed setting over the grid topology with period H = 16 (left), H = 32 (middle), H = 64 (right). and Gossip-PGA is smaller than the non-iid scenario in all three plots in Figure 4. All these observations are consistent with the transient stage comparisons in Table 2. Experiments on different topologies. Figure 5 illustrates how Gossip SGD and Gossip-PGA converges over the exponential graph, grid and ring topology. For all plots, it is observed that Gossip-PGA is no worse than Gossip SGD. Moreover, as the network gets sparser and hence β →1 from the left plot to right, it is observed that the superiority of Gossip-PGA gets more evident, which is consistent with the transient stage comparisons between Gossip SGD and Gossip-PGA in Table 2. Comparison with Local SGD. Figure 6 illustrates how Local SGD and Gossip-PGA converges over the exponential graph, grid and ring topology. The period is set as H = 16. In all three plots, Gossip-PGA always converges faster than Local SGD because of the additional gossip communications. Moreover, since the exponential graph has the smallest β, it is observed Gossip-PGA has almost the same convergence performance as parallel SGD. Figure 7 illustrates how Local SGD and Gossip-PGA converges over the grid topology with different periods. It is observed that Gossip-PGA can be signiﬁcantly faster when H is large. All these observations are consistent with the transient stage comparisons in Table 3. F.3. More experiments on image classiﬁcation. Training accuracy. Figure F.3 shows the iteration-wise and time-wise training accuracy curves of aforementioned algorithms separately. In the left ﬁgure, it is observed Gossip-PGA/AGA converges faster (in iteration) and more accurate than local and Gossip SGD, which is consistent with our theory. In the right ﬁgure, it is observed that Gossip-PGA/AGA is the fastest method (in time) that can reach the same training accuracy as parallel SGD. The effect of averaging period. Table 15 compares the top-1 accuracy in the validation set with a different averaging period setting in Gossip-PGA SGD. Compared to Gossip SGD, a relatively large global averaging period (48), roughly 2.1% iterations with global averaging can still result in 0.32% gain in validation accuracy. With a moderate global averaging period (6/12), the validation accuracy is comparable with the parallel SGD baseline. The communication overhead of global averaging can be amortized since it happens every H iterations.Accelerating Gossip SGD with Periodic Global Averaging PARALLEL SGD G OSSIP SGD G OSSIP -PGA PERIOD H - - 3 6 12 24 48 VAL ACC.(%) 76.22 75.34 76.19 76.28 76.04 75.68 75.66 Table 15.Comparison of Top-1 validation accuracy with different averaging period setting in Gossip-PGA. 0 2500 5000 7500 10000 12500 15000 17500 Iterations 0 10 20 30 40 50 60 70 80Training Acc 16000 18000 75.0 77.5 Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA 0 1000 2000 3000 4000 5000 6000 7000 8000 Training time 0 10 20 30 40 50 60 70 80Training Acc Parallel SGD Gossip SGD Local SGD Gossip-PGA Gossip-AGA Figure 8.Convergence results on the ImageNet classiﬁcation task.(a) Iteration-wise convergence in terms of training accuracy. (b) runtime-wise convergence speed in terms of training accuracy. Experiments on SGD optimizer (without momentum). In previous Imagenet training, Nesterov momentum SGD optimizer is used. Following common practice (Lian et al., 2017; Assran et al., 2019; Tang et al., 2018), we establish the convergence rate of the non-accelerated method while running experiments with momentum. For the sake of clarity, we further add a new series of experiments on Gossip-SGD without momentum, see Table 16. Gossip-PGA still outperform Gossip-SGD utilizing the SGD optimizer. METHOD ACC. % PARALLEL SGD 69.5 GOSSIP SGD 68.47 GOSSIP -PGA 69.21 Table 16.Comparison of validation accuracy of Imagenet training on different algorithms with SGD optimizer. G. Implementation of Gossip AGA Practical consideration. The Gossip-AGA algorithm is listed in Algorithm 2. We use a counter Cto record the number of gossip iterations since last global averaging. The global averaging period H is initialized to a small value Hinit (e.g. 2∼4). Once Cequals to current H, global averaging happens. In practice, we sample loss scores for the ﬁrst fewer iterations and get a Finit estimation in a running-average fashion. We remove the exponential term in the loss score ratio for ﬂexible period adjustment. H. Comparison of communication overhead between gossip and All-Reduce Table 17 compares the overhead of different communication styles in two deep training tasks. The implementation details follow Appendix F. For each proﬁling, we run a 500 iterations and take their average as the iteration time. As typically All-Reduce implementation containing overlapping between computation and communication, we run a series of separate experiments which do not perform communication (Column 2) to get communication overhead fairly (the ﬁgures in the brackets). For ResNet-50 training, gossip introduces 150ms communication overhead while All-Reduce needs 278ms. ForAccelerating Gossip SGD with Periodic Global Averaging Algorithm 2 Gossip-AGA Require: Initialize x0,i = x0, learning rate γ >0, topology matrix W for all nodes i ∈{1,2,...,n }, global averaging period H = Hinit, C ←0, Finit ←0, warmup iterations Kw for k= 0,1,2,...,T −1, every node ido C ←C+ 1 Sample mini-batch data ξ(k+1) i from local dataset Compute stochastic gradient ∇Fi(x(k) i ; ξ(k+1) i ) and loss Fi(x(k) i ; ξ(k+1) i ) Conduct local udpate x (k+ 1 2 ) i = x(k) i −γ∇Fi(x(k) i ; ξ(k+1) i ) if C == H then C ←0 x(k+1) i ←1 n ∑n j=1 xk+ 1 2 ,j F(xk; ξk) = 1 n ∑n i=1 Fi(xk,i; ξk,i) if k<K w then Finit ←1 2 (Finit + F(xk; ξk)) else H ← ⌈ Finit F(xk;ξk) Hinit ⌉ else x(k+1) i = ∑ j∈Ni wijx (k+ 1 2 ) j MODEL ITERATION TIME (MS) NO COMMUNICATION ALL-REDUCE GOSSIP RESNET-50 146 424 (278) 296 (150) BERT 445 1913.8 (1468.8) 1011.5 (566.5) Table 17.Comparison of communication overhead between gossip and All-Reduce in terms of runtime. BERT training, gossip introduces 566.5ms communication overhead while All-Reduce needs 1468.8ms with the tremendous model size of BERT-Large.",
      "meta_data": {
        "arxiv_id": "2105.09080v1",
        "authors": [
          "Yiming Chen",
          "Kun Yuan",
          "Yingya Zhang",
          "Pan Pan",
          "Yinghui Xu",
          "Wotao Yin"
        ],
        "published_date": "2021-05-19T11:59:25Z",
        "pdf_url": "https://arxiv.org/pdf/2105.09080v1.pdf"
      }
    }
  ]
}