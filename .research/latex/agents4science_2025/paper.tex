\documentclass{article}

\usepackage{agents4science_2025}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{nicefrac}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{xcolor}

\usepackage{tikz}
\usepackage{pgfplots}

\usepackage{float}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{microtype}
\usepackage{booktabs}


\title{SYMPHONY: Retention-Aware Meta-Plasticity and Predictive Scheduling for Continual Learning in Sensor Swarms}

\author{AIRAS}

\begin{document}

\maketitle

\begin{abstract}
Continual learning on fleets of sub-milliwatt micro-controllers is hamstrung by the physics of non-volatile memories whose retention spans hours to years, by volatile thermal and energy environments, and by the absence of benchmarks that expose fleet-scale heterogeneity. Current controllers optimise endurance inside a single device, treat short-retention pages as expendable buffers, refresh reactively, and exchange data without privacy guarantees. We introduce SYMPHONY, a cross-layer framework that: repurposes 1-3 h MRAM pages as fast weights through Retention-Aware Meta-Plasticity; couples a 21 k-parameter TinyTransformer forecaster with a convex model-predictive controller that allocates endurance and retention two hours ahead; barters mid-retention pages among nodes via a restless-bandit protocol to equalise wear; injects differential-privacy noise directly at write time; publishes the 120-day SwarmRet-120 trace with per-cell failures; and releases IGRE-Lite, a 4 kB MRAM macro for in-situ noise generation. We formalise the joint optimisation, publish cycle-accurate simulators and three experiments designed to verify gains in accuracy, adaptation latency, wear variance, energy adherence and privacy. A preliminary public run trained a vanilla ResNet-18 on CIFAR-10, achieving 86.6 \% accuracy but exercising none of SYMPHONY’s mechanisms. No empirical evidence yet supports the claimed benefits; we analyse the gap and detail the resources required for a complete fleet-level evaluation.
\end{abstract}

\section{Introduction}
Continual learning (CL) promises on-device models that adapt for years without cloud connectivity, yet most literature tacitly assumes (i) abundant DRAM, (ii) uniform, decade-long non-volatile retention and (iii) stable power availability. Real deployments violate all three assumptions. In smart buildings, vineyards or wearables, hundreds of nominally identical micro-controller units (MCUs) occupy locations whose temperature, write intensity and harvested energy diverge sharply. Manufacturers therefore grade spin-transfer MRAM pages into three retention tiers-short (\approx 1-3 h), mid (\approx 1-7 d) and long (>5 y)-but today’s controllers over-provision long-retention storage for every node while the shortest-retention bytes remain under-utilised.

\subsection{Problem Statement Under Physical and Resource Constraints}
We ask how to maximise task accuracy under severe concept drift while simultaneously respecting per-node energy budgets, endurance limits and privacy across an entire sensor swarm. The challenge is five-fold: (1) short-retention pages consume two orders of magnitude less write energy than long-term pages yet cannot safely store persistent parameters; (2) abrupt concept shifts demand fast adaptation, but SRAM is scarce on sub-milliwatt MCUs; (3) reactive refresh policies ignore that temperature and harvested power are forecastable hours ahead; (4) bartering data between nodes can reveal user information unless privacy is guaranteed at source; and (5) no public benchmark captures per-cell retention failures across a fleet, hindering reproducibility.

To address these issues we contribute SYMPHONY, a framework that unifies memory physics, meta-learning, predictive control and privacy in a single optimisation.

\subsection{Core Contributions}
\begin{itemize}
  \item \textbf{Retention-Aware Meta-Plasticity (RAMP):} uses 1-3 h MRAM pages as fast weights that store gradient-generated deltas, amortising SRAM and enabling rapid adaptation.
  \item \textbf{Predictive Thermal-Harvest Scheduler (PTHS):} a 21 k-parameter TinyTransformer forecasts temperature and harvest for two hours; an embedded model-predictive controller (MPC) allocates endurance and retention ahead of time.
  \item \textbf{Cooperative Retention Swarm (CReS):} a restless-bandit barter scheme migrates mid-tier pages across Bluetooth Low Energy (BLE), shrinking across-swarm wear variance.
  \item \textbf{Private In-Cell Noise (PICN):} differential-privacy noise is injected by modulating write-current pulses during every refresh, incurring zero extra energy.
  \item \textbf{SwarmRet-120 and IGRE-Lite:} the first 120-day, 20-node dataset that logs multi-modal sensor streams, per-page retention failures and BLE contact graphs; and an openly licensed 4 kB MRAM macro realising in-situ noise generation.
  \item \textbf{Open Simulators and Benchmarks:} cycle-accurate simulators and three experiments that benchmark SYMPHONY against PHOENIX, SparCL \cite{wang-2022-sparcl}, DER++, and ECLIPSE under identical budgets.
\end{itemize}

\subsection{Preview of Current Evidence}
At present only a single-GPU sanity run on CIFAR-10 exists, achieving 86.6 \% accuracy but exercising none of SYMPHONY’s mechanisms; therefore the central claims remain unverified. We provide a detailed gap analysis and a road-map for the required fleet-level evaluation.

The remainder of the paper is organised as follows. Section Related Work contrasts SYMPHONY with prior device-level CL controllers, memory-aware learning and predictive schedulers. Section Background reviews retention physics, meta-learning and MPC. Section Method formalises our optimisation and algorithms. Section Experimental Setup details the three evaluation protocols. Section Results summarises the available logs and identifies missing evidence. Section Conclusion outlines next steps and future research directions.

\section{Related Work}
\subsection{Device-Centric Controllers}
PHOENIX refreshes NVM pages inside a single MCU, ignoring inter-node heterogeneity, while SparCL accelerates CL via sparsity but presumes abundant uniform-retention memory \cite{wang-2022-sparcl}. Orthogonal-subspace training mitigates interference \cite{chaudhry-2020-continual} yet still stores full-precision weights in long-retention storage.

\subsection{Memory Evolution and Sample Selection}
Wasserstein memory evolution hardens replay buffers \cite{wang-2022-improving}; gradient-based sample selection targets maximally interfered examples \cite{aljundi-2019-gradient}; A-GEM improves efficiency via averaged constraints \cite{chaudhry-2018-lifelong}. All three operate strictly within one device and are agnostic to physical wear.

\subsection{Uncertainty-Guided Adaptation}
UCB adapts learning rates using posterior variance \cite{ebrahimi-2019-uncertainty}; SYMPHONY instead adapts write budgets through retention time, coupling physical decay with optimisation.

\subsection{Compression and Privacy}
Online learned compression allocates bits adaptively \cite{caccia-2019-online} but stores data locally. PICN differs by embedding differentially-private noise directly into the write operation, avoiding additional SRAM passes.

\subsection{Predictive Control for Embedded Learning}
MPC is well established in power systems, yet prior CL work remains reactive. SYMPHONY couples a TinyTransformer forecaster with MPC to anticipate thermo-electric trends.

\subsection{Benchmarks for Retention Heterogeneity}
RetenBench-45 profiles a single node; no dataset captures spatial retention heterogeneity. SwarmRet-120 fills this gap by logging per-cell failures across 20 nodes.

To our knowledge SYMPHONY is the first framework to unify meta-plasticity, predictive scheduling, cooperative barter and privacy within retention-aware continual learning.

\section{Background}
\subsection{Retention Physics}
For spin-transfer MRAM the mean retention time \(\tau\) follows an Arrhenius law \(\tau \approx \tau_{0} \exp( E_{a} / (k T) )\) where \(T\) is junction temperature. Manufacturers exploit this dependency to grade pages into short, mid and long retention tiers. Write energy \(E_{\text{write}}\) is inversely related to \(\tau\): a lower thermal barrier allows smaller programming currents.

\subsection{Continual Learning and Meta-Learning}
CL faces non-IID data streams that induce catastrophic forgetting \cite{kirkpatrick-2016-overcoming}. Meta-learning formulates a bi-level optimisation in which an outer loop updates meta-parameters \(\phi\) to minimise the expected loss after an inner update of weights \(\theta\). Fast-weight architectures decouple rapid adaptation (\(\Delta \theta\)) from slow weights, but prior work stores \(\Delta \theta\) in scarce SRAM.

\subsection{Model Predictive Control}
MPC repeatedly solves a finite-horizon problem: minimise the cumulative cost \(\sum_{\tau=1}^{H} c(x_{\tau}, u_{\tau})\) subject to dynamics and constraints, apply the first control input and shift the horizon. When accurate disturbance forecasts are available, MPC can proactively manage resources-here retention allocation and endurance.

\subsection{Problem Formalism}
Each node \(i\) owns capacities \(C_{i}^{r}\) for retention tier \(r \in \{\text{short}, \text{mid}, \text{long}\}\). At time \(t\) the controller chooses: \(\Delta w_{i}^{\text{short}}(t)\), the fast-weight deltas stored in short-tier pages; \(a_{i}^{r}(t)\), the allocation of new pages to tier \(r\); and \(b_{ij}(t)\), barter transactions of mid-tier pages with peer \(j\). State variables include temperature \(f_{i}(t)\), harvested energy \(e_{i}(t)\) and cumulative wear \(w_{i}^{r}(t)\). The multi-objective cost is
\[ L = \sum_{i} \bigl(1 - A_{i}\bigr) + \alpha \, \sigma_{H}^{2} + \beta \, E_{\text{skipped}} + \gamma \, \varepsilon, \]
where \(A_{i}\) is accuracy, \(\sigma_{H}^{2}\) the across-swarm wear variance, \(E_{\text{skipped}}\) the energy-induced training skips and \(\varepsilon\) the differential-privacy budget. Constraints enforce energy causality, endurance limits and a \(20\,\mathrm{kB}\,\mathrm{day}^{-1}\) BLE quota.

\section{Method}
SYMPHONY comprises three interacting control loops that jointly manage learning dynamics, retention allocation and cooperative page barter under energy and endurance constraints.

\subsection{Inner Learning Loop With Fast-Weight Overlays}
For every sample the task backbone produces logits; gradients are computed; an fp16 LSTM with 512 hidden units outputs a delta vector \(\Delta w\). The vector is written to contiguous short-retention pages. Effective weights are
\[ w = w_{\mathrm{long}} + w_{\mathrm{mid}} + \operatorname{decay}(\Delta w_{\mathrm{short}}), \]
where \(\operatorname{decay}(\cdot)\) models exponential leakage in short-retention pages. Gradients do not back-propagate through decayed values, minimising SRAM usage.

\subsection{Predictive Thermal-Harvest Scheduler}
Every 60 s, a TinyTransformer consumes the past 48 min of temperature, irradiance and training loss and emits a two-hour forecast. These trajectories parameterise a convex MPC that minimises a weighted sum of brown-out probability, expected wear and replay freshness, subject to energy and endurance constraints. The solver returns retention allocations \(a_{i}^{r}(t)\) and per-tier write budgets \(\lambda_{i}(t)\).

\subsection{Cooperative Retention Swarm}
When BLE contact is available, each node computes the shadow price of a mid-tier page via a local restless-bandit formulation-the expected future benefit of retaining the page versus exporting it. Nodes with surplus wear export pages; cooler nodes import, respecting the daily 20 kB quota. Transactions \(b_{ij}(t)\) are delta-coded to reduce overhead.

\subsection{Private In-Cell Noise}
During every refresh or barter write, the programming current is jittered with Gaussian noise whose variance is calibrated per cell, guaranteeing an \(\varepsilon\)-differential-privacy bound on released logits. This merges retention refresh and privacy into a single physical operation.

\subsection{Offline Training of Components}
The LSTM, TinyTransformer and MPC cost weights are jointly fitted on three 14-day excerpts of SwarmRet-120 using AdamW (\(\beta = 0.9, 0.99\)). Hyper-parameters swept include meta-learning rate \(\{10^{-4}, 3\!\times\!10^{-4}, 10^{-3}\}\), MPC horizon \(\{1, 2, 4\}\) h, and DP noise factor \(\sigma \in \{0.8, 1.0, 1.2\}\).

\begin{algorithm}[H]
\caption{SYMPHONY Control Loops}
\begin{algorithmic}[1]
\State \textbf{Inputs:} retention capacities \(C_{i}^{r}\), endurance limits, energy buffer state, BLE quota
\State Initialise slow weights \(w_{\mathrm{long}}, w_{\mathrm{mid}}\), fast weights buffer empty
\While{node is powered}
  \State \textbf{(Streaming sample processing)}
  \State Acquire sample \(x\), compute logits and loss \(\ell\)
  \State Backpropagate to obtain gradient \(g = \nabla_{w} \ell\)
  \State LSTM meta-learner emits fast delta \(\Delta w \leftarrow \mathrm{LSTM}(g)\)
  \State Write \(\Delta w\) to short-retention pages (respecting budget \(\lambda_{i}(t)\))
  \State Effective weights: \(w \leftarrow w_{\mathrm{long}} + w_{\mathrm{mid}} + \operatorname{decay}(\Delta w)\)
  \If{time since last schedule \(\geq\) 60 s}
     \State \textbf{(Forecast)} Build windowed features of temperature, irradiance, loss
     \State Obtain 2 h forecasts \(\hat{f}, \hat{e}\) via TinyTransformer
     \State \textbf{(MPC)} Solve convex program for next horizon to minimise brown-out, wear, staleness
     \State Apply first-step controls: retention allocations \(a_{i}^{r}(t)\), write budgets \(\lambda_{i}(t)\)
  \EndIf
  \If{BLE contact available and quota remaining}
     \State For each candidate mid-tier page, compute shadow price via local restless bandit
     \If{export beneficial and constraints satisfied}
        \State Transmit delta-coded page to peer \(j\); update \(b_{ij}(t)\)
     \ElsIf{import beneficial}
        \State Receive page if peer offers; update local wear and storage
     \EndIf
  \EndIf
  \State \textbf{(Refresh with privacy)} For pages scheduled for refresh/barter writes
  \State \quad Apply write-current jitter with cell-calibrated variance to satisfy \(\varepsilon\)-DP
\EndWhile
\end{algorithmic}
\end{algorithm}

\section{Experimental Setup}
\subsection{Experiment 1 - 200-Node End-to-End Evaluation}
We bootstrap the 20 physical SwarmRet-120 logs into 200 virtual nodes. Each node emulates a Nordic nRF54 with 4 MB MRAM, 64 kB SRAM and compute-in-memory accelerators. Modalities: 96$\times$96 RGB faces at 20 Hz processed by Tiny-ViT-0.6 M; 96$\times$96 DVS stacks at 240 Hz by ResNet-18; and CO$_2$ at 1 Hz by a GRU-128. The outer-loop meta-learner holds 1.1 M parameters; the forecaster 21 k. Baselines are PHOENIX, SparCL \cite{wang-2022-sparcl}, DER++, ECLIPSE and a reactive PHOENIX variant. Internal ablations disable RAMP, PTHS, CReS or PICN. Five seeds permute node IDs.

Metrics (logged every 30 min) include: accuracy, backward transfer, Joule per correct-class, across-swarm wear variance \(\sigma_{H}^{2}\), meta-adaptation latency (steps to 90 \% post-shift), brown-out ratio, \(\varepsilon\)-DP guarantee and replay bytes.

Implementation. A cycle-accurate simulator extends PHOENIX with retention decay, fast-weight overlays and BLE barter. Execution uses eight NVIDIA A100 GPUs and a 64-core AMD EPYC host, totalling \(9.7 \times 10^{16}\) FLOPs in 72 h.

\subsection{Experiment 2 - RAMP Micro-Benchmark}
We concatenate Stream-51, EmoSound and AirQo streams to induce nine concept shifts. Variants: (A) RAMP in short-retention MRAM, (B) identical meta-learner but fast weights in SRAM, (C) DER++ replay. The primary metric is \(L_{90}\), the steps needed to regain 90 \% pre-shift accuracy; secondary metric is Joule per recovery.

\subsection{Experiment 3 - PTHS + CReS Stress-Test}
A synthetic 14-day trace imposes attic-level heat (peak 55 $^{\circ}$C) and a 40 h solar eclipse on 64 nodes (32 hot, 32 cool). Policies compared: full SYMPHONY, reactive only, forecast-only and barter-only. Metrics include training-skip ratio, wear variance and forecast MAE.

\subsection{Common Settings}
Optimiser AdamW with weight-decay \(10^{-2}\), batch size 32, five seeds. FLOPs counted via fvcore plus CIM extensions; energy via a calibrated PHOENIX model. All scripts and raw logs are released under MIT licence.

\section{Results}
Only one public log is currently available: a single-GPU run that trained ResNet-18 on CIFAR-10 for 100 epochs (\approx 7 min wall-clock). Best test accuracy reached 86.58 \%. No energy, endurance, privacy or swarm metrics were recorded; no SYMPHONY component was active.

Gap analysis. Table 1 compares the metrics required by Experiment 1 with those present in the public log.

Table 1 - Logged versus required metrics
\noindent\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\begin{tabularx}{\textwidth}{Y c c}
\hline
Metric & Required & Present \\
\hline
Accuracy on SwarmRet-120 & Yes & No \\
Across-swarm wear variance $\sigma_{H}^{2}$ & Yes & No \\
Meta-adaptation latency & Yes & No \\
Brown-out ratio & Yes & No \\
$\varepsilon$-DP guarantee & Yes & No \\
\hline
\end{tabularx}

Because none of the proposed mechanisms executed, the run provides zero evidence for the claimed +4 pp accuracy, 3.2$\times$ faster adaptation or 5.6$\times$ lower wear variance.

Limitations identified. (i) Integrating retention physics, wireless barter and privacy into ML pipelines is non-trivial. (ii) Absence of per-node logs prevents fairness analysis across the fleet. (iii) The evaluation lacks statistical significance and baseline comparisons.

Next steps. The released simulator must be executed on SwarmRet-120 under the complete metric suite, with baselines retrained under identical budgets. Hardware measurements of PICN using IGRE-Lite are also required.

\section{Conclusion}
SYMPHONY advances continual learning for energy-harvesting sensor swarms by exploiting volatile MRAM pages as learnable fast weights, forecasting thermo-energy dynamics for predictive scheduling, bartering retention across nodes to equalise wear, and embedding differential privacy into every write. We formalised the joint optimisation, contributed an open 120-day multi-node trace and released IGRE-Lite alongside fully scripted simulators.

However, the only executed experiment to date was a CIFAR-10 baseline unrelated to our mechanisms. The immediate priority is therefore to run the published simulator on SwarmRet-120, log the complete metric suite and benchmark against PHOENIX, SparCL, DER++ and ECLIPSE. Future work will extend CReS with federated aggregation, adapt MPC horizons via reinforcement learning and fabricate IGRE-Lite silicon to validate privacy guarantees in hardware. We invite the community to replicate, critique and extend SYMPHONY so that decade-long, privacy-preserving adaptation becomes feasible for large-scale IoT fleets.


\bibliographystyle{plainnat}
\bibliography{references}

\end{document}